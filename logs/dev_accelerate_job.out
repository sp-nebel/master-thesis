Obtaining file:///pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (0.30.1)
Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (2.2.4)
Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (2024.11.6)
Requirement already satisfied: requests in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (2.32.3)
Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (0.19.1)
Requirement already satisfied: safetensors>=0.4.1 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./.env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0) (2024.12.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0) (4.13.1)
Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.12/site-packages (from requests->transformers==4.47.0) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.12/site-packages (from requests->transformers==4.47.0) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.12/site-packages (from requests->transformers==4.47.0) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.12/site-packages (from requests->transformers==4.47.0) (2025.1.31)
Building wheels for collected packages: transformers
  Building editable for transformers (pyproject.toml): started
  Building editable for transformers (pyproject.toml): finished with status 'done'
  Created wheel for transformers: filename=transformers-4.47.0-0.editable-py3-none-any.whl size=4778 sha256=b9d6523532a77bf95931de783603a1d9e9ac421f81c49009ca05e6ad1290bf6e
  Stored in directory: /scratch/slurm_tmpdir/job_9858/pip-ephem-wheel-cache-ocw210t9/wheels/ee/bf/dc/6be6e7328682f7a5b12fc84f97e7a227c132b8da4f915209aa
Successfully built transformers
Installing collected packages: transformers
  Attempting uninstall: transformers
    Found existing installation: transformers 4.47.0
    Uninstalling transformers-4.47.0:
      Successfully uninstalled transformers-4.47.0
Successfully installed transformers-4.47.0

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Looking in indexes: https://download.pytorch.org/whl/cu128
Requirement already satisfied: torch in ./.env/lib/python3.12/site-packages (2.6.0)
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from torch) (3.18.0)
Requirement already satisfied: typing-extensions>=4.10.0 in ./.env/lib/python3.12/site-packages (from torch) (4.13.1)
Requirement already satisfied: networkx in ./.env/lib/python3.12/site-packages (from torch) (3.4.2)
Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in ./.env/lib/python3.12/site-packages (from torch) (2024.12.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.env/lib/python3.12/site-packages (from torch) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.env/lib/python3.12/site-packages (from torch) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.env/lib/python3.12/site-packages (from torch) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.env/lib/python3.12/site-packages (from torch) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.env/lib/python3.12/site-packages (from torch) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.env/lib/python3.12/site-packages (from torch) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.env/lib/python3.12/site-packages (from torch) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.env/lib/python3.12/site-packages (from torch) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: triton==3.2.0 in ./.env/lib/python3.12/site-packages (from torch) (3.2.0)
Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch) (78.1.0)
Requirement already satisfied: sympy==1.13.1 in ./.env/lib/python3.12/site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Requirement already satisfied: deepspeed in ./.env/lib/python3.12/site-packages (0.16.5)
Requirement already satisfied: einops in ./.env/lib/python3.12/site-packages (from deepspeed) (0.8.1)
Requirement already satisfied: hjson in ./.env/lib/python3.12/site-packages (from deepspeed) (3.1.0)
Requirement already satisfied: msgpack in ./.env/lib/python3.12/site-packages (from deepspeed) (1.1.0)
Requirement already satisfied: ninja in ./.env/lib/python3.12/site-packages (from deepspeed) (1.11.1.4)
Requirement already satisfied: numpy in ./.env/lib/python3.12/site-packages (from deepspeed) (2.2.4)
Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.12/site-packages (from deepspeed) (24.2)
Requirement already satisfied: psutil in ./.env/lib/python3.12/site-packages (from deepspeed) (7.0.0)
Requirement already satisfied: py-cpuinfo in ./.env/lib/python3.12/site-packages (from deepspeed) (9.0.0)
Requirement already satisfied: pydantic>=2.0.0 in ./.env/lib/python3.12/site-packages (from deepspeed) (2.11.2)
Requirement already satisfied: torch in ./.env/lib/python3.12/site-packages (from deepspeed) (2.6.0)
Requirement already satisfied: tqdm in ./.env/lib/python3.12/site-packages (from deepspeed) (4.67.1)
Requirement already satisfied: annotated-types>=0.6.0 in ./.env/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.1 in ./.env/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (2.33.1)
Requirement already satisfied: typing-extensions>=4.12.2 in ./.env/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (4.13.1)
Requirement already satisfied: typing-inspection>=0.4.0 in ./.env/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (3.18.0)
Requirement already satisfied: networkx in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (3.4.2)
Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (3.1.6)
Requirement already satisfied: fsspec in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (2024.12.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: triton==3.2.0 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (3.2.0)
Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (78.1.0)
Requirement already satisfied: sympy==1.13.1 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.12/site-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch->deepspeed) (3.0.2)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Requirement already satisfied: datasets in ./.env/lib/python3.12/site-packages (3.5.0)
Requirement already satisfied: evaluate in ./.env/lib/python3.12/site-packages (0.4.3)
Requirement already satisfied: peft in ./.env/lib/python3.12/site-packages (0.15.1)
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from datasets) (3.18.0)
Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.12/site-packages (from datasets) (2.2.4)
Requirement already satisfied: pyarrow>=15.0.0 in ./.env/lib/python3.12/site-packages (from datasets) (19.0.1)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.env/lib/python3.12/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in ./.env/lib/python3.12/site-packages (from datasets) (2.2.3)
Requirement already satisfied: requests>=2.32.2 in ./.env/lib/python3.12/site-packages (from datasets) (2.32.3)
Requirement already satisfied: tqdm>=4.66.3 in ./.env/lib/python3.12/site-packages (from datasets) (4.67.1)
Requirement already satisfied: xxhash in ./.env/lib/python3.12/site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in ./.env/lib/python3.12/site-packages (from datasets) (0.70.16)
Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.env/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)
Requirement already satisfied: aiohttp in ./.env/lib/python3.12/site-packages (from datasets) (3.11.16)
Requirement already satisfied: huggingface-hub>=0.24.0 in ./.env/lib/python3.12/site-packages (from datasets) (0.30.1)
Requirement already satisfied: packaging in ./.env/lib/python3.12/site-packages (from datasets) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.12/site-packages (from datasets) (6.0.2)
Requirement already satisfied: psutil in ./.env/lib/python3.12/site-packages (from peft) (7.0.0)
Requirement already satisfied: torch>=1.13.0 in ./.env/lib/python3.12/site-packages (from peft) (2.6.0)
Requirement already satisfied: transformers in ./.env/lib/python3.12/site-packages (from peft) (4.47.0)
Requirement already satisfied: accelerate>=0.21.0 in ./.env/lib/python3.12/site-packages (from peft) (1.6.0)
Requirement already satisfied: safetensors in ./.env/lib/python3.12/site-packages (from peft) (0.5.3)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)
Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (6.3.2)
Requirement already satisfied: propcache>=0.2.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.19.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)
Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)
Requirement already satisfied: networkx in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)
Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: triton==3.2.0 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.2.0)
Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (78.1.0)
Requirement already satisfied: sympy==1.13.1 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)
Requirement already satisfied: python-dateutil>=2.8.2 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.12/site-packages (from transformers->peft) (2024.11.6)
Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.env/lib/python3.12/site-packages (from transformers->peft) (0.19.1)
Requirement already satisfied: six>=1.5 in ./.env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Requirement already satisfied: scikit-learn in ./.env/lib/python3.12/site-packages (1.6.1)
Requirement already satisfied: hf_mtask_trainer in ./.env/lib/python3.12/site-packages (0.0.5)
Requirement already satisfied: numpy>=1.19.5 in ./.env/lib/python3.12/site-packages (from scikit-learn) (2.2.4)
Requirement already satisfied: scipy>=1.6.0 in ./.env/lib/python3.12/site-packages (from scikit-learn) (1.15.2)
Requirement already satisfied: joblib>=1.2.0 in ./.env/lib/python3.12/site-packages (from scikit-learn) (1.4.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in ./.env/lib/python3.12/site-packages (from scikit-learn) (3.6.0)
Requirement already satisfied: torch in ./.env/lib/python3.12/site-packages (from hf_mtask_trainer) (2.6.0)
Requirement already satisfied: transformers>=4.47.0 in ./.env/lib/python3.12/site-packages (from hf_mtask_trainer) (4.47.0)
Requirement already satisfied: accelerate in ./.env/lib/python3.12/site-packages (from hf_mtask_trainer) (1.6.0)
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (0.30.1)
Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (2024.11.6)
Requirement already satisfied: requests in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (2.32.3)
Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (0.19.1)
Requirement already satisfied: safetensors>=0.4.1 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (4.67.1)
Requirement already satisfied: psutil in ./.env/lib/python3.12/site-packages (from accelerate->hf_mtask_trainer) (7.0.0)
Requirement already satisfied: typing-extensions>=4.10.0 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (4.13.1)
Requirement already satisfied: networkx in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (3.4.2)
Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (3.1.6)
Requirement already satisfied: fsspec in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (2024.12.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: triton==3.2.0 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (3.2.0)
Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (78.1.0)
Requirement already satisfied: sympy==1.13.1 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.12/site-packages (from sympy==1.13.1->torch->hf_mtask_trainer) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch->hf_mtask_trainer) (3.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.12/site-packages (from requests->transformers>=4.47.0->hf_mtask_trainer) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.12/site-packages (from requests->transformers>=4.47.0->hf_mtask_trainer) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.12/site-packages (from requests->transformers>=4.47.0->hf_mtask_trainer) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.12/site-packages (from requests->transformers>=4.47.0->hf_mtask_trainer) (2025.1.31)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Requirement already satisfied: seqeval in ./.env/lib/python3.12/site-packages (1.2.2)
Requirement already satisfied: levenshtein in ./.env/lib/python3.12/site-packages (0.27.1)
Requirement already satisfied: numpy>=1.14.0 in ./.env/lib/python3.12/site-packages (from seqeval) (2.2.4)
Requirement already satisfied: scikit-learn>=0.21.3 in ./.env/lib/python3.12/site-packages (from seqeval) (1.6.1)
Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in ./.env/lib/python3.12/site-packages (from levenshtein) (3.13.0)
Requirement already satisfied: scipy>=1.6.0 in ./.env/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)
Requirement already satisfied: joblib>=1.2.0 in ./.env/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in ./.env/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
[2025-04-11 01:10:46,475] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:10:58,598] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:10:58,600] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:10:58,601] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:10:58,608] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:11:01,303] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-11 01:11:01,317] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-11 01:11:01,317] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-11 01:11:01,355] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-11 01:11:01,470] [INFO] [comm.py:658:init_distributed] cdb=None
04/11/2025 01:11:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2025 01:11:01 - INFO - __main__ - Training/evaluation parameters LoRATrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=3600,
debug=[],
deepspeed=./config/deepspeed_config.json,
disable_tqdm=True,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=True,
eval_steps=200,
eval_strategy=IntervalStrategy.STEPS,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=True,
load_lora_from=None,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./test_run_outputs_accelerate_multi_gpu/runs/Apr11_01-10-57_uc3n082.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lora_config=./config/lora_config.json,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.INVERSE_SQRT,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=./test_run_outputs_accelerate_multi_gpu,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=./test_run_outputs_accelerate_multi_gpu,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=1,
seed=1,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=200,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-851ce74e92ca2b7e
04/11/2025 01:11:02 - INFO - datasets.builder - Using custom data configuration default-851ce74e92ca2b7e
Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
04/11/2025 01:11:02 - INFO - datasets.info - Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
04/11/2025 01:11:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/11/2025 01:11:02 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
04/11/2025 01:11:02 - INFO - datasets.builder - Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/11/2025 01:11:02 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/11/2025 01:11:02 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2025 01:11:02 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2025 01:11:02 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:733] 2025-04-11 01:11:02,438 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:11:02,439 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
  "additional_loss_layer": 16,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": 1.0,
  "contrastive_loss_weight": 1.0,
  "contrastive_pooling_type": "mean",
  "distance_function": "cosine",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": false,
  "only_train_language_modeling": true,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": false,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2269] 2025-04-11 01:11:02,556 >> loading file tokenizer.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json
[INFO|tokenization_utils_base.py:2269] 2025-04-11 01:11:02,556 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2269] 2025-04-11 01:11:02,556 >> loading file special_tokens_map.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json
[INFO|tokenization_utils_base.py:2269] 2025-04-11 01:11:02,556 >> loading file tokenizer_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json
[INFO|tokenization_utils_base.py:2513] 2025-04-11 01:11:02,757 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
04/11/2025 01:11:02 - INFO - __main__ - Tokenizer is fast: True
[INFO|modeling_utils.py:3667] 2025-04-11 01:11:02,763 >> loading weights file model.safetensors from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json
[INFO|modeling_utils.py:1591] 2025-04-11 01:11:02,766 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1038] 2025-04-11 01:11:02,767 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.04it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.61it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.51it/s]
[INFO|modeling_utils.py:4499] 2025-04-11 01:11:03,160 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4507] 2025-04-11 01:11:03,160 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][INFO|configuration_utils.py:993] 2025-04-11 01:11:03,283 >> loading configuration file generation_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json
[INFO|configuration_utils.py:1038] 2025-04-11 01:11:03,283 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

adding special tokens...
04/11/2025 01:11:03 - INFO - __main__ - ================ pad, eos, bos, unk, padding ================ 
04/11/2025 01:11:03 - INFO - __main__ - <|eot_id|>, 128009
04/11/2025 01:11:03 - INFO - __main__ - <|eot_id|>, 128009
04/11/2025 01:11:03 - INFO - __main__ - <|begin_of_text|>, 128000
04/11/2025 01:11:03 - INFO - __main__ - <|reserved_special_token_0|>, 128002
04/11/2025 01:11:03 - INFO - __main__ - right
04/11/2025 01:11:03 - INFO - __main__ - lora_r : 8
04/11/2025 01:11:03 - INFO - __main__ - lora_alpha : 16
04/11/2025 01:11:03 - INFO - __main__ - lora_dropout : 0.1
04/11/2025 01:11:03 - INFO - __main__ - lora_target_modules : ['q_proj', 'v_proj']
04/11/2025 01:11:03 - INFO - __main__ - LoRA configs: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
04/11/2025 01:11:03 - INFO - __main__ - block size: 2048
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.61it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.34it/s]Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
04/11/2025 01:11:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.42it/s]Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
04/11/2025 01:11:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.68it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.49it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.38it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.21it/s]
adding special tokens...
adding special tokens...
[rank0]:[W411 01:11:03.917593139 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
[rank2]:[W411 01:11:03.936898339 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
[rank3]:[W411 01:11:03.957371091 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
adding special tokens...
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
[rank1]:[W411 01:11:03.020185732 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
04/11/2025 01:11:06 - INFO - __main__ - xxx: Showcase the tokenized training samples.
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[INFO|trainer.py:658] 2025-04-11 01:11:07,351 >> Using auto half precision backend
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[2025-04-11 01:11:07,544] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-11 01:11:07,544] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-11 01:11:35,325] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 1.3694212436676025 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 1.406226634979248 seconds
[2025-04-11 01:11:36,739] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-04-11 01:11:36,739] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
Time to load fused_adam op: 1.4070625305175781 seconds
Loading extension module fused_adam...
[2025-04-11 01:11:36,743] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-11 01:11:36,743] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-11 01:11:36,743] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
Time to load fused_adam op: 1.4069392681121826 seconds
[2025-04-11 01:11:36,743] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-04-11 01:11:36,743] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-04-11 01:11:36,743] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-11 01:11:36,743] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-11 01:11:36,943] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-11 01:11:36,943] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.03 GB         Max_CA 6 GB 
[2025-04-11 01:11:36,944] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 37.86 GB, percent = 5.0%
[2025-04-11 01:11:37,095] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-11 01:11:37,096] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.05 GB         Max_CA 6 GB 
[2025-04-11 01:11:37,096] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.11 GB, percent = 5.0%
[2025-04-11 01:11:37,096] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-11 01:11:37,246] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-11 01:11:37,247] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.05 GB         Max_CA 6 GB 
[2025-04-11 01:11:37,247] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.51 GB, percent = 5.1%
[2025-04-11 01:11:37,248] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-11 01:11:37,248] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler
[2025-04-11 01:11:37,248] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x14a618926ab0>
[2025-04-11 01:11:37,248] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[[0.9, 0.999]]
[2025-04-11 01:11:37,249] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   bfloat16_enabled ............. True
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  True
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14a628859fd0>
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... None
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   fp16_auto_cast ............... None
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   fp16_enabled ................. False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 1
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   loss_scale ................... 1.0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   optimizer_name ............... adam
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0005, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   steps_per_print .............. inf
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   train_batch_size ............. 128
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  32
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   world_size ................... 4
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-11 01:11:37,251] [INFO] [config.py:990:print_user_config]   json = {
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0005, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "wall_clock_breakdown": false
}
[INFO|trainer.py:2145] 2025-04-11 01:11:37,253 >> ***** Running training *****
[INFO|trainer.py:2146] 2025-04-11 01:11:37,253 >>   Num examples = 392,702
[INFO|trainer.py:2147] 2025-04-11 01:11:37,253 >>   Num Epochs = 1
[INFO|trainer.py:2148] 2025-04-11 01:11:37,253 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2151] 2025-04-11 01:11:37,253 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2152] 2025-04-11 01:11:37,253 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2153] 2025-04-11 01:11:37,253 >>   Total optimization steps = 3,068
[INFO|trainer.py:2154] 2025-04-11 01:11:37,254 >>   Number of trainable parameters = 2,293,760
[INFO|trainer.py:3831] 2025-04-11 01:11:37,258 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:11:37,258 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:11:37,258 >>   Batch size = 16
{'eval_loss': 4.181094646453857, 'eval_accuracy': 0.11632878820671909, 'eval_runtime': 3.5449, 'eval_samples_per_second': 702.415, 'eval_steps_per_second': 11.002, 'epoch': 0}
{'loss': 4.0011, 'grad_norm': 6.475341796875, 'learning_rate': 5.3763440860215054e-05, 'epoch': 0.003259452411994785}
{'loss': 2.5782, 'grad_norm': 7.224255084991455, 'learning_rate': 0.00010752688172043011, 'epoch': 0.00651890482398957}
{'loss': 0.3632, 'grad_norm': 1.1436693668365479, 'learning_rate': 0.00016129032258064516, 'epoch': 0.009778357235984355}
{'loss': 0.1961, 'grad_norm': 0.703717052936554, 'learning_rate': 0.00021505376344086021, 'epoch': 0.01303780964797914}
{'loss': 0.1392, 'grad_norm': 0.4350185990333557, 'learning_rate': 0.00026881720430107527, 'epoch': 0.016297262059973925}
{'loss': 0.1096, 'grad_norm': 0.9797947406768799, 'learning_rate': 0.0003225806451612903, 'epoch': 0.01955671447196871}
{'loss': 0.1018, 'grad_norm': 1.3039120435714722, 'learning_rate': 0.0003763440860215054, 'epoch': 0.022816166883963495}
{'loss': 0.09, 'grad_norm': 0.8019970059394836, 'learning_rate': 0.00043010752688172043, 'epoch': 0.02607561929595828}
{'loss': 0.0833, 'grad_norm': 0.4534195363521576, 'learning_rate': 0.0004838709677419355, 'epoch': 0.029335071707953065}
{'loss': 0.0805, 'grad_norm': 0.7451388239860535, 'learning_rate': 0.00048218253804964786, 'epoch': 0.03259452411994785}
{'loss': 0.0729, 'grad_norm': 0.7122637629508972, 'learning_rate': 0.00045974301121782846, 'epoch': 0.03585397653194263}
{'loss': 0.0838, 'grad_norm': 0.370341956615448, 'learning_rate': 0.00044017042154147526, 'epoch': 0.03911342894393742}
{'loss': 0.0746, 'grad_norm': 0.5737557411193848, 'learning_rate': 0.00042290206176626036, 'epoch': 0.0423728813559322}
{'loss': 0.0723, 'grad_norm': 0.482598215341568, 'learning_rate': 0.00040751862358845463, 'epoch': 0.04563233376792699}
{'loss': 0.0697, 'grad_norm': 0.2839057743549347, 'learning_rate': 0.00039370039370059055, 'epoch': 0.04889178617992177}
{'loss': 0.0716, 'grad_norm': 0.2679470479488373, 'learning_rate': 0.0003811987670494227, 'epoch': 0.05215123859191656}
{'loss': 0.0663, 'grad_norm': 0.4278111457824707, 'learning_rate': 0.0003698171249176449, 'epoch': 0.05541069100391134}
{'loss': 0.0701, 'grad_norm': 0.2978014349937439, 'learning_rate': 0.00035939764421413046, 'epoch': 0.05867014341590613}
{'loss': 0.0626, 'grad_norm': 0.676891028881073, 'learning_rate': 0.0003498119795727865, 'epoch': 0.06192959582790091}
{'loss': 0.0685, 'grad_norm': 0.3426222801208496, 'learning_rate': 0.0003409545424246464, 'epoch': 0.0651890482398957}
[INFO|trainer.py:3831] 2025-04-11 01:13:15,182 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:13:15,182 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:13:15,182 >>   Batch size = 16
{'eval_loss': 0.06155926734209061, 'eval_accuracy': 0.13350648101387586, 'eval_runtime': 3.3879, 'eval_samples_per_second': 734.965, 'eval_steps_per_second': 11.511, 'epoch': 0.0651890482398957}
[INFO|trainer.py:3515] 2025-04-11 01:13:19,865 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-200
[INFO|configuration_utils.py:733] 2025-04-11 01:13:20,180 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:13:20,180 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:13:20,195 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:13:20,196 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/special_tokens_map.json
[2025-04-11 01:13:20,312] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-04-11 01:13:20,321] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2025-04-11 01:13:20,321] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2025-04-11 01:13:21,293] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2025-04-11 01:13:21,296] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:13:21,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:13:21,309] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:13:21,309] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|configuration_utils.py:733] 2025-04-11 01:13:21,563 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:13:21,564 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0634, 'grad_norm': 0.29729172587394714, 'learning_rate': 0.0003327375628243462, 'epoch': 0.06844850065189048}
{'loss': 0.0606, 'grad_norm': 0.25571343302726746, 'learning_rate': 0.00032508740083524954, 'epoch': 0.07170795306388526}
{'loss': 0.0721, 'grad_norm': 0.2791629731655121, 'learning_rate': 0.0003179417502023588, 'epoch': 0.07496740547588006}
{'loss': 0.0655, 'grad_norm': 0.4568226635456085, 'learning_rate': 0.00031124748994971834, 'epoch': 0.07822685788787484}
{'loss': 0.0627, 'grad_norm': 0.31770098209381104, 'learning_rate': 0.00030495901363953817, 'epoch': 0.08148631029986962}
{'loss': 0.0637, 'grad_norm': 0.33484312891960144, 'learning_rate': 0.0002990369156526948, 'epoch': 0.0847457627118644}
{'loss': 0.0518, 'grad_norm': 0.27704551815986633, 'learning_rate': 0.0002934469476943168, 'epoch': 0.0880052151238592}
{'loss': 0.0583, 'grad_norm': 0.1985533982515335, 'learning_rate': 0.00028815918219920447, 'epoch': 0.09126466753585398}
{'loss': 0.0606, 'grad_norm': 0.3099674582481384, 'learning_rate': 0.00028314733583967105, 'epoch': 0.09452411994784876}
{'loss': 0.0592, 'grad_norm': 0.3921815752983093, 'learning_rate': 0.00027838821814150114, 'epoch': 0.09778357235984354}
{'loss': 0.0604, 'grad_norm': 0.241848424077034, 'learning_rate': 0.00027386127875258305, 'epoch': 0.10104302477183832}
{'loss': 0.0614, 'grad_norm': 0.2843869626522064, 'learning_rate': 0.0002695482331605978, 'epoch': 0.10430247718383312}
{'loss': 0.0595, 'grad_norm': 0.38516637682914734, 'learning_rate': 0.00026543275128466237, 'epoch': 0.1075619295958279}
{'loss': 0.06, 'grad_norm': 0.3079080581665039, 'learning_rate': 0.0002615001968281792, 'epoch': 0.11082138200782268}
{'loss': 0.0694, 'grad_norm': 0.33549943566322327, 'learning_rate': 0.00025773740789526733, 'epoch': 0.11408083441981746}
{'loss': 0.0627, 'grad_norm': 0.22225433588027954, 'learning_rate': 0.0002541325113662818, 'epoch': 0.11734028683181226}
{'loss': 0.0598, 'grad_norm': 0.3368756175041199, 'learning_rate': 0.00025067476505990355, 'epoch': 0.12059973924380704}
{'loss': 0.0594, 'grad_norm': 0.2834566831588745, 'learning_rate': 0.0002473544228962074, 'epoch': 0.12385919165580182}
{'loss': 0.0601, 'grad_norm': 0.25408756732940674, 'learning_rate': 0.00024416261920159817, 'epoch': 0.1271186440677966}
{'loss': 0.0578, 'grad_norm': 0.2036093771457672, 'learning_rate': 0.00024109126902482393, 'epoch': 0.1303780964797914}
[INFO|trainer.py:3831] 2025-04-11 01:14:54,465 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:14:54,465 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:14:54,465 >>   Batch size = 16
{'eval_loss': 0.05581275746226311, 'eval_accuracy': 0.13360267416973282, 'eval_runtime': 3.3828, 'eval_samples_per_second': 736.078, 'eval_steps_per_second': 11.529, 'epoch': 0.1303780964797914}
[INFO|trainer.py:3515] 2025-04-11 01:14:59,043 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-400
[INFO|configuration_utils.py:733] 2025-04-11 01:14:59,303 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:14:59,303 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:14:59,319 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:14:59,321 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/special_tokens_map.json
[2025-04-11 01:14:59,428] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-04-11 01:14:59,436] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2025-04-11 01:14:59,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2025-04-11 01:15:00,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2025-04-11 01:15:00,412] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:15:00,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:15:00,425] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:15:00,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|configuration_utils.py:733] 2025-04-11 01:15:00,673 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:15:00,673 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0612, 'grad_norm': 0.22801560163497925, 'learning_rate': 0.000238132981909627, 'epoch': 0.13363754889178617}
{'loss': 0.0588, 'grad_norm': 0.3386504054069519, 'learning_rate': 0.00023528098702858003, 'epoch': 0.13689700130378096}
{'loss': 0.054, 'grad_norm': 0.24290193617343903, 'learning_rate': 0.00023252906795035426, 'epoch': 0.14015645371577576}
{'loss': 0.0591, 'grad_norm': 0.40227210521698, 'learning_rate': 0.00022987150560891423, 'epoch': 0.14341590612777053}
{'loss': 0.061, 'grad_norm': 0.25004178285598755, 'learning_rate': 0.0002273030282830976, 'epoch': 0.14667535853976532}
{'loss': 0.0592, 'grad_norm': 0.43192288279533386, 'learning_rate': 0.00022481876759040728, 'epoch': 0.14993481095176012}
{'loss': 0.0584, 'grad_norm': 0.2631235718727112, 'learning_rate': 0.0002224142196586877, 'epoch': 0.15319426336375488}
{'loss': 0.0576, 'grad_norm': 0.23236486315727234, 'learning_rate': 0.00022008521077073763, 'epoch': 0.15645371577574968}
{'loss': 0.0542, 'grad_norm': 0.47378450632095337, 'learning_rate': 0.0002178278668853844, 'epoch': 0.15971316818774445}
{'loss': 0.0556, 'grad_norm': 0.2766890525817871, 'learning_rate': 0.00021563858652847822, 'epoch': 0.16297262059973924}
{'loss': 0.0526, 'grad_norm': 0.2232927680015564, 'learning_rate': 0.00021351401662213573, 'epoch': 0.16623207301173404}
{'loss': 0.0581, 'grad_norm': 0.2900848984718323, 'learning_rate': 0.00021145103088313018, 'epoch': 0.1694915254237288}
{'loss': 0.0586, 'grad_norm': 0.24277061223983765, 'learning_rate': 0.0002094467104738145, 'epoch': 0.1727509778357236}
{'loss': 0.0569, 'grad_norm': 0.33006253838539124, 'learning_rate': 0.00020749832663314552, 'epoch': 0.1760104302477184}
{'loss': 0.0611, 'grad_norm': 0.24546699225902557, 'learning_rate': 0.00020560332505270259, 'epoch': 0.17926988265971316}
{'loss': 0.0601, 'grad_norm': 0.25890809297561646, 'learning_rate': 0.00020375931179422732, 'epoch': 0.18252933507170796}
{'loss': 0.0588, 'grad_norm': 0.2103394865989685, 'learning_rate': 0.00020196404057210414, 'epoch': 0.18578878748370273}
{'loss': 0.0618, 'grad_norm': 0.3027498126029968, 'learning_rate': 0.00020021540124713614, 'epoch': 0.18904823989569752}
{'loss': 0.054, 'grad_norm': 0.28454816341400146, 'learning_rate': 0.0001985114093975884, 'epoch': 0.19230769230769232}
{'loss': 0.0565, 'grad_norm': 0.24706344306468964, 'learning_rate': 0.00019685019685029527, 'epoch': 0.19556714471968709}
[INFO|trainer.py:3831] 2025-04-11 01:16:35,539 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:16:35,539 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:16:35,539 >>   Batch size = 16
{'eval_loss': 0.05300065129995346, 'eval_accuracy': 0.13361469831421494, 'eval_runtime': 3.3983, 'eval_samples_per_second': 732.72, 'eval_steps_per_second': 11.476, 'epoch': 0.19556714471968709}
[INFO|trainer.py:3515] 2025-04-11 01:16:40,144 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-600
[INFO|configuration_utils.py:733] 2025-04-11 01:16:40,386 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:16:40,387 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:16:40,401 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:16:40,403 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/special_tokens_map.json
[2025-04-11 01:16:40,510] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2025-04-11 01:16:40,519] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/mp_rank_00_model_states.pt
[2025-04-11 01:16:40,519] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/mp_rank_00_model_states.pt...
[2025-04-11 01:16:41,478] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/mp_rank_00_model_states.pt.
[2025-04-11 01:16:41,480] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:16:41,490] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:16:41,492] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:16:41,492] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[INFO|trainer.py:3607] 2025-04-11 01:16:41,500 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-11 01:16:41,943 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:16:41,943 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.051, 'grad_norm': 0.3618951737880707, 'learning_rate': 0.00019523000306909962, 'epoch': 0.19882659713168188}
{'loss': 0.0559, 'grad_norm': 0.4594591557979584, 'learning_rate': 0.00019364916731037085, 'epoch': 0.20208604954367665}
{'loss': 0.0584, 'grad_norm': 0.24958327412605286, 'learning_rate': 0.0001921061214661363, 'epoch': 0.20534550195567144}
{'loss': 0.0559, 'grad_norm': 0.3014610707759857, 'learning_rate': 0.00019059938352471135, 'epoch': 0.20860495436766624}
{'loss': 0.063, 'grad_norm': 0.32150718569755554, 'learning_rate': 0.00018912755158683456, 'epoch': 0.211864406779661}
{'loss': 0.0603, 'grad_norm': 0.24348366260528564, 'learning_rate': 0.00018768929838238707, 'epoch': 0.2151238591916558}
{'loss': 0.0567, 'grad_norm': 0.20405063033103943, 'learning_rate': 0.0001862833662389464, 'epoch': 0.2183833116036506}
{'loss': 0.0521, 'grad_norm': 0.27425551414489746, 'learning_rate': 0.00018490856245882245, 'epoch': 0.22164276401564537}
{'loss': 0.0561, 'grad_norm': 0.25703343749046326, 'learning_rate': 0.00018356375506595264, 'epoch': 0.22490221642764016}
{'loss': 0.0554, 'grad_norm': 0.24313108623027802, 'learning_rate': 0.00018224786888818676, 'epoch': 0.22816166883963493}
{'loss': 0.0552, 'grad_norm': 0.21311093866825104, 'learning_rate': 0.00018095988194414647, 'epoch': 0.23142112125162972}
{'loss': 0.0526, 'grad_norm': 0.2095230221748352, 'learning_rate': 0.00017969882210706523, 'epoch': 0.23468057366362452}
{'loss': 0.0558, 'grad_norm': 0.21875272691249847, 'learning_rate': 0.00017846376402085983, 'epoch': 0.2379400260756193}
{'loss': 0.0602, 'grad_norm': 0.42362791299819946, 'learning_rate': 0.00017725382624620242, 'epoch': 0.24119947848761408}
{'loss': 0.0548, 'grad_norm': 0.24822181463241577, 'learning_rate': 0.0001760681686165901, 'epoch': 0.24445893089960888}
{'loss': 0.0558, 'grad_norm': 0.3080138564109802, 'learning_rate': 0.00017490598978639325, 'epoch': 0.24771838331160365}
{'loss': 0.0548, 'grad_norm': 0.22705844044685364, 'learning_rate': 0.00017376652495462177, 'epoch': 0.25097783572359844}
{'loss': 0.0568, 'grad_norm': 0.1883169263601303, 'learning_rate': 0.00017264904374971875, 'epoch': 0.2542372881355932}
{'loss': 0.0536, 'grad_norm': 0.305027574300766, 'learning_rate': 0.00017155284826208934, 'epoch': 0.25749674054758803}
{'loss': 0.0601, 'grad_norm': 0.29442980885505676, 'learning_rate': 0.0001704772712123232, 'epoch': 0.2607561929595828}
[INFO|trainer.py:3831] 2025-04-11 01:18:15,713 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:18:15,713 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:18:15,713 >>   Batch size = 16
{'eval_loss': 0.05379442498087883, 'eval_accuracy': 0.1336411514320756, 'eval_runtime': 3.397, 'eval_samples_per_second': 733.001, 'eval_steps_per_second': 11.481, 'epoch': 0.2607561929595828}
[INFO|trainer.py:3515] 2025-04-11 01:18:20,329 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-800
[INFO|configuration_utils.py:733] 2025-04-11 01:18:20,584 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:18:20,585 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:18:20,600 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:18:20,601 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/special_tokens_map.json
[2025-04-11 01:18:20,707] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
[2025-04-11 01:18:20,716] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/mp_rank_00_model_states.pt
[2025-04-11 01:18:20,716] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/mp_rank_00_model_states.pt...
[2025-04-11 01:18:21,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/mp_rank_00_model_states.pt.
[2025-04-11 01:18:21,676] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:18:21,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:18:21,688] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:18:21,688] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[INFO|trainer.py:3607] 2025-04-11 01:18:21,695 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-11 01:18:21,951 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:18:21,952 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0579, 'grad_norm': 0.21903987228870392, 'learning_rate': 0.00016942167424418786, 'epoch': 0.26401564537157757}
{'loss': 0.0537, 'grad_norm': 0.221793070435524, 'learning_rate': 0.0001683854463324707, 'epoch': 0.26727509778357234}
{'loss': 0.0551, 'grad_norm': 0.28474846482276917, 'learning_rate': 0.00016736800229664894, 'epoch': 0.27053455019556716}
{'loss': 0.0547, 'grad_norm': 0.46689990162849426, 'learning_rate': 0.0001663687814121731, 'epoch': 0.2737940026075619}
{'loss': 0.0561, 'grad_norm': 0.1990491896867752, 'learning_rate': 0.00016538724611187705, 'epoch': 0.2770534550195567}
{'loss': 0.0558, 'grad_norm': 0.3514726459980011, 'learning_rate': 0.00016442288077068298, 'epoch': 0.2803129074315515}
{'loss': 0.0524, 'grad_norm': 0.31406471133232117, 'learning_rate': 0.00016347519056735942, 'epoch': 0.2835723598435463}
{'loss': 0.0572, 'grad_norm': 0.3588111400604248, 'learning_rate': 0.00016254370041762477, 'epoch': 0.28683181225554105}
{'loss': 0.0502, 'grad_norm': 0.2632896602153778, 'learning_rate': 0.00016162795397337053, 'epoch': 0.2900912646675359}
{'loss': 0.0578, 'grad_norm': 0.21954305469989777, 'learning_rate': 0.00016072751268321592, 'epoch': 0.29335071707953064}
{'loss': 0.0574, 'grad_norm': 0.25954148173332214, 'learning_rate': 0.00015984195491000025, 'epoch': 0.2966101694915254}
{'loss': 0.0497, 'grad_norm': 0.3536636233329773, 'learning_rate': 0.0001589708751011794, 'epoch': 0.29986962190352023}
{'loss': 0.057, 'grad_norm': 0.2310969978570938, 'learning_rate': 0.00015811388300841897, 'epoch': 0.303129074315515}
{'loss': 0.0572, 'grad_norm': 0.18835866451263428, 'learning_rate': 0.0001572706029529724, 'epoch': 0.30638852672750977}
{'loss': 0.0582, 'grad_norm': 0.2912144958972931, 'learning_rate': 0.00015644067313370366, 'epoch': 0.30964797913950454}
{'loss': 0.0496, 'grad_norm': 0.24995091557502747, 'learning_rate': 0.00015562374497485917, 'epoch': 0.31290743155149936}
{'loss': 0.0586, 'grad_norm': 0.254310667514801, 'learning_rate': 0.00015481948251091802, 'epoch': 0.3161668839634941}
{'loss': 0.0561, 'grad_norm': 0.26981624960899353, 'learning_rate': 0.0001540275618060559, 'epoch': 0.3194263363754889}
{'loss': 0.0529, 'grad_norm': 0.3228878676891327, 'learning_rate': 0.00015324767040594283, 'epoch': 0.3226857887874837}
{'loss': 0.0546, 'grad_norm': 0.31385937333106995, 'learning_rate': 0.00015247950681976908, 'epoch': 0.3259452411994785}
[INFO|trainer.py:3831] 2025-04-11 01:19:58,225 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:19:58,225 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:19:58,225 >>   Batch size = 16
{'eval_loss': 0.05158242955803871, 'eval_accuracy': 0.13367962869441838, 'eval_runtime': 3.3959, 'eval_samples_per_second': 733.239, 'eval_steps_per_second': 11.484, 'epoch': 0.3259452411994785}
[INFO|trainer.py:3515] 2025-04-11 01:20:02,842 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000
[INFO|configuration_utils.py:733] 2025-04-11 01:20:03,083 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:20:03,083 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:20:03,098 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:20:03,099 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/special_tokens_map.json
[2025-04-11 01:20:03,205] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2025-04-11 01:20:03,214] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2025-04-11 01:20:03,214] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[2025-04-11 01:20:04,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2025-04-11 01:20:04,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:20:04,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:20:04,203] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:20:04,203] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[INFO|trainer.py:3607] 2025-04-11 01:20:04,210 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-11 01:20:04,454 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:20:04,455 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

slurmstepd: error: *** JOB 9858 ON uc3n082 CANCELLED AT 2025-04-11T01:20:06 DUE TO TIME LIMIT ***

============================= JOB FEEDBACK =============================

NodeName=uc3n082
Job ID: 9858
Cluster: uc3
User/Group: ka_usxcp/ka_stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 192
CPU Utilized: 00:00:23
CPU Efficiency: 0.02% of 1-09:16:48 core-walltime
Job Wall-clock time: 00:10:24
Memory Utilized: 11.85 GB
Memory Efficiency: 18.52% of 64.00 GB (64.00 GB/node)
