Obtaining file:///pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (0.30.1)
Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (2.2.4)
Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (2024.11.6)
Requirement already satisfied: requests in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (2.32.3)
Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (0.19.1)
Requirement already satisfied: safetensors>=0.4.1 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in ./.env/lib/python3.12/site-packages (from transformers==4.47.0) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./.env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0) (2024.12.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0) (4.13.1)
Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.12/site-packages (from requests->transformers==4.47.0) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.12/site-packages (from requests->transformers==4.47.0) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.12/site-packages (from requests->transformers==4.47.0) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.12/site-packages (from requests->transformers==4.47.0) (2025.1.31)
Building wheels for collected packages: transformers
  Building editable for transformers (pyproject.toml): started
  Building editable for transformers (pyproject.toml): finished with status 'done'
  Created wheel for transformers: filename=transformers-4.47.0-0.editable-py3-none-any.whl size=4778 sha256=28b70b2ba0331fbe2e07ba6d3e18f0743d5910a960b04030402a987e81fe2a68
  Stored in directory: /scratch/slurm_tmpdir/job_9294/pip-ephem-wheel-cache-fziwfcox/wheels/ee/bf/dc/6be6e7328682f7a5b12fc84f97e7a227c132b8da4f915209aa
Successfully built transformers
Installing collected packages: transformers
  Attempting uninstall: transformers
    Found existing installation: transformers 4.47.0
    Uninstalling transformers-4.47.0:
      Successfully uninstalled transformers-4.47.0
Successfully installed transformers-4.47.0

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Looking in indexes: https://download.pytorch.org/whl/cu128
Requirement already satisfied: torch in ./.env/lib/python3.12/site-packages (2.6.0)
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from torch) (3.18.0)
Requirement already satisfied: typing-extensions>=4.10.0 in ./.env/lib/python3.12/site-packages (from torch) (4.13.1)
Requirement already satisfied: networkx in ./.env/lib/python3.12/site-packages (from torch) (3.4.2)
Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in ./.env/lib/python3.12/site-packages (from torch) (2024.12.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.env/lib/python3.12/site-packages (from torch) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.env/lib/python3.12/site-packages (from torch) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.env/lib/python3.12/site-packages (from torch) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.env/lib/python3.12/site-packages (from torch) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.env/lib/python3.12/site-packages (from torch) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.env/lib/python3.12/site-packages (from torch) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.env/lib/python3.12/site-packages (from torch) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.env/lib/python3.12/site-packages (from torch) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch) (12.4.127)
Requirement already satisfied: triton==3.2.0 in ./.env/lib/python3.12/site-packages (from torch) (3.2.0)
Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch) (78.1.0)
Requirement already satisfied: sympy==1.13.1 in ./.env/lib/python3.12/site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Requirement already satisfied: deepspeed in ./.env/lib/python3.12/site-packages (0.16.5)
Requirement already satisfied: einops in ./.env/lib/python3.12/site-packages (from deepspeed) (0.8.1)
Requirement already satisfied: hjson in ./.env/lib/python3.12/site-packages (from deepspeed) (3.1.0)
Requirement already satisfied: msgpack in ./.env/lib/python3.12/site-packages (from deepspeed) (1.1.0)
Requirement already satisfied: ninja in ./.env/lib/python3.12/site-packages (from deepspeed) (1.11.1.4)
Requirement already satisfied: numpy in ./.env/lib/python3.12/site-packages (from deepspeed) (2.2.4)
Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.12/site-packages (from deepspeed) (24.2)
Requirement already satisfied: psutil in ./.env/lib/python3.12/site-packages (from deepspeed) (7.0.0)
Requirement already satisfied: py-cpuinfo in ./.env/lib/python3.12/site-packages (from deepspeed) (9.0.0)
Requirement already satisfied: pydantic>=2.0.0 in ./.env/lib/python3.12/site-packages (from deepspeed) (2.11.2)
Requirement already satisfied: torch in ./.env/lib/python3.12/site-packages (from deepspeed) (2.6.0)
Requirement already satisfied: tqdm in ./.env/lib/python3.12/site-packages (from deepspeed) (4.67.1)
Requirement already satisfied: annotated-types>=0.6.0 in ./.env/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.1 in ./.env/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (2.33.1)
Requirement already satisfied: typing-extensions>=4.12.2 in ./.env/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (4.13.1)
Requirement already satisfied: typing-inspection>=0.4.0 in ./.env/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (3.18.0)
Requirement already satisfied: networkx in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (3.4.2)
Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (3.1.6)
Requirement already satisfied: fsspec in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (2024.12.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (12.4.127)
Requirement already satisfied: triton==3.2.0 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (3.2.0)
Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (78.1.0)
Requirement already satisfied: sympy==1.13.1 in ./.env/lib/python3.12/site-packages (from torch->deepspeed) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.12/site-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch->deepspeed) (3.0.2)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Requirement already satisfied: datasets in ./.env/lib/python3.12/site-packages (3.5.0)
Requirement already satisfied: evaluate in ./.env/lib/python3.12/site-packages (0.4.3)
Requirement already satisfied: peft in ./.env/lib/python3.12/site-packages (0.15.1)
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from datasets) (3.18.0)
Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.12/site-packages (from datasets) (2.2.4)
Requirement already satisfied: pyarrow>=15.0.0 in ./.env/lib/python3.12/site-packages (from datasets) (19.0.1)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.env/lib/python3.12/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in ./.env/lib/python3.12/site-packages (from datasets) (2.2.3)
Requirement already satisfied: requests>=2.32.2 in ./.env/lib/python3.12/site-packages (from datasets) (2.32.3)
Requirement already satisfied: tqdm>=4.66.3 in ./.env/lib/python3.12/site-packages (from datasets) (4.67.1)
Requirement already satisfied: xxhash in ./.env/lib/python3.12/site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in ./.env/lib/python3.12/site-packages (from datasets) (0.70.16)
Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.env/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)
Requirement already satisfied: aiohttp in ./.env/lib/python3.12/site-packages (from datasets) (3.11.16)
Requirement already satisfied: huggingface-hub>=0.24.0 in ./.env/lib/python3.12/site-packages (from datasets) (0.30.1)
Requirement already satisfied: packaging in ./.env/lib/python3.12/site-packages (from datasets) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.12/site-packages (from datasets) (6.0.2)
Requirement already satisfied: psutil in ./.env/lib/python3.12/site-packages (from peft) (7.0.0)
Requirement already satisfied: torch>=1.13.0 in ./.env/lib/python3.12/site-packages (from peft) (2.6.0)
Requirement already satisfied: transformers in ./.env/lib/python3.12/site-packages (from peft) (4.47.0)
Requirement already satisfied: accelerate>=0.21.0 in ./.env/lib/python3.12/site-packages (from peft) (1.6.0)
Requirement already satisfied: safetensors in ./.env/lib/python3.12/site-packages (from peft) (0.5.3)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)
Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (6.3.2)
Requirement already satisfied: propcache>=0.2.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.19.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)
Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)
Requirement already satisfied: networkx in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)
Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)
Requirement already satisfied: triton==3.2.0 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.2.0)
Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (78.1.0)
Requirement already satisfied: sympy==1.13.1 in ./.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)
Requirement already satisfied: python-dateutil>=2.8.2 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.12/site-packages (from transformers->peft) (2024.11.6)
Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.env/lib/python3.12/site-packages (from transformers->peft) (0.19.1)
Requirement already satisfied: six>=1.5 in ./.env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Requirement already satisfied: scikit-learn in ./.env/lib/python3.12/site-packages (1.6.1)
Requirement already satisfied: hf_mtask_trainer in ./.env/lib/python3.12/site-packages (0.0.5)
Requirement already satisfied: numpy>=1.19.5 in ./.env/lib/python3.12/site-packages (from scikit-learn) (2.2.4)
Requirement already satisfied: scipy>=1.6.0 in ./.env/lib/python3.12/site-packages (from scikit-learn) (1.15.2)
Requirement already satisfied: joblib>=1.2.0 in ./.env/lib/python3.12/site-packages (from scikit-learn) (1.4.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in ./.env/lib/python3.12/site-packages (from scikit-learn) (3.6.0)
Requirement already satisfied: torch in ./.env/lib/python3.12/site-packages (from hf_mtask_trainer) (2.6.0)
Requirement already satisfied: transformers>=4.47.0 in ./.env/lib/python3.12/site-packages (from hf_mtask_trainer) (4.47.0)
Requirement already satisfied: accelerate in ./.env/lib/python3.12/site-packages (from hf_mtask_trainer) (1.6.0)
Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (0.30.1)
Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (2024.11.6)
Requirement already satisfied: requests in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (2.32.3)
Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (0.19.1)
Requirement already satisfied: safetensors>=0.4.1 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in ./.env/lib/python3.12/site-packages (from transformers>=4.47.0->hf_mtask_trainer) (4.67.1)
Requirement already satisfied: psutil in ./.env/lib/python3.12/site-packages (from accelerate->hf_mtask_trainer) (7.0.0)
Requirement already satisfied: typing-extensions>=4.10.0 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (4.13.1)
Requirement already satisfied: networkx in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (3.4.2)
Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (3.1.6)
Requirement already satisfied: fsspec in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (2024.12.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (12.4.127)
Requirement already satisfied: triton==3.2.0 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (3.2.0)
Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (78.1.0)
Requirement already satisfied: sympy==1.13.1 in ./.env/lib/python3.12/site-packages (from torch->hf_mtask_trainer) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.12/site-packages (from sympy==1.13.1->torch->hf_mtask_trainer) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch->hf_mtask_trainer) (3.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.12/site-packages (from requests->transformers>=4.47.0->hf_mtask_trainer) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.12/site-packages (from requests->transformers>=4.47.0->hf_mtask_trainer) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.12/site-packages (from requests->transformers>=4.47.0->hf_mtask_trainer) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.12/site-packages (from requests->transformers>=4.47.0->hf_mtask_trainer) (2025.1.31)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
Requirement already satisfied: seqeval in ./.env/lib/python3.12/site-packages (1.2.2)
Requirement already satisfied: levenshtein in ./.env/lib/python3.12/site-packages (0.27.1)
Requirement already satisfied: numpy>=1.14.0 in ./.env/lib/python3.12/site-packages (from seqeval) (2.2.4)
Requirement already satisfied: scikit-learn>=0.21.3 in ./.env/lib/python3.12/site-packages (from seqeval) (1.6.1)
Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in ./.env/lib/python3.12/site-packages (from levenshtein) (3.13.0)
Requirement already satisfied: scipy>=1.6.0 in ./.env/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)
Requirement already satisfied: joblib>=1.2.0 in ./.env/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in ./.env/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)

[notice] A new release of pip is available: 24.0 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
04/10/2025 18:49:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/10/2025 18:49:30 - INFO - __main__ - Training/evaluation parameters LoRATrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=3600,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=True,
eval_steps=200,
eval_strategy=IntervalStrategy.STEPS,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=True,
load_lora_from=None,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./test_run_outputs/runs/Apr10_18-49-30_uc3n081.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lora_config=./config/lora_config.json,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.INVERSE_SQRT,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=./test_run_outputs,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=./test_run_outputs,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=1,
seed=1,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=200,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-851ce74e92ca2b7e
Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
04/10/2025 18:49:30 - INFO - datasets.builder - Using custom data configuration default-851ce74e92ca2b7e
04/10/2025 18:49:30 - INFO - datasets.info - Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
04/10/2025 18:49:30 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/10/2025 18:49:30 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/10/2025 18:49:30 - INFO - datasets.builder - Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
04/10/2025 18:49:30 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:733] 2025-04-10 18:49:31,270 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:49:31,271 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
  "additional_loss_layer": 16,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": 1.0,
  "contrastive_loss_weight": 1.0,
  "contrastive_pooling_type": "mean",
  "distance_function": "cosine",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": false,
  "only_train_language_modeling": true,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": false,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2269] 2025-04-10 18:49:31,392 >> loading file tokenizer.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json
[INFO|tokenization_utils_base.py:2269] 2025-04-10 18:49:31,392 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2269] 2025-04-10 18:49:31,392 >> loading file special_tokens_map.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json
[INFO|tokenization_utils_base.py:2269] 2025-04-10 18:49:31,392 >> loading file tokenizer_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json
[INFO|tokenization_utils_base.py:2513] 2025-04-10 18:49:31,603 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
04/10/2025 18:49:31 - INFO - __main__ - Tokenizer is fast: True
[INFO|modeling_utils.py:3667] 2025-04-10 18:49:31,612 >> loading weights file model.safetensors from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json
[INFO|modeling_utils.py:1591] 2025-04-10 18:49:31,616 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1038] 2025-04-10 18:49:31,616 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
[INFO|modeling_utils.py:4499] 2025-04-10 18:49:36,229 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4507] 2025-04-10 18:49:36,229 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:993] 2025-04-10 18:49:36,350 >> loading configuration file generation_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json
[INFO|configuration_utils.py:1038] 2025-04-10 18:49:36,350 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

adding special tokens...
04/10/2025 18:49:36 - INFO - __main__ - ================ pad, eos, bos, unk, padding ================ 
04/10/2025 18:49:36 - INFO - __main__ - <|eot_id|>, 128009
04/10/2025 18:49:36 - INFO - __main__ - <|eot_id|>, 128009
04/10/2025 18:49:36 - INFO - __main__ - <|begin_of_text|>, 128000
04/10/2025 18:49:36 - INFO - __main__ - <|reserved_special_token_0|>, 128002
04/10/2025 18:49:36 - INFO - __main__ - right
04/10/2025 18:49:36 - INFO - __main__ - lora_r : 8
04/10/2025 18:49:36 - INFO - __main__ - lora_alpha : 16
04/10/2025 18:49:36 - INFO - __main__ - lora_dropout : 0.1
04/10/2025 18:49:36 - INFO - __main__ - lora_target_modules : ['q_proj', 'v_proj']
04/10/2025 18:49:36 - INFO - __main__ - LoRA configs: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
04/10/2025 18:49:36 - INFO - __main__ - block size: 2048
Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
04/10/2025 18:49:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
04/10/2025 18:49:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
04/10/2025 18:49:36 - INFO - __main__ - xxx: Showcase the tokenized training samples.
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[INFO|trainer.py:658] 2025-04-10 18:49:37,360 >> Using auto half precision backend
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
[2025-04-10 18:49:39,667] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|trainer.py:2145] 2025-04-10 18:49:42,552 >> ***** Running training *****
[INFO|trainer.py:2146] 2025-04-10 18:49:42,553 >>   Num examples = 392,702
[INFO|trainer.py:2147] 2025-04-10 18:49:42,553 >>   Num Epochs = 1
[INFO|trainer.py:2148] 2025-04-10 18:49:42,553 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2151] 2025-04-10 18:49:42,553 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2152] 2025-04-10 18:49:42,553 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2153] 2025-04-10 18:49:42,553 >>   Total optimization steps = 12,272
[INFO|trainer.py:2154] 2025-04-10 18:49:42,554 >>   Number of trainable parameters = 2,293,760
[INFO|trainer.py:3831] 2025-04-10 18:49:42,557 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 18:49:42,557 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 18:49:42,557 >>   Batch size = 32
{'eval_loss': 4.186171531677246, 'eval_accuracy': 0.13471370511988073, 'eval_runtime': 11.1119, 'eval_samples_per_second': 224.083, 'eval_steps_per_second': 7.019, 'epoch': 0}
[INFO|trainer.py:3831] 2025-04-10 18:51:28,700 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 18:51:28,701 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 18:51:28,701 >>   Batch size = 32
{'loss': 4.069, 'grad_norm': 5.3711042404174805, 'learning_rate': 1.3550135501355015e-05, 'epoch': 0.0008148631029986962}
{'loss': 3.8931, 'grad_norm': 6.799342155456543, 'learning_rate': 2.710027100271003e-05, 'epoch': 0.0016297262059973925}
{'loss': 3.0661, 'grad_norm': 7.968966007232666, 'learning_rate': 4.065040650406504e-05, 'epoch': 0.0024445893089960887}
{'loss': 1.3665, 'grad_norm': 5.947422027587891, 'learning_rate': 5.420054200542006e-05, 'epoch': 0.003259452411994785}
{'loss': 0.2514, 'grad_norm': 2.5701072216033936, 'learning_rate': 6.775067750677507e-05, 'epoch': 0.004074315514993481}
{'loss': 0.2002, 'grad_norm': 0.7639427185058594, 'learning_rate': 8.130081300813008e-05, 'epoch': 0.0048891786179921775}
{'loss': 0.1599, 'grad_norm': 1.262850046157837, 'learning_rate': 9.48509485094851e-05, 'epoch': 0.005704041720990874}
{'loss': 0.1577, 'grad_norm': 1.356672763824463, 'learning_rate': 0.00010840108401084012, 'epoch': 0.00651890482398957}
{'loss': 0.1546, 'grad_norm': 1.0716265439987183, 'learning_rate': 0.00012195121951219512, 'epoch': 0.007333767926988266}
{'loss': 0.1283, 'grad_norm': 0.8824846744537354, 'learning_rate': 0.00013550135501355014, 'epoch': 0.008148631029986962}
{'loss': 0.1139, 'grad_norm': 1.6368541717529297, 'learning_rate': 0.00014905149051490515, 'epoch': 0.008963494132985658}
{'loss': 0.1126, 'grad_norm': 1.2451423406600952, 'learning_rate': 0.00016260162601626016, 'epoch': 0.009778357235984355}
{'loss': 0.1129, 'grad_norm': 2.379960298538208, 'learning_rate': 0.00017615176151761518, 'epoch': 0.01059322033898305}
{'loss': 0.103, 'grad_norm': 1.4009037017822266, 'learning_rate': 0.0001897018970189702, 'epoch': 0.011408083441981747}
{'loss': 0.1181, 'grad_norm': 3.4304726123809814, 'learning_rate': 0.0002032520325203252, 'epoch': 0.012222946544980443}
{'loss': 0.0993, 'grad_norm': 1.0397613048553467, 'learning_rate': 0.00021680216802168024, 'epoch': 0.01303780964797914}
{'loss': 0.0883, 'grad_norm': 1.3769025802612305, 'learning_rate': 0.00023035230352303523, 'epoch': 0.013852672750977835}
{'loss': 0.1065, 'grad_norm': 1.1244748830795288, 'learning_rate': 0.00024390243902439024, 'epoch': 0.014667535853976532}
{'loss': 0.1029, 'grad_norm': 0.7811725735664368, 'learning_rate': 0.00025745257452574526, 'epoch': 0.015482398956975228}
{'loss': 0.0873, 'grad_norm': 1.437669038772583, 'learning_rate': 0.00027100271002710027, 'epoch': 0.016297262059973925}
[INFO|trainer.py:3515] 2025-04-10 18:51:39,065 >> Saving model checkpoint to ./test_run_outputs/checkpoint-200
[INFO|configuration_utils.py:733] 2025-04-10 18:51:39,827 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:51:39,828 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 18:51:39,848 >> tokenizer config file saved in ./test_run_outputs/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 18:51:39,849 >> Special tokens file saved in ./test_run_outputs/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 18:51:39,984 >> Deleting older checkpoint [test_run_outputs/checkpoint-1800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 18:51:40,244 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:51:40,245 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 18:53:14,562 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 18:53:14,563 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 18:53:14,563 >>   Batch size = 32
{'eval_loss': 0.08335176855325699, 'eval_accuracy': 0.15170622610201284, 'eval_runtime': 10.3661, 'eval_samples_per_second': 240.206, 'eval_steps_per_second': 7.525, 'epoch': 0.016297262059973925}
{'loss': 0.087, 'grad_norm': 0.5459930896759033, 'learning_rate': 0.0002845528455284553, 'epoch': 0.01711212516297262}
{'loss': 0.0964, 'grad_norm': 1.1822863817214966, 'learning_rate': 0.0002981029810298103, 'epoch': 0.017926988265971316}
{'loss': 0.0799, 'grad_norm': 0.9296766519546509, 'learning_rate': 0.0003116531165311653, 'epoch': 0.018741851368970015}
{'loss': 0.0887, 'grad_norm': 1.1004317998886108, 'learning_rate': 0.0003252032520325203, 'epoch': 0.01955671447196871}
{'loss': 0.0964, 'grad_norm': 0.7474788427352905, 'learning_rate': 0.00033875338753387534, 'epoch': 0.020371577574967405}
{'loss': 0.0736, 'grad_norm': 0.8607267737388611, 'learning_rate': 0.00035230352303523035, 'epoch': 0.0211864406779661}
{'loss': 0.0938, 'grad_norm': 1.0141544342041016, 'learning_rate': 0.00036585365853658537, 'epoch': 0.0220013037809648}
{'loss': 0.0876, 'grad_norm': 0.6067882180213928, 'learning_rate': 0.0003794037940379404, 'epoch': 0.022816166883963495}
{'loss': 0.0799, 'grad_norm': 0.8495304584503174, 'learning_rate': 0.0003929539295392954, 'epoch': 0.02363102998696219}
{'loss': 0.0812, 'grad_norm': 1.0046833753585815, 'learning_rate': 0.0004065040650406504, 'epoch': 0.024445893089960886}
{'loss': 0.1052, 'grad_norm': 0.7403861284255981, 'learning_rate': 0.0004200542005420054, 'epoch': 0.02526075619295958}
{'loss': 0.0753, 'grad_norm': 0.7999104857444763, 'learning_rate': 0.0004336043360433605, 'epoch': 0.02607561929595828}
{'loss': 0.0662, 'grad_norm': 0.5509985685348511, 'learning_rate': 0.00044715447154471545, 'epoch': 0.026890482398956975}
{'loss': 0.0811, 'grad_norm': 0.7341604232788086, 'learning_rate': 0.00046070460704607046, 'epoch': 0.02770534550195567}
{'loss': 0.0791, 'grad_norm': 0.7299050092697144, 'learning_rate': 0.00047425474254742553, 'epoch': 0.028520208604954366}
{'loss': 0.0786, 'grad_norm': 0.41779106855392456, 'learning_rate': 0.0004878048780487805, 'epoch': 0.029335071707953065}
{'loss': 0.0796, 'grad_norm': 0.5777068734169006, 'learning_rate': 0.0004993238671687188, 'epoch': 0.03014993481095176}
{'loss': 0.0749, 'grad_norm': 0.4610600769519806, 'learning_rate': 0.0004927100139988397, 'epoch': 0.030964797913950456}
{'loss': 0.0779, 'grad_norm': 0.49563053250312805, 'learning_rate': 0.00048635219906818713, 'epoch': 0.03177966101694915}
{'loss': 0.0762, 'grad_norm': 0.5929020643234253, 'learning_rate': 0.00048023431780746373, 'epoch': 0.03259452411994785}
[INFO|trainer.py:3515] 2025-04-10 18:53:24,934 >> Saving model checkpoint to ./test_run_outputs/checkpoint-400
[INFO|configuration_utils.py:733] 2025-04-10 18:53:25,183 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:53:25,184 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 18:53:25,204 >> tokenizer config file saved in ./test_run_outputs/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 18:53:25,205 >> Special tokens file saved in ./test_run_outputs/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 18:53:25,332 >> Deleting older checkpoint [test_run_outputs/checkpoint-200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 18:53:25,596 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:53:25,597 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 18:54:59,672 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 18:54:59,672 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 18:54:59,672 >>   Batch size = 32
{'eval_loss': 0.07102902233600616, 'eval_accuracy': 0.15174951302214848, 'eval_runtime': 10.3724, 'eval_samples_per_second': 240.06, 'eval_steps_per_second': 7.52, 'epoch': 0.03259452411994785}
{'loss': 0.0852, 'grad_norm': 0.6229436993598938, 'learning_rate': 0.0004743416490252569, 'epoch': 0.03340938722294654}
{'loss': 0.0705, 'grad_norm': 0.3705342710018158, 'learning_rate': 0.0004686607057806929, 'epoch': 0.03422425032594524}
{'loss': 0.0744, 'grad_norm': 0.369976282119751, 'learning_rate': 0.0004631791054451078, 'epoch': 0.03503911342894394}
{'loss': 0.0487, 'grad_norm': 0.3438034653663635, 'learning_rate': 0.00045788545610129497, 'epoch': 0.03585397653194263}
{'loss': 0.086, 'grad_norm': 0.5549997687339783, 'learning_rate': 0.0004527692569068709, 'epoch': 0.03666883963494133}
{'loss': 0.0678, 'grad_norm': 0.5600486993789673, 'learning_rate': 0.0004478208104374668, 'epoch': 0.03748370273794003}
{'loss': 0.0719, 'grad_norm': 0.4697161018848419, 'learning_rate': 0.0004430311453438469, 'epoch': 0.03829856584093872}
{'loss': 0.0737, 'grad_norm': 0.5351260304450989, 'learning_rate': 0.0004383919479187546, 'epoch': 0.03911342894393742}
{'loss': 0.0861, 'grad_norm': 0.5522578954696655, 'learning_rate': 0.00043389550138535544, 'epoch': 0.03992829204693611}
{'loss': 0.0692, 'grad_norm': 0.30207526683807373, 'learning_rate': 0.0004295346318982906, 'epoch': 0.04074315514993481}
{'loss': 0.0631, 'grad_norm': 0.6082266569137573, 'learning_rate': 0.00042530266039748265, 'epoch': 0.04155801825293351}
{'loss': 0.0715, 'grad_norm': 0.6498546004295349, 'learning_rate': 0.00042119335957947654, 'epoch': 0.0423728813559322}
{'loss': 0.0719, 'grad_norm': 0.6093307137489319, 'learning_rate': 0.0004172009153556412, 'epoch': 0.0431877444589309}
{'loss': 0.0758, 'grad_norm': 0.8770602345466614, 'learning_rate': 0.00041331989225457485, 'epoch': 0.0440026075619296}
{'loss': 0.0707, 'grad_norm': 0.2997080385684967, 'learning_rate': 0.0004095452023003965, 'epoch': 0.04481747066492829}
{'loss': 0.0603, 'grad_norm': 0.4353085458278656, 'learning_rate': 0.00040587207696162446, 'epoch': 0.04563233376792699}
{'loss': 0.0724, 'grad_norm': 0.4568563997745514, 'learning_rate': 0.00040229604181890474, 'epoch': 0.04644719687092568}
{'loss': 0.0796, 'grad_norm': 0.2836431860923767, 'learning_rate': 0.00039881289364554284, 'epoch': 0.04726205997392438}
{'loss': 0.0677, 'grad_norm': 0.3012627065181732, 'learning_rate': 0.00039541867963386587, 'epoch': 0.04807692307692308}
{'loss': 0.0607, 'grad_norm': 0.6553592681884766, 'learning_rate': 0.00039210967853395306, 'epoch': 0.04889178617992177}
[INFO|trainer.py:3515] 2025-04-10 18:55:10,048 >> Saving model checkpoint to ./test_run_outputs/checkpoint-600
[INFO|configuration_utils.py:733] 2025-04-10 18:55:10,339 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:55:10,340 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 18:55:10,360 >> tokenizer config file saved in ./test_run_outputs/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 18:55:10,362 >> Special tokens file saved in ./test_run_outputs/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 18:55:10,488 >> Deleting older checkpoint [test_run_outputs/checkpoint-400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 18:55:10,842 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:55:10,843 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 18:56:45,103 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 18:56:45,103 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 18:56:45,103 >>   Batch size = 32
{'eval_loss': 0.05907154083251953, 'eval_accuracy': 0.15191063655820888, 'eval_runtime': 10.3768, 'eval_samples_per_second': 239.958, 'eval_steps_per_second': 7.517, 'epoch': 0.04889178617992177}
{'loss': 0.0621, 'grad_norm': 0.4797149896621704, 'learning_rate': 0.0003888823835001032, 'epoch': 0.04970664928292047}
{'loss': 0.0784, 'grad_norm': 0.45698344707489014, 'learning_rate': 0.00038573348646526036, 'epoch': 0.05052151238591916}
{'loss': 0.0676, 'grad_norm': 0.4649524390697479, 'learning_rate': 0.0003826598638851107, 'epoch': 0.05133637548891786}
{'loss': 0.0725, 'grad_norm': 0.4451313018798828, 'learning_rate': 0.00037965856371218604, 'epoch': 0.05215123859191656}
{'loss': 0.0623, 'grad_norm': 0.5466156005859375, 'learning_rate': 0.00037672679347648864, 'epoch': 0.05296610169491525}
{'loss': 0.068, 'grad_norm': 0.29514577984809875, 'learning_rate': 0.00037386190936323973, 'epoch': 0.05378096479791395}
{'loss': 0.0698, 'grad_norm': 0.4300667941570282, 'learning_rate': 0.0003710614061906453, 'epoch': 0.05459582790091265}
{'loss': 0.0776, 'grad_norm': 0.43991580605506897, 'learning_rate': 0.0003683229082013259, 'epoch': 0.05541069100391134}
{'loss': 0.0686, 'grad_norm': 0.4141886532306671, 'learning_rate': 0.00036564416059047493, 'epoch': 0.05622555410691004}
{'loss': 0.0621, 'grad_norm': 0.4662877023220062, 'learning_rate': 0.0003630230217020875, 'epoch': 0.05704041720990873}
{'loss': 0.0718, 'grad_norm': 0.4108864367008209, 'learning_rate': 0.0003604574558318759, 'epoch': 0.05785528031290743}
{'loss': 0.0667, 'grad_norm': 0.4355160892009735, 'learning_rate': 0.00035794552658190886, 'epoch': 0.05867014341590613}
{'loss': 0.0526, 'grad_norm': 0.22928954660892487, 'learning_rate': 0.0003554853907176758, 'epoch': 0.05948500651890482}
{'loss': 0.0514, 'grad_norm': 0.4453262686729431, 'learning_rate': 0.00035307529248329193, 'epoch': 0.06029986962190352}
{'loss': 0.0627, 'grad_norm': 0.7355374693870544, 'learning_rate': 0.0003507135583350036, 'epoch': 0.06111473272490222}
{'loss': 0.0654, 'grad_norm': 0.5034920573234558, 'learning_rate': 0.0003483985920570983, 'epoch': 0.06192959582790091}
{'loss': 0.0802, 'grad_norm': 0.3751084506511688, 'learning_rate': 0.00034612887022783116, 'epoch': 0.06274445893089961}
{'loss': 0.0702, 'grad_norm': 0.4969881772994995, 'learning_rate': 0.0003439029380061048, 'epoch': 0.0635593220338983}
{'loss': 0.0546, 'grad_norm': 0.3381989598274231, 'learning_rate': 0.00034171940521242594, 'epoch': 0.06437418513689701}
{'loss': 0.0578, 'grad_norm': 0.5160276889801025, 'learning_rate': 0.00033957694268015314, 'epoch': 0.0651890482398957}
[INFO|trainer.py:3515] 2025-04-10 18:56:55,513 >> Saving model checkpoint to ./test_run_outputs/checkpoint-800
[INFO|configuration_utils.py:733] 2025-04-10 18:56:55,766 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:56:55,766 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 18:56:55,787 >> tokenizer config file saved in ./test_run_outputs/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 18:56:55,788 >> Special tokens file saved in ./test_run_outputs/checkpoint-800/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 18:56:55,914 >> Deleting older checkpoint [test_run_outputs/checkpoint-800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 18:56:56,168 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:56:56,168 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 18:58:29,991 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 18:58:29,991 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 18:58:29,991 >>   Batch size = 32
{'eval_loss': 0.07495006918907166, 'eval_accuracy': 0.151763941995527, 'eval_runtime': 10.411, 'eval_samples_per_second': 239.17, 'eval_steps_per_second': 7.492, 'epoch': 0.0651890482398957}
{'loss': 0.072, 'grad_norm': 0.3927404284477234, 'learning_rate': 0.0003374742788552765, 'epoch': 0.06600391134289439}
{'loss': 0.065, 'grad_norm': 0.5435999035835266, 'learning_rate': 0.0003354101966249685, 'epoch': 0.06681877444589308}
{'loss': 0.0588, 'grad_norm': 0.46560022234916687, 'learning_rate': 0.0003333835303569344, 'epoch': 0.06763363754889179}
{'loss': 0.0524, 'grad_norm': 0.12296764552593231, 'learning_rate': 0.00033139316313320126, 'epoch': 0.06844850065189048}
{'loss': 0.0694, 'grad_norm': 0.3882589638233185, 'learning_rate': 0.0003294380241634318, 'epoch': 0.06926336375488917}
{'loss': 0.051, 'grad_norm': 0.4192541539669037, 'learning_rate': 0.00032751708636415464, 'epoch': 0.07007822685788788}
{'loss': 0.0566, 'grad_norm': 0.8306898474693298, 'learning_rate': 0.0003256293640914785, 'epoch': 0.07089308996088657}
{'loss': 0.0545, 'grad_norm': 0.49497532844543457, 'learning_rate': 0.00032377391101592087, 'epoch': 0.07170795306388526}
{'loss': 0.0698, 'grad_norm': 0.3167041838169098, 'learning_rate': 0.00032194981812894136, 'epoch': 0.07252281616688397}
{'loss': 0.0808, 'grad_norm': 0.5971222519874573, 'learning_rate': 0.00032015621187164245, 'epoch': 0.07333767926988266}
{'loss': 0.0691, 'grad_norm': 0.6209506392478943, 'learning_rate': 0.00031839225237688553, 'epoch': 0.07415254237288135}
{'loss': 0.064, 'grad_norm': 0.32074370980262756, 'learning_rate': 0.00031665713181678825, 'epoch': 0.07496740547588006}
{'loss': 0.0628, 'grad_norm': 0.3421691656112671, 'learning_rate': 0.0003149500728482163, 'epoch': 0.07578226857887875}
{'loss': 0.0697, 'grad_norm': 0.5150133967399597, 'learning_rate': 0.0003132703271494771, 'epoch': 0.07659713168187744}
{'loss': 0.0498, 'grad_norm': 0.2987039089202881, 'learning_rate': 0.00031161717404195607, 'epoch': 0.07741199478487613}
{'loss': 0.0643, 'grad_norm': 0.5100870132446289, 'learning_rate': 0.0003099899191909311, 'epoch': 0.07822685788787484}
{'loss': 0.0654, 'grad_norm': 0.42000120878219604, 'learning_rate': 0.0003083878933802447, 'epoch': 0.07904172099087353}
{'loss': 0.049, 'grad_norm': 0.6121360659599304, 'learning_rate': 0.0003068104513559219, 'epoch': 0.07985658409387222}
{'loss': 0.0622, 'grad_norm': 0.7551648020744324, 'learning_rate': 0.0003052569707341966, 'epoch': 0.08067144719687093}
{'loss': 0.0577, 'grad_norm': 0.6374449133872986, 'learning_rate': 0.00030372685096974886, 'epoch': 0.08148631029986962}
[INFO|trainer.py:3515] 2025-04-10 18:58:40,412 >> Saving model checkpoint to ./test_run_outputs/checkpoint-1000
[INFO|configuration_utils.py:733] 2025-04-10 18:58:40,664 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:58:40,665 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 18:58:40,685 >> tokenizer config file saved in ./test_run_outputs/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 18:58:40,687 >> Special tokens file saved in ./test_run_outputs/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 18:58:40,813 >> Deleting older checkpoint [test_run_outputs/checkpoint-600] due to args.save_total_limit
[INFO|trainer.py:3607] 2025-04-10 18:58:40,828 >> Deleting older checkpoint [test_run_outputs/checkpoint-800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 18:58:41,086 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 18:58:41,087 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:00:15,200 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:00:15,200 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:00:15,200 >>   Batch size = 32
{'eval_loss': 0.05548730120062828, 'eval_accuracy': 0.1520044248851694, 'eval_runtime': 10.4226, 'eval_samples_per_second': 238.904, 'eval_steps_per_second': 7.484, 'epoch': 0.08148631029986962}
{'loss': 0.0696, 'grad_norm': 0.5623803734779358, 'learning_rate': 0.00030221951238026696, 'epoch': 0.08230117340286831}
{'loss': 0.0711, 'grad_norm': 0.4608232378959656, 'learning_rate': 0.0003007343952237393, 'epoch': 0.08311603650586702}
{'loss': 0.0728, 'grad_norm': 0.40778058767318726, 'learning_rate': 0.00029927095882513645, 'epoch': 0.08393089960886571}
{'loss': 0.0476, 'grad_norm': 0.41435304284095764, 'learning_rate': 0.00029782868074939173, 'epoch': 0.0847457627118644}
{'loss': 0.0422, 'grad_norm': 0.4544229209423065, 'learning_rate': 0.0002964070560178061, 'epoch': 0.08556062581486311}
{'loss': 0.0664, 'grad_norm': 0.7318626046180725, 'learning_rate': 0.0002950055963652087, 'epoch': 0.0863754889178618}
{'loss': 0.0576, 'grad_norm': 0.4824035167694092, 'learning_rate': 0.0002936238295353905, 'epoch': 0.08719035202086049}
{'loss': 0.0439, 'grad_norm': 0.24151553213596344, 'learning_rate': 0.00029226129861250304, 'epoch': 0.0880052151238592}
{'loss': 0.0696, 'grad_norm': 0.2206316590309143, 'learning_rate': 0.0002909175613862728, 'epoch': 0.08882007822685789}
{'loss': 0.062, 'grad_norm': 0.3557039499282837, 'learning_rate': 0.00028959218974902685, 'epoch': 0.08963494132985658}
{'loss': 0.0637, 'grad_norm': 0.36738815903663635, 'learning_rate': 0.00028828476912266473, 'epoch': 0.09044980443285527}
{'loss': 0.0585, 'grad_norm': 0.2468843311071396, 'learning_rate': 0.00028699489791383296, 'epoch': 0.09126466753585398}
{'loss': 0.0565, 'grad_norm': 0.47030726075172424, 'learning_rate': 0.00028572218699567754, 'epoch': 0.09207953063885267}
{'loss': 0.0494, 'grad_norm': 0.2564672827720642, 'learning_rate': 0.0002844662592146544, 'epoch': 0.09289439374185136}
{'loss': 0.0671, 'grad_norm': 0.648134708404541, 'learning_rate': 0.00028322674892098, 'epoch': 0.09370925684485007}
{'loss': 0.0622, 'grad_norm': 0.37573784589767456, 'learning_rate': 0.0002820033015213927, 'epoch': 0.09452411994784876}
{'loss': 0.053, 'grad_norm': 0.2564205527305603, 'learning_rate': 0.0002807955730529843, 'epoch': 0.09533898305084745}
{'loss': 0.0612, 'grad_norm': 0.3861939609050751, 'learning_rate': 0.0002796032297769375, 'epoch': 0.09615384615384616}
{'loss': 0.0687, 'grad_norm': 0.43176478147506714, 'learning_rate': 0.00027842594779108027, 'epoch': 0.09696870925684485}
{'loss': 0.0523, 'grad_norm': 0.3518138527870178, 'learning_rate': 0.0002772634126602354, 'epoch': 0.09778357235984354}
[INFO|trainer.py:3515] 2025-04-10 19:00:25,567 >> Saving model checkpoint to ./test_run_outputs/checkpoint-1200
[INFO|configuration_utils.py:733] 2025-04-10 19:00:25,808 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:00:25,809 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:00:25,829 >> tokenizer config file saved in ./test_run_outputs/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:00:25,831 >> Special tokens file saved in ./test_run_outputs/checkpoint-1200/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:00:25,955 >> Deleting older checkpoint [test_run_outputs/checkpoint-1200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:00:26,204 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:00:26,205 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:01:59,326 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:01:59,327 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:01:59,327 >>   Batch size = 32
{'eval_loss': 0.06315318495035172, 'eval_accuracy': 0.15187456412476252, 'eval_runtime': 10.3683, 'eval_samples_per_second': 240.155, 'eval_steps_per_second': 7.523, 'epoch': 0.09778357235984354}
{'loss': 0.06, 'grad_norm': 0.1155071035027504, 'learning_rate': 0.000276115319063408, 'epoch': 0.09859843546284225}
{'loss': 0.0598, 'grad_norm': 0.5356249809265137, 'learning_rate': 0.0002749813704569105, 'epoch': 0.09941329856584094}
{'loss': 0.0626, 'grad_norm': 0.3247694671154022, 'learning_rate': 0.00027386127875258305, 'epoch': 0.10022816166883963}
{'loss': 0.0493, 'grad_norm': 0.44022542238235474, 'learning_rate': 0.0002727547640103149, 'epoch': 0.10104302477183832}
{'loss': 0.0624, 'grad_norm': 0.5098254680633545, 'learning_rate': 0.00027166155414412254, 'epoch': 0.10185788787483703}
{'loss': 0.0682, 'grad_norm': 0.2467552274465561, 'learning_rate': 0.000270581384641083, 'epoch': 0.10267275097783572}
{'loss': 0.0546, 'grad_norm': 0.3688707649707794, 'learning_rate': 0.00026951399829246446, 'epoch': 0.10348761408083441}
{'loss': 0.065, 'grad_norm': 0.3363821506500244, 'learning_rate': 0.00026845914493643164, 'epoch': 0.10430247718383312}
{'loss': 0.057, 'grad_norm': 0.24449358880519867, 'learning_rate': 0.000267416581211743, 'epoch': 0.10511734028683181}
{'loss': 0.0515, 'grad_norm': 0.3169960379600525, 'learning_rate': 0.0002663860703218891, 'epoch': 0.1059322033898305}
{'loss': 0.0582, 'grad_norm': 0.512930154800415, 'learning_rate': 0.000265367381809152, 'epoch': 0.10674706649282921}
{'loss': 0.0597, 'grad_norm': 0.5687573552131653, 'learning_rate': 0.0002643602913380972, 'epoch': 0.1075619295958279}
{'loss': 0.0703, 'grad_norm': 0.461920827627182, 'learning_rate': 0.0002633645804880358, 'epoch': 0.1083767926988266}
{'loss': 0.0524, 'grad_norm': 0.3389383852481842, 'learning_rate': 0.0002623800365540213, 'epoch': 0.1091916558018253}
{'loss': 0.0526, 'grad_norm': 0.49030059576034546, 'learning_rate': 0.0002614064523559687, 'epoch': 0.11000651890482399}
{'loss': 0.0536, 'grad_norm': 0.3121575713157654, 'learning_rate': 0.0002604436260555078, 'epoch': 0.11082138200782268}
{'loss': 0.0535, 'grad_norm': 0.4556879699230194, 'learning_rate': 0.0002594913609802023, 'epoch': 0.11163624511082139}
{'loss': 0.0791, 'grad_norm': 0.5842474699020386, 'learning_rate': 0.00025854946545478784, 'epoch': 0.11245110821382008}
{'loss': 0.0634, 'grad_norm': 0.6348283886909485, 'learning_rate': 0.0002576177526390993, 'epoch': 0.11326597131681877}
{'loss': 0.0612, 'grad_norm': 0.3970174491405487, 'learning_rate': 0.00025669604037237726, 'epoch': 0.11408083441981746}
[INFO|trainer.py:3515] 2025-04-10 19:02:09,707 >> Saving model checkpoint to ./test_run_outputs/checkpoint-1400
[INFO|configuration_utils.py:733] 2025-04-10 19:02:09,956 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:02:09,957 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:02:09,979 >> tokenizer config file saved in ./test_run_outputs/checkpoint-1400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:02:09,980 >> Special tokens file saved in ./test_run_outputs/checkpoint-1400/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:02:10,108 >> Deleting older checkpoint [test_run_outputs/checkpoint-1200] due to args.save_total_limit
[INFO|trainer.py:3607] 2025-04-10 19:02:10,112 >> Deleting older checkpoint [test_run_outputs/checkpoint-1400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:02:10,366 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:02:10,367 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:03:45,073 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:03:45,073 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:03:45,073 >>   Batch size = 32
{'eval_loss': 0.06370875239372253, 'eval_accuracy': 0.15182646754683404, 'eval_runtime': 10.381, 'eval_samples_per_second': 239.862, 'eval_steps_per_second': 7.514, 'epoch': 0.11408083441981746}
{'loss': 0.0573, 'grad_norm': 0.32519084215164185, 'learning_rate': 0.00025578415102365826, 'epoch': 0.11489569752281617}
{'loss': 0.0556, 'grad_norm': 0.34473544359207153, 'learning_rate': 0.00025488191134796984, 'epoch': 0.11571056062581486}
{'loss': 0.0644, 'grad_norm': 0.40460875630378723, 'learning_rate': 0.00025398915234806687, 'epoch': 0.11652542372881355}
{'loss': 0.0667, 'grad_norm': 0.2878745496273041, 'learning_rate': 0.00025310570914145736, 'epoch': 0.11734028683181226}
{'loss': 0.0486, 'grad_norm': 0.23535671830177307, 'learning_rate': 0.00025223142083248155, 'epoch': 0.11815514993481095}
{'loss': 0.0589, 'grad_norm': 0.7113186120986938, 'learning_rate': 0.00025136613038921797, 'epoch': 0.11897001303780964}
{'loss': 0.0541, 'grad_norm': 0.36907631158828735, 'learning_rate': 0.0002505096845250026, 'epoch': 0.11978487614080835}
{'loss': 0.0728, 'grad_norm': 0.3337663412094116, 'learning_rate': 0.0002496619335843594, 'epoch': 0.12059973924380704}
{'loss': 0.0468, 'grad_norm': 0.5637889504432678, 'learning_rate': 0.0002488227314331477, 'epoch': 0.12141460234680573}
{'loss': 0.0622, 'grad_norm': 0.43088585138320923, 'learning_rate': 0.0002479919353527449, 'epoch': 0.12222946544980444}
{'loss': 0.059, 'grad_norm': 0.4223102331161499, 'learning_rate': 0.00024716940593808956, 'epoch': 0.12304432855280313}
{'loss': 0.0626, 'grad_norm': 0.33920007944107056, 'learning_rate': 0.00024635500699941984, 'epoch': 0.12385919165580182}
{'loss': 0.0503, 'grad_norm': 0.23829349875450134, 'learning_rate': 0.0002455486054675506, 'epoch': 0.12467405475880051}
{'loss': 0.0751, 'grad_norm': 0.43890586495399475, 'learning_rate': 0.00024475007130253794, 'epoch': 0.12548891786179922}
{'loss': 0.0634, 'grad_norm': 0.39882591366767883, 'learning_rate': 0.00024395927740559093, 'epoch': 0.1263037809647979}
{'loss': 0.0523, 'grad_norm': 0.40269899368286133, 'learning_rate': 0.00024317609953409356, 'epoch': 0.1271186440677966}
{'loss': 0.0631, 'grad_norm': 0.2916545867919922, 'learning_rate': 0.0002424004162196086, 'epoch': 0.1279335071707953}
{'loss': 0.0573, 'grad_norm': 0.19368162751197815, 'learning_rate': 0.0002416321086887401, 'epoch': 0.12874837027379402}
{'loss': 0.0541, 'grad_norm': 0.19876112043857574, 'learning_rate': 0.00024087106078673776, 'epoch': 0.1295632333767927}
{'loss': 0.0485, 'grad_norm': 0.3166668713092804, 'learning_rate': 0.00024011715890373187, 'epoch': 0.1303780964797914}
[INFO|trainer.py:3515] 2025-04-10 19:03:55,501 >> Saving model checkpoint to ./test_run_outputs/checkpoint-1600
[INFO|configuration_utils.py:733] 2025-04-10 19:03:55,838 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:03:55,839 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:03:55,866 >> tokenizer config file saved in ./test_run_outputs/checkpoint-1600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:03:55,867 >> Special tokens file saved in ./test_run_outputs/checkpoint-1600/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:03:55,995 >> Deleting older checkpoint [test_run_outputs/checkpoint-1400] due to args.save_total_limit
[INFO|trainer.py:3607] 2025-04-10 19:03:55,999 >> Deleting older checkpoint [test_run_outputs/checkpoint-1600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:03:56,254 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:03:56,255 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:05:30,331 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:05:30,332 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:05:30,332 >>   Batch size = 32
{'eval_loss': 0.05771418288350105, 'eval_accuracy': 0.1519611379650338, 'eval_runtime': 10.4288, 'eval_samples_per_second': 238.761, 'eval_steps_per_second': 7.479, 'epoch': 0.1303780964797914}
{'loss': 0.0517, 'grad_norm': 0.30782389640808105, 'learning_rate': 0.00023937029190349148, 'epoch': 0.1311929595827901}
{'loss': 0.0685, 'grad_norm': 0.3871415853500366, 'learning_rate': 0.00023863035105460586, 'epoch': 0.13200782268578878}
{'loss': 0.0652, 'grad_norm': 0.5273477435112, 'learning_rate': 0.0002378972299639907, 'epoch': 0.13282268578878748}
{'loss': 0.0603, 'grad_norm': 0.31898656487464905, 'learning_rate': 0.00023717082451262845, 'epoch': 0.13363754889178617}
{'loss': 0.0648, 'grad_norm': 0.49392908811569214, 'learning_rate': 0.0002364510327934537, 'epoch': 0.1344524119947849}
{'loss': 0.0452, 'grad_norm': 0.14330580830574036, 'learning_rate': 0.00023573775505129956, 'epoch': 0.13526727509778358}
{'loss': 0.0571, 'grad_norm': 0.4328843355178833, 'learning_rate': 0.00023503089362482505, 'epoch': 0.13608213820078227}
{'loss': 0.0663, 'grad_norm': 0.40068596601486206, 'learning_rate': 0.00023433035289034646, 'epoch': 0.13689700130378096}
{'loss': 0.0591, 'grad_norm': 0.2794678807258606, 'learning_rate': 0.00023363603920749913, 'epoch': 0.13771186440677965}
{'loss': 0.0661, 'grad_norm': 0.3362908959388733, 'learning_rate': 0.00023294786086666035, 'epoch': 0.13852672750977835}
{'loss': 0.0547, 'grad_norm': 0.378947913646698, 'learning_rate': 0.00023226572803806554, 'epoch': 0.13934159061277707}
{'loss': 0.0481, 'grad_norm': 0.5107226967811584, 'learning_rate': 0.0002315895527225539, 'epoch': 0.14015645371577576}
{'loss': 0.0513, 'grad_norm': 0.24295660853385925, 'learning_rate': 0.00023091924870388204, 'epoch': 0.14097131681877445}
{'loss': 0.0765, 'grad_norm': 0.45189791917800903, 'learning_rate': 0.0002302547315025477, 'epoch': 0.14178617992177314}
{'loss': 0.0543, 'grad_norm': 0.28606992959976196, 'learning_rate': 0.0002295959183310664, 'epoch': 0.14260104302477183}
{'loss': 0.0575, 'grad_norm': 0.36891964077949524, 'learning_rate': 0.00022894272805064748, 'epoch': 0.14341590612777053}
{'loss': 0.0546, 'grad_norm': 0.29565396904945374, 'learning_rate': 0.00022829508112921882, 'epoch': 0.14423076923076922}
{'loss': 0.0721, 'grad_norm': 0.32624661922454834, 'learning_rate': 0.00022765289960075013, 'epoch': 0.14504563233376794}
{'loss': 0.0598, 'grad_norm': 0.420706570148468, 'learning_rate': 0.0002270161070258276, 'epoch': 0.14586049543676663}
{'loss': 0.0589, 'grad_norm': 0.39655807614326477, 'learning_rate': 0.00022638462845343544, 'epoch': 0.14667535853976532}
[INFO|trainer.py:3515] 2025-04-10 19:05:40,735 >> Saving model checkpoint to ./test_run_outputs/checkpoint-1800
[INFO|configuration_utils.py:733] 2025-04-10 19:05:41,001 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:05:41,002 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:05:41,022 >> tokenizer config file saved in ./test_run_outputs/checkpoint-1800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:05:41,024 >> Special tokens file saved in ./test_run_outputs/checkpoint-1800/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:05:41,150 >> Deleting older checkpoint [test_run_outputs/checkpoint-1000] due to args.save_total_limit
[INFO|trainer.py:3607] 2025-04-10 19:05:41,164 >> Deleting older checkpoint [test_run_outputs/checkpoint-1600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:05:41,454 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:05:41,455 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:07:16,454 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:07:16,454 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:07:16,454 >>   Batch size = 32
{'eval_loss': 0.05195078253746033, 'eval_accuracy': 0.1520525214630979, 'eval_runtime': 10.4041, 'eval_samples_per_second': 239.329, 'eval_steps_per_second': 7.497, 'epoch': 0.14667535853976532}
{'loss': 0.0706, 'grad_norm': 0.5619696378707886, 'learning_rate': 0.00022575839038389974, 'epoch': 0.147490221642764}
{'loss': 0.0604, 'grad_norm': 0.35736802220344543, 'learning_rate': 0.00022513732073295442, 'epoch': 0.1483050847457627}
{'loss': 0.0475, 'grad_norm': 0.22269049286842346, 'learning_rate': 0.00022452134879688784, 'epoch': 0.1491199478487614}
{'loss': 0.0604, 'grad_norm': 0.40380221605300903, 'learning_rate': 0.0002239104052187334, 'epoch': 0.14993481095176012}
{'loss': 0.0641, 'grad_norm': 0.4674174189567566, 'learning_rate': 0.0002233044219554661, 'epoch': 0.1507496740547588}
{'loss': 0.0556, 'grad_norm': 0.350780725479126, 'learning_rate': 0.0002227033322461709, 'epoch': 0.1515645371577575}
{'loss': 0.0517, 'grad_norm': 0.24479278922080994, 'learning_rate': 0.00022210707058114792, 'epoch': 0.1523794002607562}
{'loss': 0.0579, 'grad_norm': 0.2986384928226471, 'learning_rate': 0.00022151557267192346, 'epoch': 0.15319426336375488}
{'loss': 0.0557, 'grad_norm': 0.26948443055152893, 'learning_rate': 0.00022092877542213423, 'epoch': 0.15400912646675358}
{'loss': 0.0629, 'grad_norm': 0.7195749282836914, 'learning_rate': 0.00022034661689925573, 'epoch': 0.15482398956975227}
{'loss': 0.0502, 'grad_norm': 0.29662221670150757, 'learning_rate': 0.00021976903630714618, 'epoch': 0.155638852672751}
{'loss': 0.0524, 'grad_norm': 0.4446738064289093, 'learning_rate': 0.0002191959739593773, 'epoch': 0.15645371577574968}
{'loss': 0.0365, 'grad_norm': 0.4598988890647888, 'learning_rate': 0.0002186273712533266, 'epoch': 0.15726857887874837}
{'loss': 0.0719, 'grad_norm': 0.40634235739707947, 'learning_rate': 0.00021806317064500505, 'epoch': 0.15808344198174706}
{'loss': 0.0506, 'grad_norm': 0.3520606458187103, 'learning_rate': 0.0002175033156245953, 'epoch': 0.15889830508474576}
{'loss': 0.0564, 'grad_norm': 0.5844467282295227, 'learning_rate': 0.00021694775069267772, 'epoch': 0.15971316818774445}
{'loss': 0.0587, 'grad_norm': 0.32789096236228943, 'learning_rate': 0.00021639642133712076, 'epoch': 0.16052803129074317}
{'loss': 0.0532, 'grad_norm': 0.20949013531208038, 'learning_rate': 0.00021584927401061395, 'epoch': 0.16134289439374186}
{'loss': 0.0595, 'grad_norm': 0.38923174142837524, 'learning_rate': 0.00021530625610882278, 'epoch': 0.16215775749674055}
{'loss': 0.0536, 'grad_norm': 0.5004670023918152, 'learning_rate': 0.0002147673159491453, 'epoch': 0.16297262059973924}
[INFO|trainer.py:3515] 2025-04-10 19:07:26,821 >> Saving model checkpoint to ./test_run_outputs/checkpoint-2000
[INFO|configuration_utils.py:733] 2025-04-10 19:07:27,212 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:07:27,213 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:07:27,234 >> tokenizer config file saved in ./test_run_outputs/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:07:27,235 >> Special tokens file saved in ./test_run_outputs/checkpoint-2000/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:07:27,364 >> Deleting older checkpoint [test_run_outputs/checkpoint-2000] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:07:27,742 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:07:27,743 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:09:00,954 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:09:00,955 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:09:00,955 >>   Batch size = 32
{'eval_loss': 0.05678869038820267, 'eval_accuracy': 0.15197797176730876, 'eval_runtime': 10.3679, 'eval_samples_per_second': 240.165, 'eval_steps_per_second': 7.523, 'epoch': 0.16297262059973924}
{'loss': 0.045, 'grad_norm': 0.7637231349945068, 'learning_rate': 0.00021423240275005018, 'epoch': 0.16378748370273793}
{'loss': 0.07, 'grad_norm': 0.34879881143569946, 'learning_rate': 0.00021370146661097854, 'epoch': 0.16460234680573663}
{'loss': 0.0508, 'grad_norm': 0.3081854581832886, 'learning_rate': 0.0002131744584927908, 'epoch': 0.16541720990873532}
{'loss': 0.0519, 'grad_norm': 0.22918979823589325, 'learning_rate': 0.00021265133019874132, 'epoch': 0.16623207301173404}
{'loss': 0.0544, 'grad_norm': 0.3556205928325653, 'learning_rate': 0.00021213203435596425, 'epoch': 0.16704693611473273}
{'loss': 0.0579, 'grad_norm': 0.43552935123443604, 'learning_rate': 0.000211616524397454, 'epoch': 0.16786179921773142}
{'loss': 0.0541, 'grad_norm': 0.3287963271141052, 'learning_rate': 0.00021110475454452548, 'epoch': 0.16867666232073011}
{'loss': 0.0597, 'grad_norm': 0.4454266130924225, 'learning_rate': 0.00021059667978973827, 'epoch': 0.1694915254237288}
{'loss': 0.0547, 'grad_norm': 0.24872806668281555, 'learning_rate': 0.00021009225588027093, 'epoch': 0.1703063885267275}
{'loss': 0.0505, 'grad_norm': 0.2958749830722809, 'learning_rate': 0.00020959143930173157, 'epoch': 0.17112125162972622}
{'loss': 0.0661, 'grad_norm': 0.3589012026786804, 'learning_rate': 0.00020909418726239003, 'epoch': 0.1719361147327249}
{'loss': 0.057, 'grad_norm': 0.26009222865104675, 'learning_rate': 0.0002086004576778206, 'epoch': 0.1727509778357236}
{'loss': 0.0414, 'grad_norm': 0.1786322295665741, 'learning_rate': 0.00020811020915594117, 'epoch': 0.1735658409387223}
{'loss': 0.074, 'grad_norm': 0.3805890381336212, 'learning_rate': 0.00020762340098243748, 'epoch': 0.17438070404172099}
{'loss': 0.0505, 'grad_norm': 0.4677262008190155, 'learning_rate': 0.00020713999310656076, 'epoch': 0.17519556714471968}
{'loss': 0.0597, 'grad_norm': 0.25665605068206787, 'learning_rate': 0.00020665994612728742, 'epoch': 0.1760104302477184}
{'loss': 0.0606, 'grad_norm': 0.3981466293334961, 'learning_rate': 0.00020618322127982967, 'epoch': 0.1768252933507171}
{'loss': 0.0638, 'grad_norm': 0.20093704760074615, 'learning_rate': 0.0002057097804224872, 'epoch': 0.17764015645371578}
{'loss': 0.0613, 'grad_norm': 0.31450411677360535, 'learning_rate': 0.00020523958602382944, 'epoch': 0.17845501955671447}
{'loss': 0.0536, 'grad_norm': 0.3897976577281952, 'learning_rate': 0.00020477260115019826, 'epoch': 0.17926988265971316}
[INFO|trainer.py:3515] 2025-04-10 19:09:11,329 >> Saving model checkpoint to ./test_run_outputs/checkpoint-2200
[INFO|configuration_utils.py:733] 2025-04-10 19:09:11,743 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:09:11,744 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:09:11,765 >> tokenizer config file saved in ./test_run_outputs/checkpoint-2200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:09:11,767 >> Special tokens file saved in ./test_run_outputs/checkpoint-2200/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:09:11,896 >> Deleting older checkpoint [test_run_outputs/checkpoint-2000] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:09:12,309 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:09:12,310 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:10:47,600 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:10:47,600 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:10:47,600 >>   Batch size = 32
{'eval_loss': 0.05497893691062927, 'eval_accuracy': 0.15198999591179088, 'eval_runtime': 10.3762, 'eval_samples_per_second': 239.972, 'eval_steps_per_second': 7.517, 'epoch': 0.17926988265971316}
{'loss': 0.0511, 'grad_norm': 0.3807816505432129, 'learning_rate': 0.00020430878945352254, 'epoch': 0.18008474576271186}
{'loss': 0.0548, 'grad_norm': 0.22774673998355865, 'learning_rate': 0.00020384811515943447, 'epoch': 0.18089960886571055}
{'loss': 0.0602, 'grad_norm': 0.4187609553337097, 'learning_rate': 0.00020339054305567973, 'epoch': 0.18171447196870927}
{'loss': 0.0666, 'grad_norm': 0.43388256430625916, 'learning_rate': 0.00020293603848081223, 'epoch': 0.18252933507170796}
{'loss': 0.0615, 'grad_norm': 0.2472914159297943, 'learning_rate': 0.00020248456731316585, 'epoch': 0.18334419817470665}
{'loss': 0.0491, 'grad_norm': 0.24350206553936005, 'learning_rate': 0.00020203609596009436, 'epoch': 0.18415906127770534}
{'loss': 0.0727, 'grad_norm': 0.3415857255458832, 'learning_rate': 0.0002015905913474728, 'epoch': 0.18497392438070404}
{'loss': 0.0583, 'grad_norm': 0.2632596492767334, 'learning_rate': 0.00020114802090945237, 'epoch': 0.18578878748370273}
{'loss': 0.0574, 'grad_norm': 0.531101405620575, 'learning_rate': 0.00020070835257846105, 'epoch': 0.18660365058670145}
{'loss': 0.0577, 'grad_norm': 0.3640139102935791, 'learning_rate': 0.00020027155477544463, 'epoch': 0.18741851368970014}
{'loss': 0.0512, 'grad_norm': 0.3340291678905487, 'learning_rate': 0.00019983759640033942, 'epoch': 0.18823337679269883}
{'loss': 0.0756, 'grad_norm': 0.4386158287525177, 'learning_rate': 0.00019940644682277142, 'epoch': 0.18904823989569752}
{'loss': 0.0473, 'grad_norm': 0.20288434624671936, 'learning_rate': 0.00019897807587297548, 'epoch': 0.18986310299869621}
{'loss': 0.0486, 'grad_norm': 0.4607437252998352, 'learning_rate': 0.00019855245383292781, 'epoch': 0.1906779661016949}
{'loss': 0.0552, 'grad_norm': 0.546354353427887, 'learning_rate': 0.00019812955142768627, 'epoch': 0.1914928292046936}
{'loss': 0.0605, 'grad_norm': 0.5009555220603943, 'learning_rate': 0.00019770933981693293, 'epoch': 0.19230769230769232}
{'loss': 0.0611, 'grad_norm': 0.4495173692703247, 'learning_rate': 0.00019729179058671295, 'epoch': 0.193122555410691}
{'loss': 0.051, 'grad_norm': 0.7470266819000244, 'learning_rate': 0.00019687687574136448, 'epoch': 0.1939374185136897}
{'loss': 0.0584, 'grad_norm': 0.23720583319664001, 'learning_rate': 0.0001964645676956347, 'epoch': 0.1947522816166884}
{'loss': 0.0468, 'grad_norm': 0.5042286515235901, 'learning_rate': 0.00019605483926697653, 'epoch': 0.19556714471968709}
[INFO|trainer.py:3515] 2025-04-10 19:10:58,014 >> Saving model checkpoint to ./test_run_outputs/checkpoint-2400
[INFO|configuration_utils.py:733] 2025-04-10 19:10:58,265 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:10:58,266 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:10:58,286 >> tokenizer config file saved in ./test_run_outputs/checkpoint-2400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:10:58,288 >> Special tokens file saved in ./test_run_outputs/checkpoint-2400/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:10:58,415 >> Deleting older checkpoint [test_run_outputs/checkpoint-2200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:10:58,690 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:10:58,690 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:12:32,855 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:12:32,855 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:12:32,855 >>   Batch size = 32
{'eval_loss': 0.05961953103542328, 'eval_accuracy': 0.15191785104489816, 'eval_runtime': 10.4145, 'eval_samples_per_second': 239.09, 'eval_steps_per_second': 7.49, 'epoch': 0.19556714471968709}
{'loss': 0.0571, 'grad_norm': 0.41669294238090515, 'learning_rate': 0.0001956476636680213, 'epoch': 0.19638200782268578}
{'loss': 0.0554, 'grad_norm': 0.6384297609329224, 'learning_rate': 0.000195243014499223, 'epoch': 0.1971968709256845}
{'loss': 0.0429, 'grad_norm': 0.2759712338447571, 'learning_rate': 0.00019484086574166867, 'epoch': 0.1980117340286832}
{'loss': 0.0474, 'grad_norm': 0.46521222591400146, 'learning_rate': 0.0001944411917500516, 'epoch': 0.19882659713168188}
{'loss': 0.0454, 'grad_norm': 0.38501375913619995, 'learning_rate': 0.00019404396724580182, 'epoch': 0.19964146023468057}
{'loss': 0.056, 'grad_norm': 0.46688368916511536, 'learning_rate': 0.00019364916731037085, 'epoch': 0.20045632333767927}
{'loss': 0.0547, 'grad_norm': 0.18777498602867126, 'learning_rate': 0.00019325676737866598, 'epoch': 0.20127118644067796}
{'loss': 0.0538, 'grad_norm': 0.3793220520019531, 'learning_rate': 0.00019286674323263018, 'epoch': 0.20208604954367665}
{'loss': 0.0555, 'grad_norm': 0.4650650918483734, 'learning_rate': 0.00019247907099496388, 'epoch': 0.20290091264667537}
{'loss': 0.0597, 'grad_norm': 0.32573190331459045, 'learning_rate': 0.00019209372712298547, 'epoch': 0.20371577574967406}
{'loss': 0.0638, 'grad_norm': 0.28843966126441956, 'learning_rate': 0.00019171068840262605, 'epoch': 0.20453063885267275}
{'loss': 0.0606, 'grad_norm': 0.338932067155838, 'learning_rate': 0.00019132993194255534, 'epoch': 0.20534550195567144}
{'loss': 0.047, 'grad_norm': 0.27171599864959717, 'learning_rate': 0.00019095143516843592, 'epoch': 0.20616036505867014}
{'loss': 0.059, 'grad_norm': 0.5202523469924927, 'learning_rate': 0.0001905751758173012, 'epoch': 0.20697522816166883}
{'loss': 0.0547, 'grad_norm': 0.2355666607618332, 'learning_rate': 0.0001902011319320558, 'epoch': 0.20779009126466755}
{'loss': 0.0568, 'grad_norm': 0.1417294293642044, 'learning_rate': 0.00018982928185609302, 'epoch': 0.20860495436766624}
{'loss': 0.0667, 'grad_norm': 0.6529159545898438, 'learning_rate': 0.00018945960422802864, 'epoch': 0.20941981747066493}
{'loss': 0.0615, 'grad_norm': 0.5119540095329285, 'learning_rate': 0.0001890920779765466, 'epoch': 0.21023468057366362}
{'loss': 0.0651, 'grad_norm': 0.36842209100723267, 'learning_rate': 0.00018872668231535418, 'epoch': 0.21104954367666232}
{'loss': 0.059, 'grad_norm': 0.6623933911323547, 'learning_rate': 0.00018836339673824432, 'epoch': 0.211864406779661}
[INFO|trainer.py:3515] 2025-04-10 19:12:43,276 >> Saving model checkpoint to ./test_run_outputs/checkpoint-2600
[INFO|configuration_utils.py:733] 2025-04-10 19:12:43,521 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:12:43,522 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:12:43,542 >> tokenizer config file saved in ./test_run_outputs/checkpoint-2600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:12:43,544 >> Special tokens file saved in ./test_run_outputs/checkpoint-2600/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:12:43,670 >> Deleting older checkpoint [test_run_outputs/checkpoint-2400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:12:43,972 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:12:43,973 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:14:17,903 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:14:17,903 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:14:17,903 >>   Batch size = 32
{'eval_loss': 0.05197529494762421, 'eval_accuracy': 0.15203328283192652, 'eval_runtime': 10.4218, 'eval_samples_per_second': 238.922, 'eval_steps_per_second': 7.484, 'epoch': 0.211864406779661}
{'loss': 0.0542, 'grad_norm': 0.2611137926578522, 'learning_rate': 0.0001880022010142618, 'epoch': 0.2126792698826597}
{'loss': 0.0559, 'grad_norm': 0.2798917889595032, 'learning_rate': 0.00018764307518297107, 'epoch': 0.21349413298565842}
{'loss': 0.0589, 'grad_norm': 0.42658618092536926, 'learning_rate': 0.00018728599954982334, 'epoch': 0.2143089960886571}
{'loss': 0.0642, 'grad_norm': 0.2925320863723755, 'learning_rate': 0.00018693095468161986, 'epoch': 0.2151238591916558}
{'loss': 0.0565, 'grad_norm': 0.28583472967147827, 'learning_rate': 0.00018657792140206996, 'epoch': 0.2159387222946545}
{'loss': 0.0569, 'grad_norm': 0.4072275459766388, 'learning_rate': 0.00018622688078744043, 'epoch': 0.2167535853976532}
{'loss': 0.0532, 'grad_norm': 0.18726038932800293, 'learning_rate': 0.00018587781416229537, 'epoch': 0.21756844850065188}
{'loss': 0.058, 'grad_norm': 0.33225828409194946, 'learning_rate': 0.00018553070309532265, 'epoch': 0.2183833116036506}
{'loss': 0.0557, 'grad_norm': 0.27270084619522095, 'learning_rate': 0.00018518552939524667, 'epoch': 0.2191981747066493}
{'loss': 0.0441, 'grad_norm': 0.35739076137542725, 'learning_rate': 0.0001848422751068236, 'epoch': 0.22001303780964798}
{'loss': 0.0618, 'grad_norm': 0.39225196838378906, 'learning_rate': 0.0001845009225069188, 'epoch': 0.22082790091264667}
{'loss': 0.0435, 'grad_norm': 0.3327234089374542, 'learning_rate': 0.00018416145410066296, 'epoch': 0.22164276401564537}
{'loss': 0.0626, 'grad_norm': 0.3617638349533081, 'learning_rate': 0.00018382385261768615, 'epoch': 0.22245762711864406}
{'loss': 0.057, 'grad_norm': 0.2563038170337677, 'learning_rate': 0.00018348810100842733, 'epoch': 0.22327249022164278}
{'loss': 0.0449, 'grad_norm': 0.41388586163520813, 'learning_rate': 0.00018315418244051798, 'epoch': 0.22408735332464147}
{'loss': 0.0616, 'grad_norm': 0.3303491473197937, 'learning_rate': 0.00018282208029523746, 'epoch': 0.22490221642764016}
{'loss': 0.0549, 'grad_norm': 0.6712534427642822, 'learning_rate': 0.00018249177816403927, 'epoch': 0.22571707953063885}
{'loss': 0.0542, 'grad_norm': 0.4647825360298157, 'learning_rate': 0.0001821632598451457, 'epoch': 0.22653194263363755}
{'loss': 0.0481, 'grad_norm': 0.32181569933891296, 'learning_rate': 0.00018183650934020995, 'epoch': 0.22734680573663624}
{'loss': 0.0671, 'grad_norm': 0.2418404519557953, 'learning_rate': 0.00018151151085104376, 'epoch': 0.22816166883963493}
[INFO|trainer.py:3515] 2025-04-10 19:14:28,347 >> Saving model checkpoint to ./test_run_outputs/checkpoint-2800
[INFO|configuration_utils.py:733] 2025-04-10 19:14:28,593 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:14:28,594 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:14:28,616 >> tokenizer config file saved in ./test_run_outputs/checkpoint-2800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:14:28,617 >> Special tokens file saved in ./test_run_outputs/checkpoint-2800/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:14:28,746 >> Deleting older checkpoint [test_run_outputs/checkpoint-1800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:14:29,006 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:14:29,006 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:16:03,971 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:16:03,971 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:16:03,971 >>   Batch size = 32
{'eval_loss': 0.05124446377158165, 'eval_accuracy': 0.15205492629199432, 'eval_runtime': 10.4447, 'eval_samples_per_second': 238.399, 'eval_steps_per_second': 7.468, 'epoch': 0.22816166883963493}
{'loss': 0.0525, 'grad_norm': 0.19823436439037323, 'learning_rate': 0.0001811882487764091, 'epoch': 0.22897653194263365}
{'loss': 0.0631, 'grad_norm': 0.4184432029724121, 'learning_rate': 0.00018086670770887272, 'epoch': 0.22979139504563234}
{'loss': 0.0463, 'grad_norm': 0.18608993291854858, 'learning_rate': 0.00018054687243172153, 'epoch': 0.23060625814863103}
{'loss': 0.0606, 'grad_norm': 0.455716997385025, 'learning_rate': 0.00018022872791593794, 'epoch': 0.23142112125162972}
{'loss': 0.0478, 'grad_norm': 0.26687148213386536, 'learning_rate': 0.0001799122593172338, 'epoch': 0.23223598435462842}
{'loss': 0.0546, 'grad_norm': 0.2507201135158539, 'learning_rate': 0.00017959745197314121, 'epoch': 0.2330508474576271}
{'loss': 0.059, 'grad_norm': 0.5634607672691345, 'learning_rate': 0.00017928429140015906, 'epoch': 0.23386571056062583}
{'loss': 0.0491, 'grad_norm': 0.2708989083766937, 'learning_rate': 0.00017897276329095443, 'epoch': 0.23468057366362452}
{'loss': 0.0594, 'grad_norm': 0.43076401948928833, 'learning_rate': 0.00017866285351161696, 'epoch': 0.2354954367666232}
{'loss': 0.0589, 'grad_norm': 0.4517742395401001, 'learning_rate': 0.00017835454809896553, 'epoch': 0.2363102998696219}
{'loss': 0.0498, 'grad_norm': 0.4028210937976837, 'learning_rate': 0.0001780478332579059, 'epoch': 0.2371251629726206}
{'loss': 0.0512, 'grad_norm': 0.41782480478286743, 'learning_rate': 0.0001777426953588379, 'epoch': 0.2379400260756193}
{'loss': 0.0589, 'grad_norm': 0.2967372238636017, 'learning_rate': 0.00017743912093511188, 'epoch': 0.23875488917861798}
{'loss': 0.0487, 'grad_norm': 0.23205794394016266, 'learning_rate': 0.00017713709668053208, 'epoch': 0.2395697522816167}
{'loss': 0.0505, 'grad_norm': 0.4890176057815552, 'learning_rate': 0.00017683660944690712, 'epoch': 0.2403846153846154}
{'loss': 0.0708, 'grad_norm': 0.5329402685165405, 'learning_rate': 0.00017653764624164596, 'epoch': 0.24119947848761408}
{'loss': 0.0602, 'grad_norm': 0.456155925989151, 'learning_rate': 0.00017624019422539817, 'epoch': 0.24201434159061277}
{'loss': 0.063, 'grad_norm': 0.3978869616985321, 'learning_rate': 0.00017594424070973785, 'epoch': 0.24282920469361147}
{'loss': 0.0493, 'grad_norm': 0.2986699044704437, 'learning_rate': 0.00017564977315489067, 'epoch': 0.24364406779661016}
{'loss': 0.043, 'grad_norm': 0.3758830428123474, 'learning_rate': 0.0001753567791675018, 'epoch': 0.24445893089960888}
[INFO|trainer.py:3515] 2025-04-10 19:16:14,397 >> Saving model checkpoint to ./test_run_outputs/checkpoint-3000
[INFO|configuration_utils.py:733] 2025-04-10 19:16:14,648 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:16:14,649 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:16:14,670 >> tokenizer config file saved in ./test_run_outputs/checkpoint-3000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:16:14,671 >> Special tokens file saved in ./test_run_outputs/checkpoint-3000/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:16:14,798 >> Deleting older checkpoint [test_run_outputs/checkpoint-2600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:16:15,062 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:16:15,062 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:17:49,774 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:17:49,774 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:17:49,774 >>   Batch size = 32
{'eval_loss': 0.05050051212310791, 'eval_accuracy': 0.1520116393718587, 'eval_runtime': 10.4272, 'eval_samples_per_second': 238.798, 'eval_steps_per_second': 7.48, 'epoch': 0.24445893089960888}
{'loss': 0.0595, 'grad_norm': 0.33278220891952515, 'learning_rate': 0.00017506524649844545, 'epoch': 0.24527379400260757}
{'loss': 0.0566, 'grad_norm': 0.34918835759162903, 'learning_rate': 0.00017477516304067358, 'epoch': 0.24608865710560626}
{'loss': 0.0407, 'grad_norm': 0.1752825677394867, 'learning_rate': 0.00017448651682710456, 'epoch': 0.24690352020860495}
{'loss': 0.0666, 'grad_norm': 0.5658913850784302, 'learning_rate': 0.00017419929602854916, 'epoch': 0.24771838331160365}
{'loss': 0.0522, 'grad_norm': 0.19570095837116241, 'learning_rate': 0.00017391348895167465, 'epoch': 0.24853324641460234}
{'loss': 0.0494, 'grad_norm': 0.2697533369064331, 'learning_rate': 0.00017362908403700518, 'epoch': 0.24934810951760103}
{'loss': 0.0659, 'grad_norm': 0.45389533042907715, 'learning_rate': 0.00017334606985695817, 'epoch': 0.2501629726205997}
{'loss': 0.0505, 'grad_norm': 0.10962708294391632, 'learning_rate': 0.00017306443511391558, 'epoch': 0.25097783572359844}
{'loss': 0.0559, 'grad_norm': 0.536126434803009, 'learning_rate': 0.00017278416863832994, 'epoch': 0.2517926988265971}
{'loss': 0.0599, 'grad_norm': 0.3151865601539612, 'learning_rate': 0.00017250525938686343, 'epoch': 0.2526075619295958}
{'loss': 0.0513, 'grad_norm': 0.40406858921051025, 'learning_rate': 0.00017222769644056052, 'epoch': 0.25342242503259454}
{'loss': 0.0466, 'grad_norm': 0.3536551296710968, 'learning_rate': 0.0001719514690030524, 'epoch': 0.2542372881355932}
{'loss': 0.0561, 'grad_norm': 0.4583377242088318, 'learning_rate': 0.00017167656639879316, 'epoch': 0.25505215123859193}
{'loss': 0.0416, 'grad_norm': 0.24005888402462006, 'learning_rate': 0.00017140297807132683, 'epoch': 0.2558670143415906}
{'loss': 0.0621, 'grad_norm': 0.48695212602615356, 'learning_rate': 0.00017113069358158485, 'epoch': 0.2566818774445893}
{'loss': 0.0579, 'grad_norm': 0.41225576400756836, 'learning_rate': 0.00017085970260621297, 'epoch': 0.25749674054758803}
{'loss': 0.0564, 'grad_norm': 0.28924527764320374, 'learning_rate': 0.00017058999493592744, 'epoch': 0.2583116036505867}
{'loss': 0.0615, 'grad_norm': 0.2504352927207947, 'learning_rate': 0.00017032156047389936, 'epoch': 0.2591264667535854}
{'loss': 0.0467, 'grad_norm': 0.319173663854599, 'learning_rate': 0.0001700543892341672, 'epoch': 0.2599413298565841}
{'loss': 0.0651, 'grad_norm': 0.37597882747650146, 'learning_rate': 0.00016978847134007657, 'epoch': 0.2607561929595828}
[INFO|trainer.py:3515] 2025-04-10 19:18:00,207 >> Saving model checkpoint to ./test_run_outputs/checkpoint-3200
[INFO|configuration_utils.py:733] 2025-04-10 19:18:00,888 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:18:00,889 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:18:00,910 >> tokenizer config file saved in ./test_run_outputs/checkpoint-3200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:18:00,911 >> Special tokens file saved in ./test_run_outputs/checkpoint-3200/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:18:01,040 >> Deleting older checkpoint [test_run_outputs/checkpoint-2800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:18:01,299 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:18:01,299 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:19:36,961 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:19:36,961 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:19:36,961 >>   Batch size = 32
{'eval_loss': 0.05558408051729202, 'eval_accuracy': 0.15197797176730876, 'eval_runtime': 10.4345, 'eval_samples_per_second': 238.632, 'eval_steps_per_second': 7.475, 'epoch': 0.2607561929595828}
{'loss': 0.0633, 'grad_norm': 0.23972372710704803, 'learning_rate': 0.00016952379702274648, 'epoch': 0.26157105606258146}
{'loss': 0.0489, 'grad_norm': 0.47589409351348877, 'learning_rate': 0.00016926035661956218, 'epoch': 0.2623859191655802}
{'loss': 0.0502, 'grad_norm': 0.2755599021911621, 'learning_rate': 0.00016899814057269347, 'epoch': 0.2632007822685789}
{'loss': 0.0665, 'grad_norm': 0.4746124744415283, 'learning_rate': 0.00016873713942763824, 'epoch': 0.26401564537157757}
{'loss': 0.0667, 'grad_norm': 0.34983015060424805, 'learning_rate': 0.0001684773438317906, 'epoch': 0.2648305084745763}
{'loss': 0.0563, 'grad_norm': 0.3235780596733093, 'learning_rate': 0.00016821874453303335, 'epoch': 0.26564537157757495}
{'loss': 0.041, 'grad_norm': 0.36308208107948303, 'learning_rate': 0.00016796133237835406, 'epoch': 0.26646023468057367}
{'loss': 0.0467, 'grad_norm': 0.3372464179992676, 'learning_rate': 0.00016770509831248425, 'epoch': 0.26727509778357234}
{'loss': 0.0648, 'grad_norm': 0.42264366149902344, 'learning_rate': 0.00016745003337656147, 'epoch': 0.26808996088657105}
{'loss': 0.046, 'grad_norm': 0.19442906975746155, 'learning_rate': 0.00016719612870681383, 'epoch': 0.2689048239895698}
{'loss': 0.0536, 'grad_norm': 0.3709116578102112, 'learning_rate': 0.00016694337553326601, 'epoch': 0.26971968709256844}
{'loss': 0.0523, 'grad_norm': 0.44957655668258667, 'learning_rate': 0.0001666917651784672, 'epoch': 0.27053455019556716}
{'loss': 0.0442, 'grad_norm': 0.39080899953842163, 'learning_rate': 0.00016644128905623958, 'epoch': 0.2713494132985658}
{'loss': 0.0625, 'grad_norm': 0.43640273809432983, 'learning_rate': 0.0001661919386704479, 'epoch': 0.27216427640156454}
{'loss': 0.0565, 'grad_norm': 0.3036365211009979, 'learning_rate': 0.00016594370561378885, 'epoch': 0.2729791395045632}
{'loss': 0.0503, 'grad_norm': 0.32168230414390564, 'learning_rate': 0.00016569658156660063, 'epoch': 0.2737940026075619}
{'loss': 0.061, 'grad_norm': 0.38430795073509216, 'learning_rate': 0.0001654505582956917, 'epoch': 0.27460886571056065}
{'loss': 0.0481, 'grad_norm': 0.23942388594150543, 'learning_rate': 0.0001652056276531887, 'epoch': 0.2754237288135593}
{'loss': 0.054, 'grad_norm': 0.36646243929862976, 'learning_rate': 0.000164961781575403, 'epoch': 0.27623859191655803}
{'loss': 0.0533, 'grad_norm': 0.48922157287597656, 'learning_rate': 0.0001647190120817159, 'epoch': 0.2770534550195567}
[INFO|trainer.py:3515] 2025-04-10 19:19:47,380 >> Saving model checkpoint to ./test_run_outputs/checkpoint-3400
[INFO|configuration_utils.py:733] 2025-04-10 19:19:47,629 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:19:47,630 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:19:47,650 >> tokenizer config file saved in ./test_run_outputs/checkpoint-3400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:19:47,651 >> Special tokens file saved in ./test_run_outputs/checkpoint-3400/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:19:47,780 >> Deleting older checkpoint [test_run_outputs/checkpoint-3000] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:19:48,035 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:19:48,036 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:21:23,136 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:21:23,136 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:21:23,136 >>   Batch size = 32
{'eval_loss': 0.04969776049256325, 'eval_accuracy': 0.15204049731861577, 'eval_runtime': 10.4201, 'eval_samples_per_second': 238.962, 'eval_steps_per_second': 7.486, 'epoch': 0.2770534550195567}
{'loss': 0.0484, 'grad_norm': 0.3513554036617279, 'learning_rate': 0.0001644773112734813, 'epoch': 0.2778683181225554}
{'loss': 0.0584, 'grad_norm': 0.25539934635162354, 'learning_rate': 0.0001642366713329466, 'epoch': 0.27868318122555413}
{'loss': 0.0517, 'grad_norm': 0.49697789549827576, 'learning_rate': 0.00016399708452219028, 'epoch': 0.2794980443285528}
{'loss': 0.0621, 'grad_norm': 0.32647615671157837, 'learning_rate': 0.00016375854318207732, 'epoch': 0.2803129074315515}
{'loss': 0.0503, 'grad_norm': 0.2810983955860138, 'learning_rate': 0.00016352103973123035, 'epoch': 0.2811277705345502}
{'loss': 0.0419, 'grad_norm': 0.35841238498687744, 'learning_rate': 0.00016328456666501784, 'epoch': 0.2819426336375489}
{'loss': 0.0473, 'grad_norm': 0.42144274711608887, 'learning_rate': 0.0001630491165545582, 'epoch': 0.28275749674054756}
{'loss': 0.0599, 'grad_norm': 0.20155782997608185, 'learning_rate': 0.00016281468204573925, 'epoch': 0.2835723598435463}
{'loss': 0.0514, 'grad_norm': 0.3745124638080597, 'learning_rate': 0.00016258125585825382, 'epoch': 0.284387222946545}
{'loss': 0.053, 'grad_norm': 0.5395992994308472, 'learning_rate': 0.0001623488307846498, 'epoch': 0.28520208604954367}
{'loss': 0.053, 'grad_norm': 0.16607621312141418, 'learning_rate': 0.00016211739968939574, 'epoch': 0.2860169491525424}
{'loss': 0.0583, 'grad_norm': 0.29666781425476074, 'learning_rate': 0.00016188695550796043, 'epoch': 0.28683181225554105}
{'loss': 0.0512, 'grad_norm': 0.4740794897079468, 'learning_rate': 0.00016165749124590756, 'epoch': 0.28764667535853977}
{'loss': 0.045, 'grad_norm': 0.43257084488868713, 'learning_rate': 0.00016142899997800367, 'epoch': 0.28846153846153844}
{'loss': 0.049, 'grad_norm': 0.35977092385292053, 'learning_rate': 0.0001612014748473405, 'epoch': 0.28927640156453716}
{'loss': 0.059, 'grad_norm': 0.45026248693466187, 'learning_rate': 0.00016097490906447068, 'epoch': 0.2900912646675359}
{'loss': 0.07, 'grad_norm': 0.4301969110965729, 'learning_rate': 0.0001607492959065569, 'epoch': 0.29090612777053454}
{'loss': 0.053, 'grad_norm': 0.20488673448562622, 'learning_rate': 0.00016052462871653372, 'epoch': 0.29172099087353326}
{'loss': 0.0438, 'grad_norm': 0.3636928200721741, 'learning_rate': 0.00016030090090228302, 'epoch': 0.2925358539765319}
{'loss': 0.0607, 'grad_norm': 0.3452856242656708, 'learning_rate': 0.00016007810593582123, 'epoch': 0.29335071707953064}
[INFO|trainer.py:3515] 2025-04-10 19:21:33,570 >> Saving model checkpoint to ./test_run_outputs/checkpoint-3600
[INFO|configuration_utils.py:733] 2025-04-10 19:21:33,827 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:21:33,828 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:21:33,849 >> tokenizer config file saved in ./test_run_outputs/checkpoint-3600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:21:33,850 >> Special tokens file saved in ./test_run_outputs/checkpoint-3600/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:21:33,980 >> Deleting older checkpoint [test_run_outputs/checkpoint-3200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:21:34,326 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:21:34,327 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:23:08,354 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:23:08,355 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:23:08,355 >>   Batch size = 32
{'eval_loss': 0.05326636880636215, 'eval_accuracy': 0.15197556693841233, 'eval_runtime': 10.4345, 'eval_samples_per_second': 238.632, 'eval_steps_per_second': 7.475, 'epoch': 0.29335071707953064}
{'loss': 0.0594, 'grad_norm': 0.32204246520996094, 'learning_rate': 0.00015985623735249939, 'epoch': 0.29416558018252936}
{'loss': 0.0644, 'grad_norm': 0.42737534642219543, 'learning_rate': 0.00015963528875021537, 'epoch': 0.294980443285528}
{'loss': 0.0451, 'grad_norm': 0.20306983590126038, 'learning_rate': 0.000159415253788638, 'epoch': 0.29579530638852675}
{'loss': 0.0543, 'grad_norm': 0.306414395570755, 'learning_rate': 0.00015919612618844276, 'epoch': 0.2966101694915254}
{'loss': 0.0423, 'grad_norm': 0.3495961129665375, 'learning_rate': 0.0001589778997305592, 'epoch': 0.29742503259452413}
{'loss': 0.0511, 'grad_norm': 0.30157557129859924, 'learning_rate': 0.0001587605682554295, 'epoch': 0.2982398956975228}
{'loss': 0.0464, 'grad_norm': 0.42325249314308167, 'learning_rate': 0.00015854412566227848, 'epoch': 0.2990547588005215}
{'loss': 0.0532, 'grad_norm': 0.27334845066070557, 'learning_rate': 0.00015832856590839413, 'epoch': 0.29986962190352023}
{'loss': 0.0508, 'grad_norm': 0.46014320850372314, 'learning_rate': 0.00015811388300841897, 'epoch': 0.3006844850065189}
{'loss': 0.0604, 'grad_norm': 0.24813713133335114, 'learning_rate': 0.00015790007103365226, 'epoch': 0.3014993481095176}
{'loss': 0.0662, 'grad_norm': 0.3267316520214081, 'learning_rate': 0.00015768712411136216, 'epoch': 0.3023142112125163}
{'loss': 0.0549, 'grad_norm': 0.28956690430641174, 'learning_rate': 0.00015747503642410816, 'epoch': 0.303129074315515}
{'loss': 0.0515, 'grad_norm': 0.35023701190948486, 'learning_rate': 0.00015726380220907382, 'epoch': 0.30394393741851367}
{'loss': 0.0559, 'grad_norm': 0.18239429593086243, 'learning_rate': 0.00015705341575740882, 'epoch': 0.3047588005215124}
{'loss': 0.0586, 'grad_norm': 0.30174145102500916, 'learning_rate': 0.00015684387141358123, 'epoch': 0.3055736636245111}
{'loss': 0.0532, 'grad_norm': 0.3873869776725769, 'learning_rate': 0.00015663516357473855, 'epoch': 0.30638852672750977}
{'loss': 0.0527, 'grad_norm': 0.25365498661994934, 'learning_rate': 0.00015642728669007896, 'epoch': 0.3072033898305085}
{'loss': 0.0543, 'grad_norm': 0.3046048581600189, 'learning_rate': 0.00015622023526023097, 'epoch': 0.30801825293350715}
{'loss': 0.0505, 'grad_norm': 0.41200730204582214, 'learning_rate': 0.0001560140038366423, 'epoch': 0.3088331160365059}
{'loss': 0.0603, 'grad_norm': 0.2104186862707138, 'learning_rate': 0.00015580858702097803, 'epoch': 0.30964797913950454}
[INFO|trainer.py:3515] 2025-04-10 19:23:18,784 >> Saving model checkpoint to ./test_run_outputs/checkpoint-3800
[INFO|configuration_utils.py:733] 2025-04-10 19:23:19,044 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:23:19,045 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:23:19,066 >> tokenizer config file saved in ./test_run_outputs/checkpoint-3800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:23:19,067 >> Special tokens file saved in ./test_run_outputs/checkpoint-3800/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:23:19,196 >> Deleting older checkpoint [test_run_outputs/checkpoint-3600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:23:19,455 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:23:19,455 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:24:54,753 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:24:54,754 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:24:54,754 >>   Batch size = 32
{'eval_loss': 0.05044671520590782, 'eval_accuracy': 0.15205492629199432, 'eval_runtime': 10.4307, 'eval_samples_per_second': 238.718, 'eval_steps_per_second': 7.478, 'epoch': 0.30964797913950454}
{'loss': 0.0523, 'grad_norm': 0.3099561333656311, 'learning_rate': 0.00015560397946452672, 'epoch': 0.31046284224250326}
{'loss': 0.0503, 'grad_norm': 0.42227238416671753, 'learning_rate': 0.00015540017586761562, 'epoch': 0.311277705345502}
{'loss': 0.0439, 'grad_norm': 0.3770025670528412, 'learning_rate': 0.00015519717097903417, 'epoch': 0.31209256844850064}
{'loss': 0.0477, 'grad_norm': 0.3118731677532196, 'learning_rate': 0.00015499495959546555, 'epoch': 0.31290743155149936}
{'loss': 0.0493, 'grad_norm': 0.30314841866493225, 'learning_rate': 0.00015479353656092673, 'epoch': 0.313722294654498}
{'loss': 0.0663, 'grad_norm': 0.5264327526092529, 'learning_rate': 0.0001545928967662161, 'epoch': 0.31453715775749674}
{'loss': 0.0618, 'grad_norm': 0.22614611685276031, 'learning_rate': 0.00015439303514836925, 'epoch': 0.31535202086049546}
{'loss': 0.0528, 'grad_norm': 0.24307847023010254, 'learning_rate': 0.00015419394669012235, 'epoch': 0.3161668839634941}
{'loss': 0.0608, 'grad_norm': 0.3810790479183197, 'learning_rate': 0.00015399562641938309, 'epoch': 0.31698174706649285}
{'loss': 0.0574, 'grad_norm': 0.3591669499874115, 'learning_rate': 0.00015379806940870927, 'epoch': 0.3177966101694915}
{'loss': 0.0537, 'grad_norm': 0.5112574696540833, 'learning_rate': 0.00015360127077479441, 'epoch': 0.31861147327249023}
{'loss': 0.0565, 'grad_norm': 0.2580130100250244, 'learning_rate': 0.00015340522567796095, 'epoch': 0.3194263363754889}
{'loss': 0.0475, 'grad_norm': 0.32230761647224426, 'learning_rate': 0.00015320992932166012, 'epoch': 0.3202411994784876}
{'loss': 0.0561, 'grad_norm': 0.592599630355835, 'learning_rate': 0.0001530153769519794, 'epoch': 0.32105606258148633}
{'loss': 0.0554, 'grad_norm': 0.35652247071266174, 'learning_rate': 0.0001528215638571561, 'epoch': 0.321870925684485}
{'loss': 0.0564, 'grad_norm': 0.3235624432563782, 'learning_rate': 0.0001526284853670983, 'epoch': 0.3226857887874837}
{'loss': 0.0633, 'grad_norm': 0.2655218243598938, 'learning_rate': 0.00015243613685291226, 'epoch': 0.3235006518904824}
{'loss': 0.058, 'grad_norm': 0.4049769639968872, 'learning_rate': 0.0001522445137264361, 'epoch': 0.3243155149934811}
{'loss': 0.0545, 'grad_norm': 0.2761540114879608, 'learning_rate': 0.00015205361143978035, 'epoch': 0.32513037809647977}
{'loss': 0.0467, 'grad_norm': 0.2992106080055237, 'learning_rate': 0.00015186342548487443, 'epoch': 0.3259452411994785}
[INFO|trainer.py:3515] 2025-04-10 19:25:05,160 >> Saving model checkpoint to ./test_run_outputs/checkpoint-4000
[INFO|configuration_utils.py:733] 2025-04-10 19:25:05,417 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:25:05,417 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:25:05,438 >> tokenizer config file saved in ./test_run_outputs/checkpoint-4000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:25:05,440 >> Special tokens file saved in ./test_run_outputs/checkpoint-4000/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:25:05,568 >> Deleting older checkpoint [test_run_outputs/checkpoint-3400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:25:05,830 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:25:05,830 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:26:39,855 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:26:39,855 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:26:39,855 >>   Batch size = 32
{'eval_loss': 0.04932515695691109, 'eval_accuracy': 0.15205011663420148, 'eval_runtime': 10.407, 'eval_samples_per_second': 239.262, 'eval_steps_per_second': 7.495, 'epoch': 0.3259452411994785}
{'loss': 0.0424, 'grad_norm': 0.445575475692749, 'learning_rate': 0.0001516739513930196, 'epoch': 0.3267601043024772}
{'loss': 0.0543, 'grad_norm': 0.30805274844169617, 'learning_rate': 0.00015148518473444804, 'epoch': 0.32757496740547587}
{'loss': 0.0496, 'grad_norm': 0.635624885559082, 'learning_rate': 0.00015129712111788777, 'epoch': 0.3283898305084746}
{'loss': 0.0608, 'grad_norm': 0.2835966646671295, 'learning_rate': 0.00015110975619013348, 'epoch': 0.32920469361147325}
{'loss': 0.0504, 'grad_norm': 0.2941480875015259, 'learning_rate': 0.00015092308563562358, 'epoch': 0.330019556714472}
{'loss': 0.0622, 'grad_norm': 0.36223477125167847, 'learning_rate': 0.00015073710517602256, 'epoch': 0.33083441981747064}
{'loss': 0.0519, 'grad_norm': 0.36620935797691345, 'learning_rate': 0.00015055181056980904, 'epoch': 0.33164928292046936}
{'loss': 0.0726, 'grad_norm': 0.31591612100601196, 'learning_rate': 0.00015036719761186965, 'epoch': 0.3324641460234681}
{'loss': 0.0525, 'grad_norm': 0.25599420070648193, 'learning_rate': 0.00015018326213309815, 'epoch': 0.33327900912646674}
{'loss': 0.0492, 'grad_norm': 0.3533899486064911, 'learning_rate': 0.00015, 'epoch': 0.33409387222946546}
{'loss': 0.0563, 'grad_norm': 0.2033185064792633, 'learning_rate': 0.00014981740711430213, 'epoch': 0.3349087353324641}
{'loss': 0.0618, 'grad_norm': 0.1653272956609726, 'learning_rate': 0.00014963547941256822, 'epoch': 0.33572359843546284}
{'loss': 0.0591, 'grad_norm': 0.395285964012146, 'learning_rate': 0.00014945421286581857, 'epoch': 0.33653846153846156}
{'loss': 0.0463, 'grad_norm': 0.3482458293437958, 'learning_rate': 0.00014927360347915562, 'epoch': 0.33735332464146023}
{'loss': 0.0649, 'grad_norm': 0.6197746396064758, 'learning_rate': 0.00014909364729139403, 'epoch': 0.33816818774445895}
{'loss': 0.0491, 'grad_norm': 0.1577981412410736, 'learning_rate': 0.00014891434037469587, 'epoch': 0.3389830508474576}
{'loss': 0.035, 'grad_norm': 0.22107082605361938, 'learning_rate': 0.00014873567883421038, 'epoch': 0.33979791395045633}
{'loss': 0.0423, 'grad_norm': 0.5962352752685547, 'learning_rate': 0.00014855765880771892, 'epoch': 0.340612777053455}
{'loss': 0.0492, 'grad_norm': 0.31861692667007446, 'learning_rate': 0.0001483802764652839, 'epoch': 0.3414276401564537}
{'loss': 0.0555, 'grad_norm': 0.1513672173023224, 'learning_rate': 0.00014820352800890306, 'epoch': 0.34224250325945244}
[INFO|trainer.py:3515] 2025-04-10 19:26:50,281 >> Saving model checkpoint to ./test_run_outputs/checkpoint-4200
[INFO|configuration_utils.py:733] 2025-04-10 19:26:50,528 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:26:50,529 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:26:50,550 >> tokenizer config file saved in ./test_run_outputs/checkpoint-4200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:26:50,552 >> Special tokens file saved in ./test_run_outputs/checkpoint-4200/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:26:50,683 >> Deleting older checkpoint [test_run_outputs/checkpoint-3800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:26:51,034 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:26:51,035 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:28:25,034 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:28:25,034 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:28:25,034 >>   Batch size = 32
{'eval_loss': 0.04854752868413925, 'eval_accuracy': 0.1520621407786836, 'eval_runtime': 10.4274, 'eval_samples_per_second': 238.795, 'eval_steps_per_second': 7.48, 'epoch': 0.34224250325945244}
{'loss': 0.0526, 'grad_norm': 0.3446149230003357, 'learning_rate': 0.00014802740967216773, 'epoch': 0.3430573663624511}
{'loss': 0.0503, 'grad_norm': 0.2428067922592163, 'learning_rate': 0.00014785191771992583, 'epoch': 0.3438722294654498}
{'loss': 0.0641, 'grad_norm': 0.5482791066169739, 'learning_rate': 0.000147677048447949, 'epoch': 0.3446870925684485}
{'loss': 0.0504, 'grad_norm': 0.3221569359302521, 'learning_rate': 0.00014750279818260436, 'epoch': 0.3455019556714472}
{'loss': 0.051, 'grad_norm': 0.4490388333797455, 'learning_rate': 0.00014732916328053037, 'epoch': 0.34631681877444587}
{'loss': 0.0547, 'grad_norm': 0.3917698562145233, 'learning_rate': 0.00014715614012831673, 'epoch': 0.3471316818774446}
{'loss': 0.0565, 'grad_norm': 0.38769376277923584, 'learning_rate': 0.0001469837251421887, 'epoch': 0.3479465449804433}
{'loss': 0.0495, 'grad_norm': 0.35122907161712646, 'learning_rate': 0.00014681191476769524, 'epoch': 0.34876140808344197}
{'loss': 0.0369, 'grad_norm': 0.5463213920593262, 'learning_rate': 0.00014664070547940126, 'epoch': 0.3495762711864407}
{'loss': 0.0668, 'grad_norm': 0.49477046728134155, 'learning_rate': 0.00014647009378058386, 'epoch': 0.35039113428943935}
{'loss': 0.0431, 'grad_norm': 0.47078654170036316, 'learning_rate': 0.00014630007620293218, 'epoch': 0.3512059973924381}
{'loss': 0.0672, 'grad_norm': 0.6664205193519592, 'learning_rate': 0.00014613064930625152, 'epoch': 0.3520208604954368}
{'loss': 0.0507, 'grad_norm': 0.2592436969280243, 'learning_rate': 0.00014596180967817082, 'epoch': 0.35283572359843546}
{'loss': 0.0523, 'grad_norm': 0.44191229343414307, 'learning_rate': 0.000145793553933854, 'epoch': 0.3536505867014342}
{'loss': 0.0504, 'grad_norm': 0.18957000970840454, 'learning_rate': 0.000145625878715715, 'epoch': 0.35446544980443284}
{'loss': 0.054, 'grad_norm': 0.46365490555763245, 'learning_rate': 0.0001454587806931364, 'epoch': 0.35528031290743156}
{'loss': 0.0628, 'grad_norm': 0.3955412209033966, 'learning_rate': 0.00014529225656219135, 'epoch': 0.3560951760104302}
{'loss': 0.0516, 'grad_norm': 0.27509304881095886, 'learning_rate': 0.00014512630304536957, 'epoch': 0.35691003911342895}
{'loss': 0.0521, 'grad_norm': 0.33754462003707886, 'learning_rate': 0.00014496091689130596, 'epoch': 0.35772490221642766}
{'loss': 0.0603, 'grad_norm': 0.40202540159225464, 'learning_rate': 0.00014479609487451342, 'epoch': 0.35853976531942633}
[INFO|trainer.py:3515] 2025-04-10 19:28:35,421 >> Saving model checkpoint to ./test_run_outputs/checkpoint-4400
[INFO|configuration_utils.py:733] 2025-04-10 19:28:35,695 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:28:35,696 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:28:35,716 >> tokenizer config file saved in ./test_run_outputs/checkpoint-4400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:28:35,717 >> Special tokens file saved in ./test_run_outputs/checkpoint-4400/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:28:35,847 >> Deleting older checkpoint [test_run_outputs/checkpoint-4000] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:28:36,120 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:28:36,120 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:30:10,050 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:30:10,050 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:30:10,050 >>   Batch size = 32
{'eval_loss': 0.05033324286341667, 'eval_accuracy': 0.15204530697640864, 'eval_runtime': 10.3883, 'eval_samples_per_second': 239.694, 'eval_steps_per_second': 7.508, 'epoch': 0.35853976531942633}
{'loss': 0.0492, 'grad_norm': 0.41399163007736206, 'learning_rate': 0.00014463183379511847, 'epoch': 0.35935462842242505}
{'loss': 0.0574, 'grad_norm': 0.26328614354133606, 'learning_rate': 0.00014446813047860036, 'epoch': 0.3601694915254237}
{'loss': 0.0487, 'grad_norm': 0.28632551431655884, 'learning_rate': 0.0001443049817755334, 'epoch': 0.36098435462842243}
{'loss': 0.0637, 'grad_norm': 0.367261677980423, 'learning_rate': 0.00014414238456133237, 'epoch': 0.3617992177314211}
{'loss': 0.0563, 'grad_norm': 0.32565104961395264, 'learning_rate': 0.0001439803357360014, 'epoch': 0.3626140808344198}
{'loss': 0.0531, 'grad_norm': 0.22708262503147125, 'learning_rate': 0.0001438188322238856, 'epoch': 0.36342894393741854}
{'loss': 0.0569, 'grad_norm': 0.3431735932826996, 'learning_rate': 0.00014365787097342578, 'epoch': 0.3642438070404172}
{'loss': 0.0382, 'grad_norm': 0.25991350412368774, 'learning_rate': 0.00014349744895691648, 'epoch': 0.3650586701434159}
{'loss': 0.0472, 'grad_norm': 0.40914618968963623, 'learning_rate': 0.00014333756317026671, 'epoch': 0.3658735332464146}
{'loss': 0.0557, 'grad_norm': 0.22305390238761902, 'learning_rate': 0.00014317821063276353, 'epoch': 0.3666883963494133}
{'loss': 0.0594, 'grad_norm': 0.4835647940635681, 'learning_rate': 0.00014301938838683886, 'epoch': 0.36750325945241197}
{'loss': 0.0518, 'grad_norm': 0.3859415054321289, 'learning_rate': 0.00014286109349783877, 'epoch': 0.3683181225554107}
{'loss': 0.0573, 'grad_norm': 0.37403643131256104, 'learning_rate': 0.00014270332305379586, 'epoch': 0.3691329856584094}
{'loss': 0.0455, 'grad_norm': 0.2411159723997116, 'learning_rate': 0.0001425460741652042, 'epoch': 0.36994784876140807}
{'loss': 0.0471, 'grad_norm': 0.2043154090642929, 'learning_rate': 0.000142389343964797, 'epoch': 0.3707627118644068}
{'loss': 0.0448, 'grad_norm': 0.2953968346118927, 'learning_rate': 0.0001422331296073272, 'epoch': 0.37157757496740546}
{'loss': 0.0558, 'grad_norm': 0.3146474063396454, 'learning_rate': 0.00014207742826935032, 'epoch': 0.3723924380704042}
{'loss': 0.0411, 'grad_norm': 0.2645452320575714, 'learning_rate': 0.00014192223714901028, 'epoch': 0.3732073011734029}
{'loss': 0.0628, 'grad_norm': 0.36241036653518677, 'learning_rate': 0.00014176755346582756, 'epoch': 0.37402216427640156}
{'loss': 0.056, 'grad_norm': 0.39231187105178833, 'learning_rate': 0.00014161337446049, 'epoch': 0.3748370273794003}
[INFO|trainer.py:3515] 2025-04-10 19:30:20,434 >> Saving model checkpoint to ./test_run_outputs/checkpoint-4600
[INFO|configuration_utils.py:733] 2025-04-10 19:30:20,680 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:30:20,680 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:30:20,701 >> tokenizer config file saved in ./test_run_outputs/checkpoint-4600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:30:20,702 >> Special tokens file saved in ./test_run_outputs/checkpoint-4600/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:30:20,832 >> Deleting older checkpoint [test_run_outputs/checkpoint-4400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:30:21,089 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:30:21,090 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:31:54,704 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:31:54,704 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:31:54,704 >>   Batch size = 32
{'eval_loss': 0.05261616408824921, 'eval_accuracy': 0.15201404420075512, 'eval_runtime': 10.3845, 'eval_samples_per_second': 239.78, 'eval_steps_per_second': 7.511, 'epoch': 0.3748370273794003}
{'loss': 0.0467, 'grad_norm': 0.37241119146347046, 'learning_rate': 0.00014145969739464597, 'epoch': 0.37565189048239894}
{'loss': 0.0517, 'grad_norm': 0.2968466877937317, 'learning_rate': 0.0001413065195507004, 'epoch': 0.37646675358539766}
{'loss': 0.0534, 'grad_norm': 0.3291669487953186, 'learning_rate': 0.0001411538382316124, 'epoch': 0.3772816166883963}
{'loss': 0.0548, 'grad_norm': 0.24622994661331177, 'learning_rate': 0.00014100165076069636, 'epoch': 0.37809647979139505}
{'loss': 0.052, 'grad_norm': 0.4436858296394348, 'learning_rate': 0.00014084995448142453, 'epoch': 0.37891134289439377}
{'loss': 0.048, 'grad_norm': 0.22583024203777313, 'learning_rate': 0.00014069874675723233, 'epoch': 0.37972620599739243}
{'loss': 0.0536, 'grad_norm': 0.33001264929771423, 'learning_rate': 0.00014054802497132597, 'epoch': 0.38054106910039115}
{'loss': 0.0463, 'grad_norm': 0.2235662043094635, 'learning_rate': 0.00014039778652649216, 'epoch': 0.3813559322033898}
{'loss': 0.0517, 'grad_norm': 0.3848065137863159, 'learning_rate': 0.0001402480288449101, 'epoch': 0.38217079530638853}
{'loss': 0.0529, 'grad_norm': 0.37927553057670593, 'learning_rate': 0.00014009874936796578, 'epoch': 0.3829856584093872}
{'loss': 0.0454, 'grad_norm': 0.6476199626922607, 'learning_rate': 0.00013994994555606836, 'epoch': 0.3838005215123859}
{'loss': 0.0648, 'grad_norm': 0.5335350036621094, 'learning_rate': 0.00013980161488846875, 'epoch': 0.38461538461538464}
{'loss': 0.0492, 'grad_norm': 0.2646141052246094, 'learning_rate': 0.00013965375486308022, 'epoch': 0.3854302477183833}
{'loss': 0.0576, 'grad_norm': 0.3270426094532013, 'learning_rate': 0.00013950636299630098, 'epoch': 0.386245110821382}
{'loss': 0.0574, 'grad_norm': 0.3313724100589752, 'learning_rate': 0.00013935943682283933, 'epoch': 0.3870599739243807}
{'loss': 0.0468, 'grad_norm': 0.38130372762680054, 'learning_rate': 0.00013921297389554014, 'epoch': 0.3878748370273794}
{'loss': 0.0616, 'grad_norm': 0.5126857757568359, 'learning_rate': 0.00013906697178521374, 'epoch': 0.3886897001303781}
{'loss': 0.0425, 'grad_norm': 0.4904577434062958, 'learning_rate': 0.00013892142808046685, 'epoch': 0.3895045632333768}
{'loss': 0.051, 'grad_norm': 0.481520414352417, 'learning_rate': 0.00013877634038753507, 'epoch': 0.3903194263363755}
{'loss': 0.041, 'grad_norm': 0.3166322112083435, 'learning_rate': 0.0001386317063301177, 'epoch': 0.39113428943937417}
[INFO|trainer.py:3515] 2025-04-10 19:32:05,083 >> Saving model checkpoint to ./test_run_outputs/checkpoint-4800
[INFO|configuration_utils.py:733] 2025-04-10 19:32:05,333 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:32:05,334 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:32:05,355 >> tokenizer config file saved in ./test_run_outputs/checkpoint-4800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:32:05,356 >> Special tokens file saved in ./test_run_outputs/checkpoint-4800/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:32:05,487 >> Deleting older checkpoint [test_run_outputs/checkpoint-4600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:32:05,913 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:32:05,914 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:33:40,366 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:33:40,366 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:33:40,366 >>   Batch size = 32
{'eval_loss': 0.04878896847367287, 'eval_accuracy': 0.15206695043647644, 'eval_runtime': 10.38, 'eval_samples_per_second': 239.884, 'eval_steps_per_second': 7.514, 'epoch': 0.39113428943937417}
{'loss': 0.0592, 'grad_norm': 0.4701864421367645, 'learning_rate': 0.00013848752354921425, 'epoch': 0.3919491525423729}
{'loss': 0.0528, 'grad_norm': 0.24014271795749664, 'learning_rate': 0.0001383437897029628, 'epoch': 0.39276401564537156}
{'loss': 0.0596, 'grad_norm': 0.32408979535102844, 'learning_rate': 0.00013820050246648015, 'epoch': 0.3935788787483703}
{'loss': 0.0476, 'grad_norm': 0.3930421769618988, 'learning_rate': 0.000138057659531704, 'epoch': 0.394393741851369}
{'loss': 0.0602, 'grad_norm': 0.5038720369338989, 'learning_rate': 0.0001379152586072369, 'epoch': 0.39520860495436766}
{'loss': 0.0505, 'grad_norm': 0.3537079095840454, 'learning_rate': 0.0001377732974181916, 'epoch': 0.3960234680573664}
{'loss': 0.0363, 'grad_norm': 0.3057255148887634, 'learning_rate': 0.00013763177370603862, 'epoch': 0.39683833116036504}
{'loss': 0.0533, 'grad_norm': 0.32346901297569275, 'learning_rate': 0.00013749068522845526, 'epoch': 0.39765319426336376}
{'loss': 0.0497, 'grad_norm': 0.17932511866092682, 'learning_rate': 0.00013735002975917636, 'epoch': 0.3984680573663624}
{'loss': 0.0499, 'grad_norm': 0.32822713255882263, 'learning_rate': 0.00013720980508784674, 'epoch': 0.39928292046936115}
{'loss': 0.0562, 'grad_norm': 0.2789861857891083, 'learning_rate': 0.00013707000901987555, 'epoch': 0.40009778357235987}
{'loss': 0.0482, 'grad_norm': 0.43652570247650146, 'learning_rate': 0.00013693063937629153, 'epoch': 0.40091264667535853}
{'loss': 0.0547, 'grad_norm': 0.42794549465179443, 'learning_rate': 0.00013679169399360088, 'epoch': 0.40172750977835725}
{'loss': 0.0472, 'grad_norm': 0.2592617869377136, 'learning_rate': 0.0001366531707236459, 'epoch': 0.4025423728813559}
{'loss': 0.0477, 'grad_norm': 0.35309603810310364, 'learning_rate': 0.00013651506743346553, 'epoch': 0.40335723598435463}
{'loss': 0.047, 'grad_norm': 0.36267000436782837, 'learning_rate': 0.00013637738200515746, 'epoch': 0.4041720990873533}
{'loss': 0.0458, 'grad_norm': 0.37391969561576843, 'learning_rate': 0.00013624011233574177, 'epoch': 0.404986962190352}
{'loss': 0.0494, 'grad_norm': 0.4689418077468872, 'learning_rate': 0.00013610325633702587, 'epoch': 0.40580182529335074}
{'loss': 0.0413, 'grad_norm': 0.15606454014778137, 'learning_rate': 0.0001359668119354712, 'epoch': 0.4066166883963494}
{'loss': 0.0552, 'grad_norm': 0.37061813473701477, 'learning_rate': 0.00013583077707206127, 'epoch': 0.4074315514993481}
[INFO|trainer.py:3515] 2025-04-10 19:33:50,732 >> Saving model checkpoint to ./test_run_outputs/checkpoint-5000
[INFO|configuration_utils.py:733] 2025-04-10 19:33:51,014 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:33:51,014 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:33:51,035 >> tokenizer config file saved in ./test_run_outputs/checkpoint-5000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:33:51,036 >> Special tokens file saved in ./test_run_outputs/checkpoint-5000/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:33:51,166 >> Deleting older checkpoint [test_run_outputs/checkpoint-4200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:33:51,443 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:33:51,443 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:35:25,400 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:35:25,400 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:35:25,400 >>   Batch size = 32
{'eval_loss': 0.04827624186873436, 'eval_accuracy': 0.15206935526537288, 'eval_runtime': 10.367, 'eval_samples_per_second': 240.184, 'eval_steps_per_second': 7.524, 'epoch': 0.4074315514993481}
{'loss': 0.0416, 'grad_norm': 0.3328638970851898, 'learning_rate': 0.00013569514970217104, 'epoch': 0.4082464146023468}
{'loss': 0.0485, 'grad_norm': 0.2418762445449829, 'learning_rate': 0.00013555992779543807, 'epoch': 0.4090612777053455}
{'loss': 0.0545, 'grad_norm': 0.41924548149108887, 'learning_rate': 0.0001354251093356346, 'epoch': 0.4098761408083442}
{'loss': 0.0451, 'grad_norm': 0.40696555376052856, 'learning_rate': 0.0001352906923205415, 'epoch': 0.4106910039113429}
{'loss': 0.0564, 'grad_norm': 0.4030598998069763, 'learning_rate': 0.00013515667476182326, 'epoch': 0.4115058670143416}
{'loss': 0.0593, 'grad_norm': 0.47419247031211853, 'learning_rate': 0.00013502305468490443, 'epoch': 0.4123207301173403}
{'loss': 0.053, 'grad_norm': 0.32932475209236145, 'learning_rate': 0.00013488983012884757, 'epoch': 0.413135593220339}
{'loss': 0.0462, 'grad_norm': 0.37098026275634766, 'learning_rate': 0.00013475699914623223, 'epoch': 0.41395045632333766}
{'loss': 0.0361, 'grad_norm': 0.18908607959747314, 'learning_rate': 0.00013462455980303557, 'epoch': 0.4147653194263364}
{'loss': 0.05, 'grad_norm': 0.2999509871006012, 'learning_rate': 0.00013449251017851385, 'epoch': 0.4155801825293351}
{'loss': 0.0459, 'grad_norm': 0.4406987130641937, 'learning_rate': 0.00013436084836508557, 'epoch': 0.41639504563233376}
{'loss': 0.047, 'grad_norm': 0.6263731718063354, 'learning_rate': 0.00013422957246821582, 'epoch': 0.4172099087353325}
{'loss': 0.0453, 'grad_norm': 0.3402961492538452, 'learning_rate': 0.00013409868060630157, 'epoch': 0.41802477183833114}
{'loss': 0.0579, 'grad_norm': 0.3306277394294739, 'learning_rate': 0.00013396817091055853, 'epoch': 0.41883963494132986}
{'loss': 0.0478, 'grad_norm': 0.8204556107521057, 'learning_rate': 0.00013383804152490914, 'epoch': 0.41965449804432853}
{'loss': 0.0338, 'grad_norm': 0.23258237540721893, 'learning_rate': 0.0001337082906058715, 'epoch': 0.42046936114732725}
{'loss': 0.047, 'grad_norm': 0.5141626000404358, 'learning_rate': 0.0001335789163224499, 'epoch': 0.42128422425032597}
{'loss': 0.0478, 'grad_norm': 0.34100309014320374, 'learning_rate': 0.0001334499168560262, 'epoch': 0.42209908735332463}
{'loss': 0.0578, 'grad_norm': 0.28400829434394836, 'learning_rate': 0.00013332129040025242, 'epoch': 0.42291395045632335}
{'loss': 0.0395, 'grad_norm': 0.19359511137008667, 'learning_rate': 0.00013319303516094456, 'epoch': 0.423728813559322}
[INFO|trainer.py:3515] 2025-04-10 19:35:35,790 >> Saving model checkpoint to ./test_run_outputs/checkpoint-5200
[INFO|configuration_utils.py:733] 2025-04-10 19:35:36,047 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:35:36,048 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:35:36,068 >> tokenizer config file saved in ./test_run_outputs/checkpoint-5200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:35:36,069 >> Special tokens file saved in ./test_run_outputs/checkpoint-5200/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:35:36,201 >> Deleting older checkpoint [test_run_outputs/checkpoint-4800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:35:36,468 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:35:36,468 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:37:09,374 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:37:09,375 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:37:09,375 >>   Batch size = 32
{'eval_loss': 0.05008916184306145, 'eval_accuracy': 0.15203568766082293, 'eval_runtime': 10.391, 'eval_samples_per_second': 239.63, 'eval_steps_per_second': 7.506, 'epoch': 0.423728813559322}
{'loss': 0.0437, 'grad_norm': 0.29616084694862366, 'learning_rate': 0.00013306514935597748, 'epoch': 0.42454367666232073}
{'loss': 0.0556, 'grad_norm': 0.2318459153175354, 'learning_rate': 0.00013293763121518095, 'epoch': 0.4253585397653194}
{'loss': 0.0485, 'grad_norm': 0.2802177369594574, 'learning_rate': 0.00013281047898023655, 'epoch': 0.4261734028683181}
{'loss': 0.0468, 'grad_norm': 0.43426576256752014, 'learning_rate': 0.000132683690904576, 'epoch': 0.42698826597131684}
{'loss': 0.042, 'grad_norm': 0.26588255167007446, 'learning_rate': 0.00013255726525328052, 'epoch': 0.4278031290743155}
{'loss': 0.045, 'grad_norm': 0.3866063058376312, 'learning_rate': 0.00013243120030298076, 'epoch': 0.4286179921773142}
{'loss': 0.0523, 'grad_norm': 0.5680568218231201, 'learning_rate': 0.00013230549434175846, 'epoch': 0.4294328552803129}
{'loss': 0.0469, 'grad_norm': 0.28832775354385376, 'learning_rate': 0.0001321801456690486, 'epoch': 0.4302477183833116}
{'loss': 0.0367, 'grad_norm': 0.2634909451007843, 'learning_rate': 0.00013205515259554295, 'epoch': 0.4310625814863103}
{'loss': 0.048, 'grad_norm': 0.458294540643692, 'learning_rate': 0.00013193051344309432, 'epoch': 0.431877444589309}
{'loss': 0.0658, 'grad_norm': 0.3544110953807831, 'learning_rate': 0.00013180622654462192, 'epoch': 0.4326923076923077}
{'loss': 0.0515, 'grad_norm': 0.22014078497886658, 'learning_rate': 0.0001316822902440179, 'epoch': 0.4335071707953064}
{'loss': 0.0502, 'grad_norm': 0.17845800518989563, 'learning_rate': 0.00013155870289605437, 'epoch': 0.4343220338983051}
{'loss': 0.0638, 'grad_norm': 0.48079389333724976, 'learning_rate': 0.00013143546286629193, 'epoch': 0.43513689700130376}
{'loss': 0.0522, 'grad_norm': 0.16081193089485168, 'learning_rate': 0.00013131256853098872, 'epoch': 0.4359517601043025}
{'loss': 0.0384, 'grad_norm': 0.3765649199485779, 'learning_rate': 0.00013119001827701065, 'epoch': 0.4367666232073012}
{'loss': 0.048, 'grad_norm': 0.2776090204715729, 'learning_rate': 0.00013106781050174247, 'epoch': 0.43758148631029986}
{'loss': 0.0579, 'grad_norm': 0.32789918780326843, 'learning_rate': 0.00013094594361299963, 'epoch': 0.4383963494132986}
{'loss': 0.0464, 'grad_norm': 0.2775024473667145, 'learning_rate': 0.00013082441602894142, 'epoch': 0.43921121251629724}
{'loss': 0.0487, 'grad_norm': 0.42132288217544556, 'learning_rate': 0.00013070322617798435, 'epoch': 0.44002607561929596}
[INFO|trainer.py:3515] 2025-04-10 19:37:19,753 >> Saving model checkpoint to ./test_run_outputs/checkpoint-5400
[INFO|configuration_utils.py:733] 2025-04-10 19:37:20,050 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:37:20,051 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:37:20,072 >> tokenizer config file saved in ./test_run_outputs/checkpoint-5400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:37:20,073 >> Special tokens file saved in ./test_run_outputs/checkpoint-5400/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:37:20,204 >> Deleting older checkpoint [test_run_outputs/checkpoint-5200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:37:20,470 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:37:20,471 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:38:54,956 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:38:54,956 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:38:54,956 >>   Batch size = 32
{'eval_loss': 0.04827682301402092, 'eval_accuracy': 0.15209580838323353, 'eval_runtime': 10.3795, 'eval_samples_per_second': 239.896, 'eval_steps_per_second': 7.515, 'epoch': 0.44002607561929596}
{'loss': 0.0576, 'grad_norm': 0.3942694365978241, 'learning_rate': 0.00013058237249871714, 'epoch': 0.44084093872229463}
{'loss': 0.0462, 'grad_norm': 0.23672470450401306, 'learning_rate': 0.000130461853439816, 'epoch': 0.44165580182529335}
{'loss': 0.0488, 'grad_norm': 0.28476738929748535, 'learning_rate': 0.00013034166745996115, 'epoch': 0.44247066492829207}
{'loss': 0.0592, 'grad_norm': 0.17534850537776947, 'learning_rate': 0.0001302218130277539, 'epoch': 0.44328552803129073}
{'loss': 0.0556, 'grad_norm': 0.2429359257221222, 'learning_rate': 0.00013010228862163478, 'epoch': 0.44410039113428945}
{'loss': 0.0534, 'grad_norm': 0.5120829939842224, 'learning_rate': 0.00012998309272980234, 'epoch': 0.4449152542372881}
{'loss': 0.0662, 'grad_norm': 0.48902127146720886, 'learning_rate': 0.00012986422385013297, 'epoch': 0.44573011734028684}
{'loss': 0.059, 'grad_norm': 0.2627165615558624, 'learning_rate': 0.00012974568049010116, 'epoch': 0.44654498044328556}
{'loss': 0.057, 'grad_norm': 0.2784505784511566, 'learning_rate': 0.00012962746116670104, 'epoch': 0.4473598435462842}
{'loss': 0.0529, 'grad_norm': 0.2674447298049927, 'learning_rate': 0.00012950956440636836, 'epoch': 0.44817470664928294}
{'loss': 0.0523, 'grad_norm': 0.38158831000328064, 'learning_rate': 0.00012939198874490322, 'epoch': 0.4489895697522816}
{'loss': 0.0542, 'grad_norm': 0.43700918555259705, 'learning_rate': 0.00012927473272739392, 'epoch': 0.4498044328552803}
{'loss': 0.0526, 'grad_norm': 0.33805879950523376, 'learning_rate': 0.00012915779490814115, 'epoch': 0.450619295958279}
{'loss': 0.0617, 'grad_norm': 0.35713306069374084, 'learning_rate': 0.0001290411738505833, 'epoch': 0.4514341590612777}
{'loss': 0.0449, 'grad_norm': 0.22533509135246277, 'learning_rate': 0.00012892486812722214, 'epoch': 0.4522490221642764}
{'loss': 0.0496, 'grad_norm': 0.5583477020263672, 'learning_rate': 0.00012880887631954964, 'epoch': 0.4530638852672751}
{'loss': 0.0464, 'grad_norm': 0.3231169581413269, 'learning_rate': 0.00012869319701797513, 'epoch': 0.4538787483702738}
{'loss': 0.0357, 'grad_norm': 0.5622417330741882, 'learning_rate': 0.00012857782882175344, 'epoch': 0.4546936114732725}
{'loss': 0.0571, 'grad_norm': 0.24709318578243256, 'learning_rate': 0.00012846277033891367, 'epoch': 0.4555084745762712}
{'loss': 0.0511, 'grad_norm': 0.28139379620552063, 'learning_rate': 0.00012834802018618863, 'epoch': 0.45632333767926986}
[INFO|trainer.py:3515] 2025-04-10 19:39:05,336 >> Saving model checkpoint to ./test_run_outputs/checkpoint-5600
[INFO|configuration_utils.py:733] 2025-04-10 19:39:05,938 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:39:05,939 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:39:05,959 >> tokenizer config file saved in ./test_run_outputs/checkpoint-5600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:39:05,961 >> Special tokens file saved in ./test_run_outputs/checkpoint-5600/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:39:06,093 >> Deleting older checkpoint [test_run_outputs/checkpoint-5400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:39:06,367 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:39:06,368 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:40:40,170 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:40:40,170 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:40:40,170 >>   Batch size = 32
{'eval_loss': 0.0514480397105217, 'eval_accuracy': 0.1520212586874444, 'eval_runtime': 10.3809, 'eval_samples_per_second': 239.864, 'eval_steps_per_second': 7.514, 'epoch': 0.45632333767926986}
{'loss': 0.0522, 'grad_norm': 0.24675174057483673, 'learning_rate': 0.00012823357698894496, 'epoch': 0.4571382007822686}
{'loss': 0.0504, 'grad_norm': 0.4010827839374542, 'learning_rate': 0.00012811943938111406, 'epoch': 0.4579530638852673}
{'loss': 0.0473, 'grad_norm': 0.25929194688796997, 'learning_rate': 0.00012800560600512346, 'epoch': 0.45876792698826596}
{'loss': 0.0513, 'grad_norm': 0.20044313371181488, 'learning_rate': 0.00012789207551182913, 'epoch': 0.4595827900912647}
{'loss': 0.049, 'grad_norm': 0.28858330845832825, 'learning_rate': 0.00012777884656044828, 'epoch': 0.46039765319426335}
{'loss': 0.0385, 'grad_norm': 0.3861227333545685, 'learning_rate': 0.00012766591781849282, 'epoch': 0.46121251629726207}
{'loss': 0.0601, 'grad_norm': 0.45197606086730957, 'learning_rate': 0.00012755328796170356, 'epoch': 0.46202737940026073}
{'loss': 0.0577, 'grad_norm': 0.2540224492549896, 'learning_rate': 0.00012744095567398492, 'epoch': 0.46284224250325945}
{'loss': 0.0543, 'grad_norm': 0.5464037656784058, 'learning_rate': 0.00012732891964734044, 'epoch': 0.46365710560625817}
{'loss': 0.0481, 'grad_norm': 0.25687873363494873, 'learning_rate': 0.00012721717858180865, 'epoch': 0.46447196870925683}
{'loss': 0.0517, 'grad_norm': 0.2093416452407837, 'learning_rate': 0.00012710573118539988, 'epoch': 0.46528683181225555}
{'loss': 0.053, 'grad_norm': 0.28608447313308716, 'learning_rate': 0.00012699457617403344, 'epoch': 0.4661016949152542}
{'loss': 0.0491, 'grad_norm': 0.21055929362773895, 'learning_rate': 0.00012688371227147547, 'epoch': 0.46691655801825294}
{'loss': 0.0439, 'grad_norm': 0.334557443857193, 'learning_rate': 0.0001267731382092775, 'epoch': 0.46773142112125166}
{'loss': 0.0491, 'grad_norm': 0.2053561806678772, 'learning_rate': 0.0001266628527267153, 'epoch': 0.4685462842242503}
{'loss': 0.0617, 'grad_norm': 0.43827584385871887, 'learning_rate': 0.00012655285457072868, 'epoch': 0.46936114732724904}
{'loss': 0.0545, 'grad_norm': 0.2996925413608551, 'learning_rate': 0.00012644314249586164, 'epoch': 0.4701760104302477}
{'loss': 0.0607, 'grad_norm': 0.40320348739624023, 'learning_rate': 0.00012633371526420313, 'epoch': 0.4709908735332464}
{'loss': 0.0501, 'grad_norm': 0.2973221242427826, 'learning_rate': 0.00012622457164532835, 'epoch': 0.4718057366362451}
{'loss': 0.0571, 'grad_norm': 0.4996911883354187, 'learning_rate': 0.00012611571041624078, 'epoch': 0.4726205997392438}
[INFO|trainer.py:3515] 2025-04-10 19:40:50,522 >> Saving model checkpoint to ./test_run_outputs/checkpoint-5800
[INFO|configuration_utils.py:733] 2025-04-10 19:40:50,788 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:40:50,788 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:40:50,809 >> tokenizer config file saved in ./test_run_outputs/checkpoint-5800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:40:50,810 >> Special tokens file saved in ./test_run_outputs/checkpoint-5800/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:40:50,941 >> Deleting older checkpoint [test_run_outputs/checkpoint-5600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:40:51,288 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:40:51,289 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3831] 2025-04-10 19:42:24,137 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:42:24,137 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:42:24,137 >>   Batch size = 32
{'eval_loss': 0.05073237791657448, 'eval_accuracy': 0.15204771180530505, 'eval_runtime': 10.3525, 'eval_samples_per_second': 240.521, 'eval_steps_per_second': 7.534, 'epoch': 0.4726205997392438}
{'loss': 0.0517, 'grad_norm': 0.5764369964599609, 'learning_rate': 0.0001260071303613144, 'epoch': 0.4734354628422425}
{'loss': 0.0641, 'grad_norm': 0.36726000905036926, 'learning_rate': 0.00012589883027223694, 'epoch': 0.4742503259452412}
{'loss': 0.0519, 'grad_norm': 0.3575509190559387, 'learning_rate': 0.00012579080894795314, 'epoch': 0.4750651890482399}
{'loss': 0.0582, 'grad_norm': 0.19491803646087646, 'learning_rate': 0.00012568306519460898, 'epoch': 0.4758800521512386}
{'loss': 0.0544, 'grad_norm': 0.6079347729682922, 'learning_rate': 0.00012557559782549622, 'epoch': 0.4766949152542373}
{'loss': 0.0496, 'grad_norm': 0.4998999834060669, 'learning_rate': 0.0001254684056609975, 'epoch': 0.47750977835723596}
{'loss': 0.0449, 'grad_norm': 0.3988025188446045, 'learning_rate': 0.00012536148752853194, 'epoch': 0.4783246414602347}
{'loss': 0.061, 'grad_norm': 0.4144149720668793, 'learning_rate': 0.0001252548422625013, 'epoch': 0.4791395045632334}
{'loss': 0.0587, 'grad_norm': 0.2365787923336029, 'learning_rate': 0.00012514846870423658, 'epoch': 0.47995436766623206}
{'loss': 0.0675, 'grad_norm': 0.3654842972755432, 'learning_rate': 0.00012504236570194514, 'epoch': 0.4807692307692308}
{'loss': 0.0485, 'grad_norm': 0.24232149124145508, 'learning_rate': 0.00012493653211065838, 'epoch': 0.48158409387222945}
{'loss': 0.0592, 'grad_norm': 0.42542141675949097, 'learning_rate': 0.0001248309667921797, 'epoch': 0.48239895697522817}
{'loss': 0.0476, 'grad_norm': 0.2880404591560364, 'learning_rate': 0.00012472566861503336, 'epoch': 0.4832138200782269}
{'loss': 0.0595, 'grad_norm': 0.3724139630794525, 'learning_rate': 0.00012462063645441324, 'epoch': 0.48402868318122555}
{'loss': 0.0422, 'grad_norm': 0.1601567566394806, 'learning_rate': 0.00012451586919213259, 'epoch': 0.48484354628422427}
{'loss': 0.0468, 'grad_norm': 0.5445806980133057, 'learning_rate': 0.00012441136571657386, 'epoch': 0.48565840938722293}
{'loss': 0.0609, 'grad_norm': 0.4027349054813385, 'learning_rate': 0.00012430712492263936, 'epoch': 0.48647327249022165}
{'loss': 0.0484, 'grad_norm': 0.5638335943222046, 'learning_rate': 0.00012420314571170198, 'epoch': 0.4872881355932203}
{'loss': 0.0595, 'grad_norm': 0.44778692722320557, 'learning_rate': 0.00012409942699155671, 'epoch': 0.48810299869621904}
{'loss': 0.049, 'grad_norm': 0.3904706537723541, 'learning_rate': 0.00012399596767637244, 'epoch': 0.48891786179921776}
[INFO|trainer.py:3515] 2025-04-10 19:42:34,522 >> Saving model checkpoint to ./test_run_outputs/checkpoint-6000
[INFO|configuration_utils.py:733] 2025-04-10 19:42:34,779 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:42:34,779 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:42:34,800 >> tokenizer config file saved in ./test_run_outputs/checkpoint-6000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:42:34,801 >> Special tokens file saved in ./test_run_outputs/checkpoint-6000/special_tokens_map.json
[INFO|trainer.py:3607] 2025-04-10 19:42:34,933 >> Deleting older checkpoint [test_run_outputs/checkpoint-5800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:42:35,196 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:42:35,197 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:2406] 2025-04-10 19:42:35,271 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2644] 2025-04-10 19:42:35,272 >> Loading best model from ./test_run_outputs/checkpoint-5000 (score: 0.04827624186873436).
[INFO|trainer.py:2447] 2025-04-10 19:42:35,293 >> Deleting older checkpoint [test_run_outputs/checkpoint-6000] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-10 19:42:35,555 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:42:35,555 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3515] 2025-04-10 19:42:35,574 >> Saving model checkpoint to ./test_run_outputs
[INFO|configuration_utils.py:733] 2025-04-10 19:42:35,827 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-10 19:42:35,827 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-10 19:42:35,847 >> tokenizer config file saved in ./test_run_outputs/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-10 19:42:35,848 >> Special tokens file saved in ./test_run_outputs/special_tokens_map.json
{'eval_loss': 0.04855913668870926, 'eval_accuracy': 0.15205973594978717, 'eval_runtime': 10.386, 'eval_samples_per_second': 239.746, 'eval_steps_per_second': 7.51, 'epoch': 0.48891786179921776}
{'train_runtime': 3172.7381, 'train_samples_per_second': 123.774, 'train_steps_per_second': 3.868, 'train_loss': 0.07906602757175764, 'epoch': 0.48891786179921776}
***** train metrics *****
  epoch                    =     0.4889
  train_loss               =     0.0791
  train_runtime            = 0:52:52.73
  train_samples            =     392702
  train_samples_per_second =    123.774
  train_steps_per_second   =      3.868
04/10/2025 19:42:35 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3831] 2025-04-10 19:42:35,946 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-10 19:42:35,946 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-10 19:42:35,946 >>   Batch size = 32
[INFO|modelcard.py:449] 2025-04-10 19:42:46,048 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.1520621407786836}]}
{'eval_loss': 0.048131465911865234, 'eval_accuracy': 0.1520621407786836, 'eval_runtime': 9.9828, 'eval_samples_per_second': 249.43, 'eval_steps_per_second': 7.813, 'epoch': 0.48891786179921776}
***** eval metrics *****
  epoch                   =     0.4889
  eval_accuracy           =     0.1521
  eval_loss               =     0.0481
  eval_runtime            = 0:00:09.98
  eval_samples            =       2490
  eval_samples_per_second =     249.43
  eval_steps_per_second   =      7.813
  perplexity              =     1.0493

============================= JOB FEEDBACK =============================

NodeName=uc3n081
Job ID: 9294
Cluster: uc3
User/Group: ka_usxcp/ka_stud
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 80
CPU Utilized: 00:54:20
CPU Efficiency: 1.25% of 3-00:41:20 core-walltime
Job Wall-clock time: 00:54:31
Memory Utilized: 2.02 GB
Memory Efficiency: 12.61% of 16.00 GB (16.00 GB/node)
