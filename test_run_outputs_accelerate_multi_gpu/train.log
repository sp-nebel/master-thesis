[2025-04-11 00:36:02,645] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 00:36:14,656] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 00:36:15,296] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 00:36:15,296] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 00:36:15,309] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 00:36:17,173] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-11 00:36:17,173] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/11/2025 00:36:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2025 00:36:17 - INFO - __main__ - Training/evaluation parameters LoRATrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=3600,
debug=[],
deepspeed=./config/deepspeed_config.json,
disable_tqdm=True,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=True,
eval_steps=200,
eval_strategy=IntervalStrategy.STEPS,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=True,
load_lora_from=None,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./test_run_outputs_accelerate_multi_gpu/runs/Apr11_00-36-13_uc3n082.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lora_config=./config/lora_config.json,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.INVERSE_SQRT,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=./test_run_outputs_accelerate_multi_gpu,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=./test_run_outputs_accelerate_multi_gpu,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=1,
seed=1,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=200,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-851ce74e92ca2b7e
04/11/2025 00:36:17 - INFO - datasets.builder - Using custom data configuration default-851ce74e92ca2b7e
Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
04/11/2025 00:36:17 - INFO - datasets.info - Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
04/11/2025 00:36:17 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/11/2025 00:36:17 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
04/11/2025 00:36:17 - INFO - datasets.builder - Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/11/2025 00:36:17 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[2025-04-11 00:36:17,847] [INFO] [comm.py:658:init_distributed] cdb=None
[INFO|configuration_utils.py:733] 2025-04-11 00:36:17,917 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 00:36:17,917 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
  "additional_loss_layer": 16,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": 1.0,
  "contrastive_loss_weight": 1.0,
  "contrastive_pooling_type": "mean",
  "distance_function": "cosine",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": false,
  "only_train_language_modeling": true,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": false,
  "use_cache": true,
  "vocab_size": 128256
}

[2025-04-11 00:36:17,977] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-11 00:36:18,015] [INFO] [comm.py:658:init_distributed] cdb=None
04/11/2025 00:36:18 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2025 00:36:18 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2025 00:36:18 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|tokenization_utils_base.py:2269] 2025-04-11 00:36:18,797 >> loading file tokenizer.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json
[INFO|tokenization_utils_base.py:2269] 2025-04-11 00:36:18,797 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2269] 2025-04-11 00:36:18,797 >> loading file special_tokens_map.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json
[INFO|tokenization_utils_base.py:2269] 2025-04-11 00:36:18,797 >> loading file tokenizer_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json
[INFO|tokenization_utils_base.py:2513] 2025-04-11 00:36:19,012 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
04/11/2025 00:36:19 - INFO - __main__ - Tokenizer is fast: True
[INFO|modeling_utils.py:3667] 2025-04-11 00:36:19,017 >> loading weights file model.safetensors from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json
[INFO|modeling_utils.py:1591] 2025-04-11 00:36:19,020 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1038] 2025-04-11 00:36:19,021 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.36it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.57it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.95it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.85it/s]
[INFO|modeling_utils.py:4499] 2025-04-11 00:36:19,393 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4507] 2025-04-11 00:36:19,393 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.04it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]adding special tokens...
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
[rank3]:[W411 00:36:19.938846476 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[INFO|configuration_utils.py:993] 2025-04-11 00:36:19,695 >> loading configuration file generation_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json
[INFO|configuration_utils.py:1038] 2025-04-11 00:36:19,695 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

adding special tokens...
04/11/2025 00:36:19 - INFO - __main__ - ================ pad, eos, bos, unk, padding ================ 
04/11/2025 00:36:19 - INFO - __main__ - <|eot_id|>, 128009
04/11/2025 00:36:19 - INFO - __main__ - <|eot_id|>, 128009
04/11/2025 00:36:19 - INFO - __main__ - <|begin_of_text|>, 128000
04/11/2025 00:36:19 - INFO - __main__ - <|reserved_special_token_0|>, 128002
04/11/2025 00:36:19 - INFO - __main__ - right
04/11/2025 00:36:19 - INFO - __main__ - lora_r : 8
04/11/2025 00:36:19 - INFO - __main__ - lora_alpha : 16
04/11/2025 00:36:19 - INFO - __main__ - lora_dropout : 0.1
04/11/2025 00:36:19 - INFO - __main__ - lora_target_modules : ['q_proj', 'v_proj']
04/11/2025 00:36:19 - INFO - __main__ - LoRA configs: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.31it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.27it/s]trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
04/11/2025 00:36:19 - INFO - __main__ - block size: 2048
Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
04/11/2025 00:36:19 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.45it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.24it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.05it/s]
Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
04/11/2025 00:36:19 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
adding special tokens...
adding special tokens...
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
[rank2]:[W411 00:36:20.285351449 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
[rank1]:[W411 00:36:20.288549089 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W411 00:36:20.332435350 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
04/11/2025 00:36:23 - INFO - __main__ - xxx: Showcase the tokenized training samples.
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[INFO|trainer.py:658] 2025-04-11 00:36:23,737 >> Using auto half precision backend
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[2025-04-11 00:36:23,885] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-11 00:36:23,886] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[2025-04-11 00:36:41,613] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...

Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 1.061474084854126 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 1.1059677600860596 seconds
Time to load fused_adam op: 1.1063425540924072 seconds
Time to load fused_adam op: 1.1051521301269531 seconds
[2025-04-11 00:36:42,723] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-04-11 00:36:42,723] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-11 00:36:42,727] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-11 00:36:42,727] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-11 00:36:42,727] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-04-11 00:36:42,728] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-04-11 00:36:42,728] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-04-11 00:36:42,728] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-11 00:36:42,728] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-11 00:36:42,920] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-11 00:36:42,921] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.03 GB         Max_CA 6 GB 
[2025-04-11 00:36:42,921] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.27 GB, percent = 6.3%
[2025-04-11 00:36:43,054] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-11 00:36:43,055] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.05 GB         Max_CA 6 GB 
[2025-04-11 00:36:43,055] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.46 GB, percent = 6.3%
[2025-04-11 00:36:43,055] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-11 00:36:43,188] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-11 00:36:43,188] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.05 GB         Max_CA 6 GB 
[2025-04-11 00:36:43,189] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.78 GB, percent = 6.3%
[2025-04-11 00:36:43,189] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-11 00:36:43,190] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler
[2025-04-11 00:36:43,190] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x148a0b4be1e0>
[2025-04-11 00:36:43,190] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[[0.9, 0.999]]
[2025-04-11 00:36:43,191] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   bfloat16_enabled ............. True
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  True
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x148a0839ee10>
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... None
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   fp16_auto_cast ............... None
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   fp16_enabled ................. False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 1
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   loss_scale ................... 1.0
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   optimizer_name ............... adam
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0005, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-11 00:36:43,192] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   steps_per_print .............. inf
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   train_batch_size ............. 128
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  32
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   world_size ................... 4
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-11 00:36:43,193] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-11 00:36:43,193] [INFO] [config.py:990:print_user_config]   json = {
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0005, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "wall_clock_breakdown": false
}
[INFO|trainer.py:2145] 2025-04-11 00:36:43,195 >> ***** Running training *****
[INFO|trainer.py:2146] 2025-04-11 00:36:43,195 >>   Num examples = 392,702
[INFO|trainer.py:2147] 2025-04-11 00:36:43,195 >>   Num Epochs = 1
[INFO|trainer.py:2148] 2025-04-11 00:36:43,195 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2151] 2025-04-11 00:36:43,195 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2152] 2025-04-11 00:36:43,195 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2153] 2025-04-11 00:36:43,195 >>   Total optimization steps = 3,068
[INFO|trainer.py:2154] 2025-04-11 00:36:43,197 >>   Number of trainable parameters = 2,293,760
[INFO|trainer.py:3831] 2025-04-11 00:36:43,201 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 00:36:43,201 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 00:36:43,201 >>   Batch size = 16
{'eval_loss': 4.181094646453857, 'eval_accuracy': 0.11632878820671909, 'eval_runtime': 3.5675, 'eval_samples_per_second': 697.97, 'eval_steps_per_second': 10.932, 'epoch': 0}
{'loss': 4.0011, 'grad_norm': 6.475341796875, 'learning_rate': 5.3763440860215054e-05, 'epoch': 0.003259452411994785}
{'loss': 2.5782, 'grad_norm': 7.224255084991455, 'learning_rate': 0.00010752688172043011, 'epoch': 0.00651890482398957}
{'loss': 0.3632, 'grad_norm': 1.1436693668365479, 'learning_rate': 0.00016129032258064516, 'epoch': 0.009778357235984355}
{'loss': 0.1961, 'grad_norm': 0.703717052936554, 'learning_rate': 0.00021505376344086021, 'epoch': 0.01303780964797914}
{'loss': 0.1392, 'grad_norm': 0.4350185990333557, 'learning_rate': 0.00026881720430107527, 'epoch': 0.016297262059973925}
{'loss': 0.1096, 'grad_norm': 0.9797947406768799, 'learning_rate': 0.0003225806451612903, 'epoch': 0.01955671447196871}
{'loss': 0.1018, 'grad_norm': 1.3039120435714722, 'learning_rate': 0.0003763440860215054, 'epoch': 0.022816166883963495}
{'loss': 0.09, 'grad_norm': 0.8019970059394836, 'learning_rate': 0.00043010752688172043, 'epoch': 0.02607561929595828}
{'loss': 0.0833, 'grad_norm': 0.4534195363521576, 'learning_rate': 0.0004838709677419355, 'epoch': 0.029335071707953065}
{'loss': 0.0805, 'grad_norm': 0.7451388239860535, 'learning_rate': 0.00048218253804964786, 'epoch': 0.03259452411994785}
{'loss': 0.0729, 'grad_norm': 0.7122637629508972, 'learning_rate': 0.00045974301121782846, 'epoch': 0.03585397653194263}
{'loss': 0.0838, 'grad_norm': 0.370341956615448, 'learning_rate': 0.00044017042154147526, 'epoch': 0.03911342894393742}
{'loss': 0.0746, 'grad_norm': 0.5737557411193848, 'learning_rate': 0.00042290206176626036, 'epoch': 0.0423728813559322}
{'loss': 0.0723, 'grad_norm': 0.482598215341568, 'learning_rate': 0.00040751862358845463, 'epoch': 0.04563233376792699}
{'loss': 0.0697, 'grad_norm': 0.2839057743549347, 'learning_rate': 0.00039370039370059055, 'epoch': 0.04889178617992177}
{'loss': 0.0716, 'grad_norm': 0.2679470479488373, 'learning_rate': 0.0003811987670494227, 'epoch': 0.05215123859191656}
{'loss': 0.0663, 'grad_norm': 0.4278111457824707, 'learning_rate': 0.0003698171249176449, 'epoch': 0.05541069100391134}
{'loss': 0.0701, 'grad_norm': 0.2978014349937439, 'learning_rate': 0.00035939764421413046, 'epoch': 0.05867014341590613}
{'loss': 0.0626, 'grad_norm': 0.676891028881073, 'learning_rate': 0.0003498119795727865, 'epoch': 0.06192959582790091}
{'loss': 0.0685, 'grad_norm': 0.3426222801208496, 'learning_rate': 0.0003409545424246464, 'epoch': 0.0651890482398957}
[INFO|trainer.py:3831] 2025-04-11 00:38:21,345 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 00:38:21,345 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 00:38:21,345 >>   Batch size = 16
{'eval_loss': 0.06155926734209061, 'eval_accuracy': 0.13350648101387586, 'eval_runtime': 3.3512, 'eval_samples_per_second': 743.015, 'eval_steps_per_second': 11.638, 'epoch': 0.0651890482398957}
E0411 00:38:31.455000 660262 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -9) local_rank: 2 (pid: 660352) of binary: /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/bin/python
Traceback (most recent call last):
  File "/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
./scripts/run_clm_lora.py FAILED
-------------------------------------------------------
Failures:
[1]:
  time      : 2025-04-11_00:38:31
  host      : uc3n082.localdomain
  rank      : 3 (local_rank: 3)
  exitcode  : -9 (pid: 660353)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 660353
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-11_00:38:31
  host      : uc3n082.localdomain
  rank      : 2 (local_rank: 2)
  exitcode  : -9 (pid: 660352)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 660352
=======================================================
[2025-04-11 01:10:46,475] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:10:58,598] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:10:58,600] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:10:58,601] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:10:58,608] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-11 01:11:01,303] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-11 01:11:01,317] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-11 01:11:01,317] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-11 01:11:01,355] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-11 01:11:01,470] [INFO] [comm.py:658:init_distributed] cdb=None
04/11/2025 01:11:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2025 01:11:01 - INFO - __main__ - Training/evaluation parameters LoRATrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=3600,
debug=[],
deepspeed=./config/deepspeed_config.json,
disable_tqdm=True,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=True,
eval_steps=200,
eval_strategy=IntervalStrategy.STEPS,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=True,
load_lora_from=None,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./test_run_outputs_accelerate_multi_gpu/runs/Apr11_01-10-57_uc3n082.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lora_config=./config/lora_config.json,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.INVERSE_SQRT,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=./test_run_outputs_accelerate_multi_gpu,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=./test_run_outputs_accelerate_multi_gpu,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=1,
seed=1,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=200,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-851ce74e92ca2b7e
04/11/2025 01:11:02 - INFO - datasets.builder - Using custom data configuration default-851ce74e92ca2b7e
Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
04/11/2025 01:11:02 - INFO - datasets.info - Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
04/11/2025 01:11:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/11/2025 01:11:02 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
04/11/2025 01:11:02 - INFO - datasets.builder - Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/11/2025 01:11:02 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/11/2025 01:11:02 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2025 01:11:02 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2025 01:11:02 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:733] 2025-04-11 01:11:02,438 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:11:02,439 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
  "additional_loss_layer": 16,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": 1.0,
  "contrastive_loss_weight": 1.0,
  "contrastive_pooling_type": "mean",
  "distance_function": "cosine",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": false,
  "only_train_language_modeling": true,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": false,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2269] 2025-04-11 01:11:02,556 >> loading file tokenizer.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json
[INFO|tokenization_utils_base.py:2269] 2025-04-11 01:11:02,556 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2269] 2025-04-11 01:11:02,556 >> loading file special_tokens_map.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json
[INFO|tokenization_utils_base.py:2269] 2025-04-11 01:11:02,556 >> loading file tokenizer_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json
[INFO|tokenization_utils_base.py:2513] 2025-04-11 01:11:02,757 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
04/11/2025 01:11:02 - INFO - __main__ - Tokenizer is fast: True
[INFO|modeling_utils.py:3667] 2025-04-11 01:11:02,763 >> loading weights file model.safetensors from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json
[INFO|modeling_utils.py:1591] 2025-04-11 01:11:02,766 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1038] 2025-04-11 01:11:02,767 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.04it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.61it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.51it/s]
[INFO|modeling_utils.py:4499] 2025-04-11 01:11:03,160 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4507] 2025-04-11 01:11:03,160 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][INFO|configuration_utils.py:993] 2025-04-11 01:11:03,283 >> loading configuration file generation_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json
[INFO|configuration_utils.py:1038] 2025-04-11 01:11:03,283 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

adding special tokens...
04/11/2025 01:11:03 - INFO - __main__ - ================ pad, eos, bos, unk, padding ================ 
04/11/2025 01:11:03 - INFO - __main__ - <|eot_id|>, 128009
04/11/2025 01:11:03 - INFO - __main__ - <|eot_id|>, 128009
04/11/2025 01:11:03 - INFO - __main__ - <|begin_of_text|>, 128000
04/11/2025 01:11:03 - INFO - __main__ - <|reserved_special_token_0|>, 128002
04/11/2025 01:11:03 - INFO - __main__ - right
04/11/2025 01:11:03 - INFO - __main__ - lora_r : 8
04/11/2025 01:11:03 - INFO - __main__ - lora_alpha : 16
04/11/2025 01:11:03 - INFO - __main__ - lora_dropout : 0.1
04/11/2025 01:11:03 - INFO - __main__ - lora_target_modules : ['q_proj', 'v_proj']
04/11/2025 01:11:03 - INFO - __main__ - LoRA configs: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
04/11/2025 01:11:03 - INFO - __main__ - block size: 2048
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.61it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.34it/s]Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
04/11/2025 01:11:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.42it/s]Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
04/11/2025 01:11:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.68it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.49it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.38it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.21it/s]
adding special tokens...
adding special tokens...
[rank0]:[W411 01:11:03.917593139 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
[rank2]:[W411 01:11:03.936898339 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
[rank3]:[W411 01:11:03.957371091 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
adding special tokens...
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
[rank1]:[W411 01:11:03.020185732 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
04/11/2025 01:11:06 - INFO - __main__ - xxx: Showcase the tokenized training samples.
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[INFO|trainer.py:658] 2025-04-11 01:11:07,351 >> Using auto half precision backend
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[2025-04-11 01:11:07,544] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-11 01:11:07,544] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-11 01:11:35,325] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 1.3694212436676025 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 1.406226634979248 seconds
[2025-04-11 01:11:36,739] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-04-11 01:11:36,739] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
Time to load fused_adam op: 1.4070625305175781 seconds
Loading extension module fused_adam...
[2025-04-11 01:11:36,743] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-11 01:11:36,743] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-11 01:11:36,743] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
Time to load fused_adam op: 1.4069392681121826 seconds
[2025-04-11 01:11:36,743] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-04-11 01:11:36,743] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-04-11 01:11:36,743] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-11 01:11:36,743] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-11 01:11:36,943] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-11 01:11:36,943] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.03 GB         Max_CA 6 GB 
[2025-04-11 01:11:36,944] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 37.86 GB, percent = 5.0%
[2025-04-11 01:11:37,095] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-11 01:11:37,096] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.05 GB         Max_CA 6 GB 
[2025-04-11 01:11:37,096] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.11 GB, percent = 5.0%
[2025-04-11 01:11:37,096] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-11 01:11:37,246] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-11 01:11:37,247] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.05 GB         Max_CA 6 GB 
[2025-04-11 01:11:37,247] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.51 GB, percent = 5.1%
[2025-04-11 01:11:37,248] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-11 01:11:37,248] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler
[2025-04-11 01:11:37,248] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x14a618926ab0>
[2025-04-11 01:11:37,248] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[[0.9, 0.999]]
[2025-04-11 01:11:37,249] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   bfloat16_enabled ............. True
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  True
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14a628859fd0>
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... None
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   fp16_auto_cast ............... None
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   fp16_enabled ................. False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 1
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   loss_scale ................... 1.0
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-11 01:11:37,250] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   optimizer_name ............... adam
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0005, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   steps_per_print .............. inf
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   train_batch_size ............. 128
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  32
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   world_size ................... 4
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-11 01:11:37,251] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-11 01:11:37,251] [INFO] [config.py:990:print_user_config]   json = {
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0005, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "wall_clock_breakdown": false
}
[INFO|trainer.py:2145] 2025-04-11 01:11:37,253 >> ***** Running training *****
[INFO|trainer.py:2146] 2025-04-11 01:11:37,253 >>   Num examples = 392,702
[INFO|trainer.py:2147] 2025-04-11 01:11:37,253 >>   Num Epochs = 1
[INFO|trainer.py:2148] 2025-04-11 01:11:37,253 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2151] 2025-04-11 01:11:37,253 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2152] 2025-04-11 01:11:37,253 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2153] 2025-04-11 01:11:37,253 >>   Total optimization steps = 3,068
[INFO|trainer.py:2154] 2025-04-11 01:11:37,254 >>   Number of trainable parameters = 2,293,760
[INFO|trainer.py:3831] 2025-04-11 01:11:37,258 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:11:37,258 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:11:37,258 >>   Batch size = 16
{'eval_loss': 4.181094646453857, 'eval_accuracy': 0.11632878820671909, 'eval_runtime': 3.5449, 'eval_samples_per_second': 702.415, 'eval_steps_per_second': 11.002, 'epoch': 0}
{'loss': 4.0011, 'grad_norm': 6.475341796875, 'learning_rate': 5.3763440860215054e-05, 'epoch': 0.003259452411994785}
{'loss': 2.5782, 'grad_norm': 7.224255084991455, 'learning_rate': 0.00010752688172043011, 'epoch': 0.00651890482398957}
{'loss': 0.3632, 'grad_norm': 1.1436693668365479, 'learning_rate': 0.00016129032258064516, 'epoch': 0.009778357235984355}
{'loss': 0.1961, 'grad_norm': 0.703717052936554, 'learning_rate': 0.00021505376344086021, 'epoch': 0.01303780964797914}
{'loss': 0.1392, 'grad_norm': 0.4350185990333557, 'learning_rate': 0.00026881720430107527, 'epoch': 0.016297262059973925}
{'loss': 0.1096, 'grad_norm': 0.9797947406768799, 'learning_rate': 0.0003225806451612903, 'epoch': 0.01955671447196871}
{'loss': 0.1018, 'grad_norm': 1.3039120435714722, 'learning_rate': 0.0003763440860215054, 'epoch': 0.022816166883963495}
{'loss': 0.09, 'grad_norm': 0.8019970059394836, 'learning_rate': 0.00043010752688172043, 'epoch': 0.02607561929595828}
{'loss': 0.0833, 'grad_norm': 0.4534195363521576, 'learning_rate': 0.0004838709677419355, 'epoch': 0.029335071707953065}
{'loss': 0.0805, 'grad_norm': 0.7451388239860535, 'learning_rate': 0.00048218253804964786, 'epoch': 0.03259452411994785}
{'loss': 0.0729, 'grad_norm': 0.7122637629508972, 'learning_rate': 0.00045974301121782846, 'epoch': 0.03585397653194263}
{'loss': 0.0838, 'grad_norm': 0.370341956615448, 'learning_rate': 0.00044017042154147526, 'epoch': 0.03911342894393742}
{'loss': 0.0746, 'grad_norm': 0.5737557411193848, 'learning_rate': 0.00042290206176626036, 'epoch': 0.0423728813559322}
{'loss': 0.0723, 'grad_norm': 0.482598215341568, 'learning_rate': 0.00040751862358845463, 'epoch': 0.04563233376792699}
{'loss': 0.0697, 'grad_norm': 0.2839057743549347, 'learning_rate': 0.00039370039370059055, 'epoch': 0.04889178617992177}
{'loss': 0.0716, 'grad_norm': 0.2679470479488373, 'learning_rate': 0.0003811987670494227, 'epoch': 0.05215123859191656}
{'loss': 0.0663, 'grad_norm': 0.4278111457824707, 'learning_rate': 0.0003698171249176449, 'epoch': 0.05541069100391134}
{'loss': 0.0701, 'grad_norm': 0.2978014349937439, 'learning_rate': 0.00035939764421413046, 'epoch': 0.05867014341590613}
{'loss': 0.0626, 'grad_norm': 0.676891028881073, 'learning_rate': 0.0003498119795727865, 'epoch': 0.06192959582790091}
{'loss': 0.0685, 'grad_norm': 0.3426222801208496, 'learning_rate': 0.0003409545424246464, 'epoch': 0.0651890482398957}
[INFO|trainer.py:3831] 2025-04-11 01:13:15,182 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:13:15,182 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:13:15,182 >>   Batch size = 16
{'eval_loss': 0.06155926734209061, 'eval_accuracy': 0.13350648101387586, 'eval_runtime': 3.3879, 'eval_samples_per_second': 734.965, 'eval_steps_per_second': 11.511, 'epoch': 0.0651890482398957}
[INFO|trainer.py:3515] 2025-04-11 01:13:19,865 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-200
[INFO|configuration_utils.py:733] 2025-04-11 01:13:20,180 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:13:20,180 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:13:20,195 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:13:20,196 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/special_tokens_map.json
[2025-04-11 01:13:20,312] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-04-11 01:13:20,321] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2025-04-11 01:13:20,321] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2025-04-11 01:13:21,293] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2025-04-11 01:13:21,296] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:13:21,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:13:21,309] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:13:21,309] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|configuration_utils.py:733] 2025-04-11 01:13:21,563 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:13:21,564 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0634, 'grad_norm': 0.29729172587394714, 'learning_rate': 0.0003327375628243462, 'epoch': 0.06844850065189048}
{'loss': 0.0606, 'grad_norm': 0.25571343302726746, 'learning_rate': 0.00032508740083524954, 'epoch': 0.07170795306388526}
{'loss': 0.0721, 'grad_norm': 0.2791629731655121, 'learning_rate': 0.0003179417502023588, 'epoch': 0.07496740547588006}
{'loss': 0.0655, 'grad_norm': 0.4568226635456085, 'learning_rate': 0.00031124748994971834, 'epoch': 0.07822685788787484}
{'loss': 0.0627, 'grad_norm': 0.31770098209381104, 'learning_rate': 0.00030495901363953817, 'epoch': 0.08148631029986962}
{'loss': 0.0637, 'grad_norm': 0.33484312891960144, 'learning_rate': 0.0002990369156526948, 'epoch': 0.0847457627118644}
{'loss': 0.0518, 'grad_norm': 0.27704551815986633, 'learning_rate': 0.0002934469476943168, 'epoch': 0.0880052151238592}
{'loss': 0.0583, 'grad_norm': 0.1985533982515335, 'learning_rate': 0.00028815918219920447, 'epoch': 0.09126466753585398}
{'loss': 0.0606, 'grad_norm': 0.3099674582481384, 'learning_rate': 0.00028314733583967105, 'epoch': 0.09452411994784876}
{'loss': 0.0592, 'grad_norm': 0.3921815752983093, 'learning_rate': 0.00027838821814150114, 'epoch': 0.09778357235984354}
{'loss': 0.0604, 'grad_norm': 0.241848424077034, 'learning_rate': 0.00027386127875258305, 'epoch': 0.10104302477183832}
{'loss': 0.0614, 'grad_norm': 0.2843869626522064, 'learning_rate': 0.0002695482331605978, 'epoch': 0.10430247718383312}
{'loss': 0.0595, 'grad_norm': 0.38516637682914734, 'learning_rate': 0.00026543275128466237, 'epoch': 0.1075619295958279}
{'loss': 0.06, 'grad_norm': 0.3079080581665039, 'learning_rate': 0.0002615001968281792, 'epoch': 0.11082138200782268}
{'loss': 0.0694, 'grad_norm': 0.33549943566322327, 'learning_rate': 0.00025773740789526733, 'epoch': 0.11408083441981746}
{'loss': 0.0627, 'grad_norm': 0.22225433588027954, 'learning_rate': 0.0002541325113662818, 'epoch': 0.11734028683181226}
{'loss': 0.0598, 'grad_norm': 0.3368756175041199, 'learning_rate': 0.00025067476505990355, 'epoch': 0.12059973924380704}
{'loss': 0.0594, 'grad_norm': 0.2834566831588745, 'learning_rate': 0.0002473544228962074, 'epoch': 0.12385919165580182}
{'loss': 0.0601, 'grad_norm': 0.25408756732940674, 'learning_rate': 0.00024416261920159817, 'epoch': 0.1271186440677966}
{'loss': 0.0578, 'grad_norm': 0.2036093771457672, 'learning_rate': 0.00024109126902482393, 'epoch': 0.1303780964797914}
[INFO|trainer.py:3831] 2025-04-11 01:14:54,465 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:14:54,465 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:14:54,465 >>   Batch size = 16
{'eval_loss': 0.05581275746226311, 'eval_accuracy': 0.13360267416973282, 'eval_runtime': 3.3828, 'eval_samples_per_second': 736.078, 'eval_steps_per_second': 11.529, 'epoch': 0.1303780964797914}
[INFO|trainer.py:3515] 2025-04-11 01:14:59,043 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-400
[INFO|configuration_utils.py:733] 2025-04-11 01:14:59,303 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:14:59,303 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:14:59,319 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:14:59,321 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/special_tokens_map.json
[2025-04-11 01:14:59,428] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-04-11 01:14:59,436] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2025-04-11 01:14:59,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2025-04-11 01:15:00,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2025-04-11 01:15:00,412] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:15:00,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:15:00,425] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:15:00,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|configuration_utils.py:733] 2025-04-11 01:15:00,673 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:15:00,673 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0612, 'grad_norm': 0.22801560163497925, 'learning_rate': 0.000238132981909627, 'epoch': 0.13363754889178617}
{'loss': 0.0588, 'grad_norm': 0.3386504054069519, 'learning_rate': 0.00023528098702858003, 'epoch': 0.13689700130378096}
{'loss': 0.054, 'grad_norm': 0.24290193617343903, 'learning_rate': 0.00023252906795035426, 'epoch': 0.14015645371577576}
{'loss': 0.0591, 'grad_norm': 0.40227210521698, 'learning_rate': 0.00022987150560891423, 'epoch': 0.14341590612777053}
{'loss': 0.061, 'grad_norm': 0.25004178285598755, 'learning_rate': 0.0002273030282830976, 'epoch': 0.14667535853976532}
{'loss': 0.0592, 'grad_norm': 0.43192288279533386, 'learning_rate': 0.00022481876759040728, 'epoch': 0.14993481095176012}
{'loss': 0.0584, 'grad_norm': 0.2631235718727112, 'learning_rate': 0.0002224142196586877, 'epoch': 0.15319426336375488}
{'loss': 0.0576, 'grad_norm': 0.23236486315727234, 'learning_rate': 0.00022008521077073763, 'epoch': 0.15645371577574968}
{'loss': 0.0542, 'grad_norm': 0.47378450632095337, 'learning_rate': 0.0002178278668853844, 'epoch': 0.15971316818774445}
{'loss': 0.0556, 'grad_norm': 0.2766890525817871, 'learning_rate': 0.00021563858652847822, 'epoch': 0.16297262059973924}
{'loss': 0.0526, 'grad_norm': 0.2232927680015564, 'learning_rate': 0.00021351401662213573, 'epoch': 0.16623207301173404}
{'loss': 0.0581, 'grad_norm': 0.2900848984718323, 'learning_rate': 0.00021145103088313018, 'epoch': 0.1694915254237288}
{'loss': 0.0586, 'grad_norm': 0.24277061223983765, 'learning_rate': 0.0002094467104738145, 'epoch': 0.1727509778357236}
{'loss': 0.0569, 'grad_norm': 0.33006253838539124, 'learning_rate': 0.00020749832663314552, 'epoch': 0.1760104302477184}
{'loss': 0.0611, 'grad_norm': 0.24546699225902557, 'learning_rate': 0.00020560332505270259, 'epoch': 0.17926988265971316}
{'loss': 0.0601, 'grad_norm': 0.25890809297561646, 'learning_rate': 0.00020375931179422732, 'epoch': 0.18252933507170796}
{'loss': 0.0588, 'grad_norm': 0.2103394865989685, 'learning_rate': 0.00020196404057210414, 'epoch': 0.18578878748370273}
{'loss': 0.0618, 'grad_norm': 0.3027498126029968, 'learning_rate': 0.00020021540124713614, 'epoch': 0.18904823989569752}
{'loss': 0.054, 'grad_norm': 0.28454816341400146, 'learning_rate': 0.0001985114093975884, 'epoch': 0.19230769230769232}
{'loss': 0.0565, 'grad_norm': 0.24706344306468964, 'learning_rate': 0.00019685019685029527, 'epoch': 0.19556714471968709}
[INFO|trainer.py:3831] 2025-04-11 01:16:35,539 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:16:35,539 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:16:35,539 >>   Batch size = 16
{'eval_loss': 0.05300065129995346, 'eval_accuracy': 0.13361469831421494, 'eval_runtime': 3.3983, 'eval_samples_per_second': 732.72, 'eval_steps_per_second': 11.476, 'epoch': 0.19556714471968709}
[INFO|trainer.py:3515] 2025-04-11 01:16:40,144 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-600
[INFO|configuration_utils.py:733] 2025-04-11 01:16:40,386 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:16:40,387 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:16:40,401 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:16:40,403 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/special_tokens_map.json
[2025-04-11 01:16:40,510] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2025-04-11 01:16:40,519] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/mp_rank_00_model_states.pt
[2025-04-11 01:16:40,519] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/mp_rank_00_model_states.pt...
[2025-04-11 01:16:41,478] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/mp_rank_00_model_states.pt.
[2025-04-11 01:16:41,480] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:16:41,490] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:16:41,492] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:16:41,492] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[INFO|trainer.py:3607] 2025-04-11 01:16:41,500 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-11 01:16:41,943 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:16:41,943 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.051, 'grad_norm': 0.3618951737880707, 'learning_rate': 0.00019523000306909962, 'epoch': 0.19882659713168188}
{'loss': 0.0559, 'grad_norm': 0.4594591557979584, 'learning_rate': 0.00019364916731037085, 'epoch': 0.20208604954367665}
{'loss': 0.0584, 'grad_norm': 0.24958327412605286, 'learning_rate': 0.0001921061214661363, 'epoch': 0.20534550195567144}
{'loss': 0.0559, 'grad_norm': 0.3014610707759857, 'learning_rate': 0.00019059938352471135, 'epoch': 0.20860495436766624}
{'loss': 0.063, 'grad_norm': 0.32150718569755554, 'learning_rate': 0.00018912755158683456, 'epoch': 0.211864406779661}
{'loss': 0.0603, 'grad_norm': 0.24348366260528564, 'learning_rate': 0.00018768929838238707, 'epoch': 0.2151238591916558}
{'loss': 0.0567, 'grad_norm': 0.20405063033103943, 'learning_rate': 0.0001862833662389464, 'epoch': 0.2183833116036506}
{'loss': 0.0521, 'grad_norm': 0.27425551414489746, 'learning_rate': 0.00018490856245882245, 'epoch': 0.22164276401564537}
{'loss': 0.0561, 'grad_norm': 0.25703343749046326, 'learning_rate': 0.00018356375506595264, 'epoch': 0.22490221642764016}
{'loss': 0.0554, 'grad_norm': 0.24313108623027802, 'learning_rate': 0.00018224786888818676, 'epoch': 0.22816166883963493}
{'loss': 0.0552, 'grad_norm': 0.21311093866825104, 'learning_rate': 0.00018095988194414647, 'epoch': 0.23142112125162972}
{'loss': 0.0526, 'grad_norm': 0.2095230221748352, 'learning_rate': 0.00017969882210706523, 'epoch': 0.23468057366362452}
{'loss': 0.0558, 'grad_norm': 0.21875272691249847, 'learning_rate': 0.00017846376402085983, 'epoch': 0.2379400260756193}
{'loss': 0.0602, 'grad_norm': 0.42362791299819946, 'learning_rate': 0.00017725382624620242, 'epoch': 0.24119947848761408}
{'loss': 0.0548, 'grad_norm': 0.24822181463241577, 'learning_rate': 0.0001760681686165901, 'epoch': 0.24445893089960888}
{'loss': 0.0558, 'grad_norm': 0.3080138564109802, 'learning_rate': 0.00017490598978639325, 'epoch': 0.24771838331160365}
{'loss': 0.0548, 'grad_norm': 0.22705844044685364, 'learning_rate': 0.00017376652495462177, 'epoch': 0.25097783572359844}
{'loss': 0.0568, 'grad_norm': 0.1883169263601303, 'learning_rate': 0.00017264904374971875, 'epoch': 0.2542372881355932}
{'loss': 0.0536, 'grad_norm': 0.305027574300766, 'learning_rate': 0.00017155284826208934, 'epoch': 0.25749674054758803}
{'loss': 0.0601, 'grad_norm': 0.29442980885505676, 'learning_rate': 0.0001704772712123232, 'epoch': 0.2607561929595828}
[INFO|trainer.py:3831] 2025-04-11 01:18:15,713 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:18:15,713 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:18:15,713 >>   Batch size = 16
{'eval_loss': 0.05379442498087883, 'eval_accuracy': 0.1336411514320756, 'eval_runtime': 3.397, 'eval_samples_per_second': 733.001, 'eval_steps_per_second': 11.481, 'epoch': 0.2607561929595828}
[INFO|trainer.py:3515] 2025-04-11 01:18:20,329 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-800
[INFO|configuration_utils.py:733] 2025-04-11 01:18:20,584 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:18:20,585 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:18:20,600 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:18:20,601 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/special_tokens_map.json
[2025-04-11 01:18:20,707] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
[2025-04-11 01:18:20,716] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/mp_rank_00_model_states.pt
[2025-04-11 01:18:20,716] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/mp_rank_00_model_states.pt...
[2025-04-11 01:18:21,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/mp_rank_00_model_states.pt.
[2025-04-11 01:18:21,676] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:18:21,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:18:21,688] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:18:21,688] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[INFO|trainer.py:3607] 2025-04-11 01:18:21,695 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-11 01:18:21,951 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:18:21,952 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0579, 'grad_norm': 0.21903987228870392, 'learning_rate': 0.00016942167424418786, 'epoch': 0.26401564537157757}
{'loss': 0.0537, 'grad_norm': 0.221793070435524, 'learning_rate': 0.0001683854463324707, 'epoch': 0.26727509778357234}
{'loss': 0.0551, 'grad_norm': 0.28474846482276917, 'learning_rate': 0.00016736800229664894, 'epoch': 0.27053455019556716}
{'loss': 0.0547, 'grad_norm': 0.46689990162849426, 'learning_rate': 0.0001663687814121731, 'epoch': 0.2737940026075619}
{'loss': 0.0561, 'grad_norm': 0.1990491896867752, 'learning_rate': 0.00016538724611187705, 'epoch': 0.2770534550195567}
{'loss': 0.0558, 'grad_norm': 0.3514726459980011, 'learning_rate': 0.00016442288077068298, 'epoch': 0.2803129074315515}
{'loss': 0.0524, 'grad_norm': 0.31406471133232117, 'learning_rate': 0.00016347519056735942, 'epoch': 0.2835723598435463}
{'loss': 0.0572, 'grad_norm': 0.3588111400604248, 'learning_rate': 0.00016254370041762477, 'epoch': 0.28683181225554105}
{'loss': 0.0502, 'grad_norm': 0.2632896602153778, 'learning_rate': 0.00016162795397337053, 'epoch': 0.2900912646675359}
{'loss': 0.0578, 'grad_norm': 0.21954305469989777, 'learning_rate': 0.00016072751268321592, 'epoch': 0.29335071707953064}
{'loss': 0.0574, 'grad_norm': 0.25954148173332214, 'learning_rate': 0.00015984195491000025, 'epoch': 0.2966101694915254}
{'loss': 0.0497, 'grad_norm': 0.3536636233329773, 'learning_rate': 0.0001589708751011794, 'epoch': 0.29986962190352023}
{'loss': 0.057, 'grad_norm': 0.2310969978570938, 'learning_rate': 0.00015811388300841897, 'epoch': 0.303129074315515}
{'loss': 0.0572, 'grad_norm': 0.18835866451263428, 'learning_rate': 0.0001572706029529724, 'epoch': 0.30638852672750977}
{'loss': 0.0582, 'grad_norm': 0.2912144958972931, 'learning_rate': 0.00015644067313370366, 'epoch': 0.30964797913950454}
{'loss': 0.0496, 'grad_norm': 0.24995091557502747, 'learning_rate': 0.00015562374497485917, 'epoch': 0.31290743155149936}
{'loss': 0.0586, 'grad_norm': 0.254310667514801, 'learning_rate': 0.00015481948251091802, 'epoch': 0.3161668839634941}
{'loss': 0.0561, 'grad_norm': 0.26981624960899353, 'learning_rate': 0.0001540275618060559, 'epoch': 0.3194263363754889}
{'loss': 0.0529, 'grad_norm': 0.3228878676891327, 'learning_rate': 0.00015324767040594283, 'epoch': 0.3226857887874837}
{'loss': 0.0546, 'grad_norm': 0.31385937333106995, 'learning_rate': 0.00015247950681976908, 'epoch': 0.3259452411994785}
[INFO|trainer.py:3831] 2025-04-11 01:19:58,225 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-11 01:19:58,225 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-11 01:19:58,225 >>   Batch size = 16
{'eval_loss': 0.05158242955803871, 'eval_accuracy': 0.13367962869441838, 'eval_runtime': 3.3959, 'eval_samples_per_second': 733.239, 'eval_steps_per_second': 11.484, 'epoch': 0.3259452411994785}
[INFO|trainer.py:3515] 2025-04-11 01:20:02,842 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000
[INFO|configuration_utils.py:733] 2025-04-11 01:20:03,083 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:20:03,083 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-11 01:20:03,098 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-11 01:20:03,099 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/special_tokens_map.json
[2025-04-11 01:20:03,205] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2025-04-11 01:20:03,214] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2025-04-11 01:20:03,214] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[2025-04-11 01:20:04,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2025-04-11 01:20:04,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-11 01:20:04,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-11 01:20:04,203] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-11 01:20:04,203] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[INFO|trainer.py:3607] 2025-04-11 01:20:04,210 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-11 01:20:04,454 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-11 01:20:04,455 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[2025-04-14 04:11:25,807] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-14 04:11:41,960] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-14 04:11:41,961] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-14 04:11:41,964] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-14 04:11:41,970] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-14 04:11:44,648] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-14 04:11:44,690] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-14 04:11:44,719] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-14 04:11:44,741] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-14 04:11:44,741] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/14/2025 04:11:45 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/14/2025 04:11:45 - INFO - __main__ - Training/evaluation parameters LoRATrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=3600,
debug=[],
deepspeed=./config/deepspeed_config.json,
disable_tqdm=True,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=True,
eval_steps=200,
eval_strategy=IntervalStrategy.STEPS,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=True,
load_lora_from=None,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./test_run_outputs_accelerate_multi_gpu/runs/Apr14_04-11-41_uc3n081.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lora_config=./config/lora_config.json,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.INVERSE_SQRT,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=./test_run_outputs_accelerate_multi_gpu,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=./test_run_outputs_accelerate_multi_gpu,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=1,
seed=1,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=200,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-851ce74e92ca2b7e
04/14/2025 04:11:45 - INFO - datasets.builder - Using custom data configuration default-851ce74e92ca2b7e
Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
04/14/2025 04:11:45 - INFO - datasets.info - Loading Dataset Infos from /pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
04/14/2025 04:11:45 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/14/2025 04:11:45 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
04/14/2025 04:11:45 - INFO - datasets.builder - Found cached dataset json (/home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/14/2025 04:11:45 - INFO - datasets.info - Loading Dataset info from /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
04/14/2025 04:11:45 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
04/14/2025 04:11:45 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
04/14/2025 04:11:45 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:733] 2025-04-14 04:11:45,986 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:11:45,986 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
  "additional_loss_layer": 16,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": 1.0,
  "contrastive_loss_weight": 1.0,
  "contrastive_pooling_type": "mean",
  "distance_function": "cosine",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": false,
  "only_train_language_modeling": true,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": false,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2269] 2025-04-14 04:11:46,107 >> loading file tokenizer.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json
[INFO|tokenization_utils_base.py:2269] 2025-04-14 04:11:46,107 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2269] 2025-04-14 04:11:46,107 >> loading file special_tokens_map.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json
[INFO|tokenization_utils_base.py:2269] 2025-04-14 04:11:46,107 >> loading file tokenizer_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json
[INFO|tokenization_utils_base.py:2513] 2025-04-14 04:11:46,346 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
04/14/2025 04:11:46 - INFO - __main__ - Tokenizer is fast: True
[INFO|modeling_utils.py:3667] 2025-04-14 04:11:46,354 >> loading weights file model.safetensors from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json
[INFO|modeling_utils.py:1591] 2025-04-14 04:11:46,358 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1038] 2025-04-14 04:11:46,359 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.48it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.01it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.58it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.92it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.98it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.80it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.75it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.46it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.28it/s]
[INFO|modeling_utils.py:4499] 2025-04-14 04:11:46,874 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4507] 2025-04-14 04:11:46,874 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]
adding special tokens...
adding special tokens...
[INFO|configuration_utils.py:993] 2025-04-14 04:11:46,997 >> loading configuration file generation_config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json
[INFO|configuration_utils.py:1038] 2025-04-14 04:11:46,997 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

adding special tokens...
adding special tokens...
04/14/2025 04:11:47 - INFO - __main__ - ================ pad, eos, bos, unk, padding ================ 
04/14/2025 04:11:47 - INFO - __main__ - <|eot_id|>, 128009
04/14/2025 04:11:47 - INFO - __main__ - <|eot_id|>, 128009
04/14/2025 04:11:47 - INFO - __main__ - <|begin_of_text|>, 128000
04/14/2025 04:11:47 - INFO - __main__ - <|reserved_special_token_0|>, 128002
04/14/2025 04:11:47 - INFO - __main__ - right
04/14/2025 04:11:47 - INFO - __main__ - lora_r : 8
04/14/2025 04:11:47 - INFO - __main__ - lora_alpha : 16
04/14/2025 04:11:47 - INFO - __main__ - lora_dropout : 0.1
04/14/2025 04:11:47 - INFO - __main__ - lora_target_modules : ['q_proj', 'v_proj']
04/14/2025 04:11:47 - INFO - __main__ - LoRA configs: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
04/14/2025 04:11:47 - INFO - __main__ - block size: 2048
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 3072)
        (layers): ModuleList(
          (0-27): 28 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((3072,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
    )
  )
)
[rank3]:[W414 04:11:47.289354935 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W414 04:11:47.289359935 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W414 04:11:47.289357885 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
04/14/2025 04:11:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9faf32f636901887.arrow
Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
04/14/2025 04:11:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/datasets/json/default-851ce74e92ca2b7e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-91e4f075ad38c423.arrow
[rank0]:[W414 04:11:47.748375572 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
04/14/2025 04:11:50 - INFO - __main__ - xxx: Showcase the tokenized training samples.
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 35, 25296, 1268, 279, 2768, 41302, 323, 31178, 527, 5552, 13, 1442, 279, 31178, 11263, 505, 279, 41302, 11, 3373, 330, 306, 607, 479, 3343, 1442, 814, 43561, 1855, 1023, 11, 3373, 330, 8386, 329, 2538, 3343, 1442, 279, 12518, 656, 539, 87092, 6463, 43561, 1855, 1023, 11, 3373, 330, 60668, 3343, 12029, 1082, 25, 35455, 1870, 12932, 1940, 41133, 706, 1403, 6913, 15696, 482, 2027, 323, 54242, 662, 39515, 78, 13491, 25, 5761, 323, 54242, 527, 1148, 1304, 12932, 1940, 41133, 990, 6905, 14711, 6075, 25, 60668, 524, 82, 29], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 60668, 524, 82, 29]}
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[INFO|trainer.py:658] 2025-04-14 04:11:51,204 >> Using auto half precision backend
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[2025-04-14 04:11:51,413] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown
[2025-04-14 04:11:51,413] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/src/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
[2025-04-14 04:12:22,985] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Using /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /pfs/data6/home/ka/ka_stud/ka_usxcp/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...
/pfs/data6/home/ka/ka_stud/ka_usxcp/master-thesis/.env/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 1.911966323852539 seconds
Time to load fused_adam op: 1.9101755619049072 seconds
Time to load fused_adam op: 1.9073948860168457 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 2.007727861404419 seconds
[2025-04-14 04:12:25,002] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-04-14 04:12:25,002] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-14 04:12:25,006] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-14 04:12:25,006] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-14 04:12:25,006] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-04-14 04:12:25,006] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-04-14 04:12:25,006] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-04-14 04:12:25,006] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-14 04:12:25,006] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-14 04:12:25,185] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-14 04:12:25,186] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.03 GB         Max_CA 6 GB 
[2025-04-14 04:12:25,186] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.41 GB, percent = 3.9%
[2025-04-14 04:12:25,310] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-14 04:12:25,311] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.05 GB         Max_CA 6 GB 
[2025-04-14 04:12:25,311] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.57 GB, percent = 3.9%
[2025-04-14 04:12:25,311] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized
[2025-04-14 04:12:25,434] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-14 04:12:25,435] [INFO] [utils.py:782:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.05 GB         Max_CA 6 GB 
[2025-04-14 04:12:25,435] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.79 GB, percent = 3.9%
[2025-04-14 04:12:25,436] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-14 04:12:25,436] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler
[2025-04-14 04:12:25,436] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x14c0e94b4890>
[2025-04-14 04:12:25,436] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[[0.9, 0.999]]
[2025-04-14 04:12:25,437] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   amp_params ................... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   bfloat16_enabled ............. True
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  True
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14c0ea369760>
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   dump_state ................... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... None
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   fp16_auto_cast ............... None
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   fp16_enabled ................. False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   global_rank .................. 0
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 1
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   loss_scale ................... 1.0
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2025-04-14 04:12:25,438] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   optimizer_name ............... adam
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   optimizer_params ............. {'lr': 0.0005, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   pld_params ................... False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   steps_per_print .............. inf
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   train_batch_size ............. 128
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  32
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   world_size ................... 4
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-14 04:12:25,439] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2025-04-14 04:12:25,439] [INFO] [config.py:990:print_user_config]   json = {
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0005, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "wall_clock_breakdown": false
}
[INFO|trainer.py:2145] 2025-04-14 04:12:25,441 >> ***** Running training *****
[INFO|trainer.py:2146] 2025-04-14 04:12:25,441 >>   Num examples = 392,702
[INFO|trainer.py:2147] 2025-04-14 04:12:25,441 >>   Num Epochs = 1
[INFO|trainer.py:2148] 2025-04-14 04:12:25,441 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2151] 2025-04-14 04:12:25,441 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2152] 2025-04-14 04:12:25,441 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2153] 2025-04-14 04:12:25,441 >>   Total optimization steps = 3,068
[INFO|trainer.py:2154] 2025-04-14 04:12:25,442 >>   Number of trainable parameters = 2,293,760
[INFO|trainer.py:3831] 2025-04-14 04:12:25,446 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:12:25,446 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:12:25,446 >>   Batch size = 16
{'eval_loss': 4.181094646453857, 'eval_accuracy': 0.11632878820671909, 'eval_runtime': 3.8984, 'eval_samples_per_second': 638.731, 'eval_steps_per_second': 10.004, 'epoch': 0}
{'loss': 4.0011, 'grad_norm': 6.475341796875, 'learning_rate': 5.3763440860215054e-05, 'epoch': 0.003259452411994785}
{'loss': 2.5782, 'grad_norm': 7.224255084991455, 'learning_rate': 0.00010752688172043011, 'epoch': 0.00651890482398957}
{'loss': 0.3632, 'grad_norm': 1.1436693668365479, 'learning_rate': 0.00016129032258064516, 'epoch': 0.009778357235984355}
{'loss': 0.1961, 'grad_norm': 0.703717052936554, 'learning_rate': 0.00021505376344086021, 'epoch': 0.01303780964797914}
{'loss': 0.1392, 'grad_norm': 0.4350185990333557, 'learning_rate': 0.00026881720430107527, 'epoch': 0.016297262059973925}
{'loss': 0.1096, 'grad_norm': 0.9797947406768799, 'learning_rate': 0.0003225806451612903, 'epoch': 0.01955671447196871}
{'loss': 0.1018, 'grad_norm': 1.3039120435714722, 'learning_rate': 0.0003763440860215054, 'epoch': 0.022816166883963495}
{'loss': 0.09, 'grad_norm': 0.8019970059394836, 'learning_rate': 0.00043010752688172043, 'epoch': 0.02607561929595828}
{'loss': 0.0833, 'grad_norm': 0.4534195363521576, 'learning_rate': 0.0004838709677419355, 'epoch': 0.029335071707953065}
{'loss': 0.0805, 'grad_norm': 0.7451388239860535, 'learning_rate': 0.00048218253804964786, 'epoch': 0.03259452411994785}
{'loss': 0.0729, 'grad_norm': 0.7122637629508972, 'learning_rate': 0.00045974301121782846, 'epoch': 0.03585397653194263}
{'loss': 0.0838, 'grad_norm': 0.370341956615448, 'learning_rate': 0.00044017042154147526, 'epoch': 0.03911342894393742}
{'loss': 0.0746, 'grad_norm': 0.5737557411193848, 'learning_rate': 0.00042290206176626036, 'epoch': 0.0423728813559322}
{'loss': 0.0723, 'grad_norm': 0.482598215341568, 'learning_rate': 0.00040751862358845463, 'epoch': 0.04563233376792699}
{'loss': 0.0697, 'grad_norm': 0.2839057743549347, 'learning_rate': 0.00039370039370059055, 'epoch': 0.04889178617992177}
{'loss': 0.0716, 'grad_norm': 0.2679470479488373, 'learning_rate': 0.0003811987670494227, 'epoch': 0.05215123859191656}
{'loss': 0.0663, 'grad_norm': 0.4278111457824707, 'learning_rate': 0.0003698171249176449, 'epoch': 0.05541069100391134}
{'loss': 0.0701, 'grad_norm': 0.2978014349937439, 'learning_rate': 0.00035939764421413046, 'epoch': 0.05867014341590613}
{'loss': 0.0626, 'grad_norm': 0.676891028881073, 'learning_rate': 0.0003498119795727865, 'epoch': 0.06192959582790091}
{'loss': 0.0685, 'grad_norm': 0.3426222801208496, 'learning_rate': 0.0003409545424246464, 'epoch': 0.0651890482398957}
[INFO|trainer.py:3831] 2025-04-14 04:14:15,661 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:14:15,662 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:14:15,662 >>   Batch size = 16
{'eval_loss': 0.06155926734209061, 'eval_accuracy': 0.13350648101387586, 'eval_runtime': 3.669, 'eval_samples_per_second': 678.652, 'eval_steps_per_second': 10.629, 'epoch': 0.0651890482398957}
[INFO|trainer.py:3515] 2025-04-14 04:14:20,754 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-200
[INFO|configuration_utils.py:733] 2025-04-14 04:14:21,056 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:14:21,056 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:14:21,070 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:14:21,071 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/special_tokens_map.json
[2025-04-14 04:14:21,186] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-04-14 04:14:21,195] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2025-04-14 04:14:21,195] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2025-04-14 04:14:22,145] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2025-04-14 04:14:22,148] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:14:22,157] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:14:22,160] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:14:22,160] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:14:22,168 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:14:22,434 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:14:22,434 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0634, 'grad_norm': 0.29729172587394714, 'learning_rate': 0.0003327375628243462, 'epoch': 0.06844850065189048}
{'loss': 0.0606, 'grad_norm': 0.25571343302726746, 'learning_rate': 0.00032508740083524954, 'epoch': 0.07170795306388526}
{'loss': 0.0721, 'grad_norm': 0.2791629731655121, 'learning_rate': 0.0003179417502023588, 'epoch': 0.07496740547588006}
{'loss': 0.0655, 'grad_norm': 0.4568226635456085, 'learning_rate': 0.00031124748994971834, 'epoch': 0.07822685788787484}
{'loss': 0.0627, 'grad_norm': 0.31770098209381104, 'learning_rate': 0.00030495901363953817, 'epoch': 0.08148631029986962}
{'loss': 0.0637, 'grad_norm': 0.33484312891960144, 'learning_rate': 0.0002990369156526948, 'epoch': 0.0847457627118644}
{'loss': 0.0518, 'grad_norm': 0.27704551815986633, 'learning_rate': 0.0002934469476943168, 'epoch': 0.0880052151238592}
{'loss': 0.0583, 'grad_norm': 0.1985533982515335, 'learning_rate': 0.00028815918219920447, 'epoch': 0.09126466753585398}
{'loss': 0.0606, 'grad_norm': 0.3099674582481384, 'learning_rate': 0.00028314733583967105, 'epoch': 0.09452411994784876}
{'loss': 0.0592, 'grad_norm': 0.3921815752983093, 'learning_rate': 0.00027838821814150114, 'epoch': 0.09778357235984354}
{'loss': 0.0604, 'grad_norm': 0.241848424077034, 'learning_rate': 0.00027386127875258305, 'epoch': 0.10104302477183832}
{'loss': 0.0614, 'grad_norm': 0.2843869626522064, 'learning_rate': 0.0002695482331605978, 'epoch': 0.10430247718383312}
{'loss': 0.0595, 'grad_norm': 0.38516637682914734, 'learning_rate': 0.00026543275128466237, 'epoch': 0.1075619295958279}
{'loss': 0.06, 'grad_norm': 0.3079080581665039, 'learning_rate': 0.0002615001968281792, 'epoch': 0.11082138200782268}
{'loss': 0.0694, 'grad_norm': 0.33549943566322327, 'learning_rate': 0.00025773740789526733, 'epoch': 0.11408083441981746}
{'loss': 0.0627, 'grad_norm': 0.22225433588027954, 'learning_rate': 0.0002541325113662818, 'epoch': 0.11734028683181226}
{'loss': 0.0598, 'grad_norm': 0.3368756175041199, 'learning_rate': 0.00025067476505990355, 'epoch': 0.12059973924380704}
{'loss': 0.0594, 'grad_norm': 0.2834566831588745, 'learning_rate': 0.0002473544228962074, 'epoch': 0.12385919165580182}
{'loss': 0.0601, 'grad_norm': 0.25408756732940674, 'learning_rate': 0.00024416261920159817, 'epoch': 0.1271186440677966}
{'loss': 0.0578, 'grad_norm': 0.2036093771457672, 'learning_rate': 0.00024109126902482393, 'epoch': 0.1303780964797914}
[INFO|trainer.py:3831] 2025-04-14 04:16:07,022 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:16:07,022 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:16:07,022 >>   Batch size = 16
{'eval_loss': 0.05581275746226311, 'eval_accuracy': 0.13360267416973282, 'eval_runtime': 3.6821, 'eval_samples_per_second': 676.236, 'eval_steps_per_second': 10.592, 'epoch': 0.1303780964797914}
[INFO|trainer.py:3515] 2025-04-14 04:16:12,003 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-400
[INFO|configuration_utils.py:733] 2025-04-14 04:16:12,247 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:16:12,247 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:16:12,262 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:16:12,263 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/special_tokens_map.json
[2025-04-14 04:16:12,374] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-04-14 04:16:12,383] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2025-04-14 04:16:12,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2025-04-14 04:16:13,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2025-04-14 04:16:13,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:16:13,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:16:13,351] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:16:13,351] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:16:13,357 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:16:13,759 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:16:13,760 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0612, 'grad_norm': 0.22801560163497925, 'learning_rate': 0.000238132981909627, 'epoch': 0.13363754889178617}
{'loss': 0.0588, 'grad_norm': 0.3386504054069519, 'learning_rate': 0.00023528098702858003, 'epoch': 0.13689700130378096}
{'loss': 0.054, 'grad_norm': 0.24290193617343903, 'learning_rate': 0.00023252906795035426, 'epoch': 0.14015645371577576}
{'loss': 0.0591, 'grad_norm': 0.40227210521698, 'learning_rate': 0.00022987150560891423, 'epoch': 0.14341590612777053}
{'loss': 0.061, 'grad_norm': 0.25004178285598755, 'learning_rate': 0.0002273030282830976, 'epoch': 0.14667535853976532}
{'loss': 0.0592, 'grad_norm': 0.43192288279533386, 'learning_rate': 0.00022481876759040728, 'epoch': 0.14993481095176012}
{'loss': 0.0584, 'grad_norm': 0.2631235718727112, 'learning_rate': 0.0002224142196586877, 'epoch': 0.15319426336375488}
{'loss': 0.0576, 'grad_norm': 0.23236486315727234, 'learning_rate': 0.00022008521077073763, 'epoch': 0.15645371577574968}
{'loss': 0.0542, 'grad_norm': 0.47378450632095337, 'learning_rate': 0.0002178278668853844, 'epoch': 0.15971316818774445}
{'loss': 0.0556, 'grad_norm': 0.2766890525817871, 'learning_rate': 0.00021563858652847822, 'epoch': 0.16297262059973924}
{'loss': 0.0526, 'grad_norm': 0.2232927680015564, 'learning_rate': 0.00021351401662213573, 'epoch': 0.16623207301173404}
{'loss': 0.0581, 'grad_norm': 0.2900848984718323, 'learning_rate': 0.00021145103088313018, 'epoch': 0.1694915254237288}
{'loss': 0.0586, 'grad_norm': 0.24277061223983765, 'learning_rate': 0.0002094467104738145, 'epoch': 0.1727509778357236}
{'loss': 0.0569, 'grad_norm': 0.33006253838539124, 'learning_rate': 0.00020749832663314552, 'epoch': 0.1760104302477184}
{'loss': 0.0611, 'grad_norm': 0.24546699225902557, 'learning_rate': 0.00020560332505270259, 'epoch': 0.17926988265971316}
{'loss': 0.0601, 'grad_norm': 0.25890809297561646, 'learning_rate': 0.00020375931179422732, 'epoch': 0.18252933507170796}
{'loss': 0.0588, 'grad_norm': 0.2103394865989685, 'learning_rate': 0.00020196404057210414, 'epoch': 0.18578878748370273}
{'loss': 0.0618, 'grad_norm': 0.3027498126029968, 'learning_rate': 0.00020021540124713614, 'epoch': 0.18904823989569752}
{'loss': 0.054, 'grad_norm': 0.28454816341400146, 'learning_rate': 0.0001985114093975884, 'epoch': 0.19230769230769232}
{'loss': 0.0565, 'grad_norm': 0.24706344306468964, 'learning_rate': 0.00019685019685029527, 'epoch': 0.19556714471968709}
[INFO|trainer.py:3831] 2025-04-14 04:17:59,607 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:17:59,607 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:17:59,607 >>   Batch size = 16
{'eval_loss': 0.05300065129995346, 'eval_accuracy': 0.13361469831421494, 'eval_runtime': 3.6944, 'eval_samples_per_second': 674.0, 'eval_steps_per_second': 10.557, 'epoch': 0.19556714471968709}
[INFO|trainer.py:3515] 2025-04-14 04:18:04,610 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-600
[INFO|configuration_utils.py:733] 2025-04-14 04:18:04,857 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:18:04,858 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:18:04,873 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:18:04,874 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/special_tokens_map.json
[2025-04-14 04:18:04,980] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2025-04-14 04:18:04,993] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/mp_rank_00_model_states.pt
[2025-04-14 04:18:04,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/mp_rank_00_model_states.pt...
[2025-04-14 04:18:05,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/mp_rank_00_model_states.pt.
[2025-04-14 04:18:05,942] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:18:05,951] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:18:05,954] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:18:05,954] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:18:05,960 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:18:06,486 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:18:06,487 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.051, 'grad_norm': 0.3618951737880707, 'learning_rate': 0.00019523000306909962, 'epoch': 0.19882659713168188}
{'loss': 0.0559, 'grad_norm': 0.4594591557979584, 'learning_rate': 0.00019364916731037085, 'epoch': 0.20208604954367665}
{'loss': 0.0584, 'grad_norm': 0.24958327412605286, 'learning_rate': 0.0001921061214661363, 'epoch': 0.20534550195567144}
{'loss': 0.0559, 'grad_norm': 0.3014610707759857, 'learning_rate': 0.00019059938352471135, 'epoch': 0.20860495436766624}
{'loss': 0.063, 'grad_norm': 0.32150718569755554, 'learning_rate': 0.00018912755158683456, 'epoch': 0.211864406779661}
{'loss': 0.0603, 'grad_norm': 0.24348366260528564, 'learning_rate': 0.00018768929838238707, 'epoch': 0.2151238591916558}
{'loss': 0.0567, 'grad_norm': 0.20405063033103943, 'learning_rate': 0.0001862833662389464, 'epoch': 0.2183833116036506}
{'loss': 0.0521, 'grad_norm': 0.27425551414489746, 'learning_rate': 0.00018490856245882245, 'epoch': 0.22164276401564537}
{'loss': 0.0561, 'grad_norm': 0.25703343749046326, 'learning_rate': 0.00018356375506595264, 'epoch': 0.22490221642764016}
{'loss': 0.0554, 'grad_norm': 0.24313108623027802, 'learning_rate': 0.00018224786888818676, 'epoch': 0.22816166883963493}
{'loss': 0.0552, 'grad_norm': 0.21311093866825104, 'learning_rate': 0.00018095988194414647, 'epoch': 0.23142112125162972}
{'loss': 0.0526, 'grad_norm': 0.2095230221748352, 'learning_rate': 0.00017969882210706523, 'epoch': 0.23468057366362452}
{'loss': 0.0558, 'grad_norm': 0.21875272691249847, 'learning_rate': 0.00017846376402085983, 'epoch': 0.2379400260756193}
{'loss': 0.0602, 'grad_norm': 0.42362791299819946, 'learning_rate': 0.00017725382624620242, 'epoch': 0.24119947848761408}
{'loss': 0.0548, 'grad_norm': 0.24822181463241577, 'learning_rate': 0.0001760681686165901, 'epoch': 0.24445893089960888}
{'loss': 0.0558, 'grad_norm': 0.3080138564109802, 'learning_rate': 0.00017490598978639325, 'epoch': 0.24771838331160365}
{'loss': 0.0548, 'grad_norm': 0.22705844044685364, 'learning_rate': 0.00017376652495462177, 'epoch': 0.25097783572359844}
{'loss': 0.0568, 'grad_norm': 0.1883169263601303, 'learning_rate': 0.00017264904374971875, 'epoch': 0.2542372881355932}
{'loss': 0.0536, 'grad_norm': 0.305027574300766, 'learning_rate': 0.00017155284826208934, 'epoch': 0.25749674054758803}
{'loss': 0.0601, 'grad_norm': 0.29442980885505676, 'learning_rate': 0.0001704772712123232, 'epoch': 0.2607561929595828}
[INFO|trainer.py:3831] 2025-04-14 04:19:52,296 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:19:52,296 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:19:52,296 >>   Batch size = 16
{'eval_loss': 0.05379442498087883, 'eval_accuracy': 0.1336411514320756, 'eval_runtime': 3.6812, 'eval_samples_per_second': 676.407, 'eval_steps_per_second': 10.594, 'epoch': 0.2607561929595828}
[INFO|trainer.py:3515] 2025-04-14 04:19:57,278 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-800
[INFO|configuration_utils.py:733] 2025-04-14 04:19:57,567 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:19:57,568 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:19:57,583 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:19:57,584 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/special_tokens_map.json
[2025-04-14 04:19:57,694] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
[2025-04-14 04:19:57,703] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/mp_rank_00_model_states.pt
[2025-04-14 04:19:57,703] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/mp_rank_00_model_states.pt...
[2025-04-14 04:19:58,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/mp_rank_00_model_states.pt.
[2025-04-14 04:19:58,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:19:58,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:19:58,672] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:19:58,672] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:19:58,678 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:19:59,059 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:19:59,059 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0579, 'grad_norm': 0.21903987228870392, 'learning_rate': 0.00016942167424418786, 'epoch': 0.26401564537157757}
{'loss': 0.0537, 'grad_norm': 0.221793070435524, 'learning_rate': 0.0001683854463324707, 'epoch': 0.26727509778357234}
{'loss': 0.0551, 'grad_norm': 0.28474846482276917, 'learning_rate': 0.00016736800229664894, 'epoch': 0.27053455019556716}
{'loss': 0.0547, 'grad_norm': 0.46689990162849426, 'learning_rate': 0.0001663687814121731, 'epoch': 0.2737940026075619}
{'loss': 0.0561, 'grad_norm': 0.1990491896867752, 'learning_rate': 0.00016538724611187705, 'epoch': 0.2770534550195567}
{'loss': 0.0558, 'grad_norm': 0.3514726459980011, 'learning_rate': 0.00016442288077068298, 'epoch': 0.2803129074315515}
{'loss': 0.0524, 'grad_norm': 0.31406471133232117, 'learning_rate': 0.00016347519056735942, 'epoch': 0.2835723598435463}
{'loss': 0.0572, 'grad_norm': 0.3588111400604248, 'learning_rate': 0.00016254370041762477, 'epoch': 0.28683181225554105}
{'loss': 0.0502, 'grad_norm': 0.2632896602153778, 'learning_rate': 0.00016162795397337053, 'epoch': 0.2900912646675359}
{'loss': 0.0578, 'grad_norm': 0.21954305469989777, 'learning_rate': 0.00016072751268321592, 'epoch': 0.29335071707953064}
{'loss': 0.0574, 'grad_norm': 0.25954148173332214, 'learning_rate': 0.00015984195491000025, 'epoch': 0.2966101694915254}
{'loss': 0.0497, 'grad_norm': 0.3536636233329773, 'learning_rate': 0.0001589708751011794, 'epoch': 0.29986962190352023}
{'loss': 0.057, 'grad_norm': 0.2310969978570938, 'learning_rate': 0.00015811388300841897, 'epoch': 0.303129074315515}
{'loss': 0.0572, 'grad_norm': 0.18835866451263428, 'learning_rate': 0.0001572706029529724, 'epoch': 0.30638852672750977}
{'loss': 0.0582, 'grad_norm': 0.2912144958972931, 'learning_rate': 0.00015644067313370366, 'epoch': 0.30964797913950454}
{'loss': 0.0496, 'grad_norm': 0.24995091557502747, 'learning_rate': 0.00015562374497485917, 'epoch': 0.31290743155149936}
{'loss': 0.0586, 'grad_norm': 0.254310667514801, 'learning_rate': 0.00015481948251091802, 'epoch': 0.3161668839634941}
{'loss': 0.0561, 'grad_norm': 0.26981624960899353, 'learning_rate': 0.0001540275618060559, 'epoch': 0.3194263363754889}
{'loss': 0.0529, 'grad_norm': 0.3228878676891327, 'learning_rate': 0.00015324767040594283, 'epoch': 0.3226857887874837}
{'loss': 0.0546, 'grad_norm': 0.31385937333106995, 'learning_rate': 0.00015247950681976908, 'epoch': 0.3259452411994785}
[INFO|trainer.py:3831] 2025-04-14 04:21:46,663 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:21:46,664 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:21:46,664 >>   Batch size = 16
{'eval_loss': 0.05158242955803871, 'eval_accuracy': 0.13367962869441838, 'eval_runtime': 3.6925, 'eval_samples_per_second': 674.347, 'eval_steps_per_second': 10.562, 'epoch': 0.3259452411994785}
[INFO|trainer.py:3515] 2025-04-14 04:21:51,644 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000
[INFO|configuration_utils.py:733] 2025-04-14 04:21:51,882 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:21:51,883 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:21:51,897 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:21:51,899 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/special_tokens_map.json
[2025-04-14 04:21:52,000] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2025-04-14 04:21:52,017] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2025-04-14 04:21:52,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[2025-04-14 04:21:52,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2025-04-14 04:21:52,977] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:21:52,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:21:52,991] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:21:52,991] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:21:52,998 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:21:53,409 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:21:53,409 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0522, 'grad_norm': 0.3802783489227295, 'learning_rate': 0.0001517227800305479, 'epoch': 0.32920469361147325}
{'loss': 0.0619, 'grad_norm': 0.2843168079853058, 'learning_rate': 0.0001509772090318894, 'epoch': 0.3324641460234681}
{'loss': 0.0557, 'grad_norm': 0.1969309002161026, 'learning_rate': 0.00015024252238957046, 'epoch': 0.33572359843546284}
{'loss': 0.0558, 'grad_norm': 0.3148787319660187, 'learning_rate': 0.0001495184578263474, 'epoch': 0.3389830508474576}
{'loss': 0.0464, 'grad_norm': 0.2448345422744751, 'learning_rate': 0.00014880476182856897, 'epoch': 0.34224250325945244}
{'loss': 0.0591, 'grad_norm': 0.31788185238838196, 'learning_rate': 0.00014810118927324973, 'epoch': 0.3455019556714472}
{'loss': 0.0548, 'grad_norm': 0.2175469994544983, 'learning_rate': 0.00014740750307435787, 'epoch': 0.34876140808344197}
{'loss': 0.0542, 'grad_norm': 0.269064724445343, 'learning_rate': 0.0001467234738471584, 'epoch': 0.3520208604954368}
{'loss': 0.0509, 'grad_norm': 0.20918138325214386, 'learning_rate': 0.0001460488795895326, 'epoch': 0.35528031290743156}
{'loss': 0.0577, 'grad_norm': 0.3534844219684601, 'learning_rate': 0.00014538350537926796, 'epoch': 0.35853976531942633}
{'loss': 0.0553, 'grad_norm': 0.244426429271698, 'learning_rate': 0.0001447271430863815, 'epoch': 0.3617992177314211}
{'loss': 0.0526, 'grad_norm': 0.1895652860403061, 'learning_rate': 0.00014407959109960224, 'epoch': 0.3650586701434159}
{'loss': 0.0544, 'grad_norm': 0.395649254322052, 'learning_rate': 0.0001434406540661958, 'epoch': 0.3683181225554107}
{'loss': 0.0519, 'grad_norm': 0.18028442561626434, 'learning_rate': 0.00014281014264436984, 'epoch': 0.37157757496740546}
{'loss': 0.0568, 'grad_norm': 0.3227783441543579, 'learning_rate': 0.00014218787326754638, 'epoch': 0.3748370273794003}
{'loss': 0.0522, 'grad_norm': 0.2102653980255127, 'learning_rate': 0.00014157366791983552, 'epoch': 0.37809647979139505}
{'loss': 0.0524, 'grad_norm': 0.24878555536270142, 'learning_rate': 0.0001409673539220868, 'epoch': 0.3813559322033898}
{'loss': 0.0539, 'grad_norm': 0.27988487482070923, 'learning_rate': 0.0001403687637279337, 'epoch': 0.38461538461538464}
{'loss': 0.0538, 'grad_norm': 0.18835659325122833, 'learning_rate': 0.0001397777347292852, 'epoch': 0.3878748370273794}
{'loss': 0.0502, 'grad_norm': 0.1703224629163742, 'learning_rate': 0.00013919410907075057, 'epoch': 0.39113428943937417}
[INFO|trainer.py:3831] 2025-04-14 04:23:38,016 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:23:38,016 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:23:38,016 >>   Batch size = 16
{'eval_loss': 0.05057536065578461, 'eval_accuracy': 0.13365317557655773, 'eval_runtime': 3.6667, 'eval_samples_per_second': 679.087, 'eval_steps_per_second': 10.636, 'epoch': 0.39113428943937417}
[INFO|trainer.py:3515] 2025-04-14 04:23:43,004 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-1200
[INFO|configuration_utils.py:733] 2025-04-14 04:23:43,249 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:23:43,250 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:23:43,264 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:23:43,265 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1200/special_tokens_map.json
[2025-04-14 04:23:43,367] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!
[2025-04-14 04:23:43,385] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt
[2025-04-14 04:23:43,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt...
[2025-04-14 04:23:44,333] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt.
[2025-04-14 04:23:44,335] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:23:44,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:23:44,348] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:23:44,348] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:23:44,356 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:23:44,606 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:23:44,607 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.055, 'grad_norm': 0.30981001257896423, 'learning_rate': 0.00013861773347251732, 'epoch': 0.394393741851369}
{'loss': 0.0506, 'grad_norm': 0.17579348385334015, 'learning_rate': 0.00013804845906123083, 'epoch': 0.39765319426336376}
{'loss': 0.0512, 'grad_norm': 0.24495992064476013, 'learning_rate': 0.00013748614120845143, 'epoch': 0.40091264667535853}
{'loss': 0.0497, 'grad_norm': 0.20015433430671692, 'learning_rate': 0.00013693063937629153, 'epoch': 0.4041720990873533}
{'loss': 0.047, 'grad_norm': 0.2530771493911743, 'learning_rate': 0.00013638181696985855, 'epoch': 0.4074315514993481}
{'loss': 0.0481, 'grad_norm': 0.2111484557390213, 'learning_rate': 0.00013583954119615154, 'epoch': 0.4106910039113429}
{'loss': 0.0574, 'grad_norm': 0.29477810859680176, 'learning_rate': 0.0001353036829290808, 'epoch': 0.41395045632333766}
{'loss': 0.0456, 'grad_norm': 0.23383569717407227, 'learning_rate': 0.0001347741165802989, 'epoch': 0.4172099087353325}
{'loss': 0.0471, 'grad_norm': 0.17756624519824982, 'learning_rate': 0.00013425071997554984, 'epoch': 0.42046936114732725}
{'loss': 0.0491, 'grad_norm': 0.2021167129278183, 'learning_rate': 0.0001337333742362593, 'epoch': 0.423728813559322}
{'loss': 0.0484, 'grad_norm': 0.25423678755760193, 'learning_rate': 0.00013322196366610662, 'epoch': 0.42698826597131684}
{'loss': 0.0473, 'grad_norm': 0.21238046884536743, 'learning_rate': 0.00013271637564233119, 'epoch': 0.4302477183833116}
{'loss': 0.052, 'grad_norm': 0.22992482781410217, 'learning_rate': 0.0001322165005115428, 'epoch': 0.4335071707953064}
{'loss': 0.0519, 'grad_norm': 0.22078344225883484, 'learning_rate': 0.00013172223148981616, 'epoch': 0.4367666232073012}
{'loss': 0.048, 'grad_norm': 0.2674277722835541, 'learning_rate': 0.00013123346456686352, 'epoch': 0.44002607561929596}
{'loss': 0.0536, 'grad_norm': 0.22187675535678864, 'learning_rate': 0.0001307500984140896, 'epoch': 0.44328552803129073}
{'loss': 0.0581, 'grad_norm': 0.20666614174842834, 'learning_rate': 0.0001302720342963448, 'epoch': 0.44654498044328556}
{'loss': 0.0541, 'grad_norm': 0.20667360723018646, 'learning_rate': 0.00012979917598720156, 'epoch': 0.4498044328552803}
{'loss': 0.0538, 'grad_norm': 0.38550978899002075, 'learning_rate': 0.00012933142968758976, 'epoch': 0.4530638852672751}
{'loss': 0.046, 'grad_norm': 0.24837824702262878, 'learning_rate': 0.00012886870394763366, 'epoch': 0.45632333767926986}
[INFO|trainer.py:3831] 2025-04-14 04:25:29,290 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:25:29,291 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:25:29,291 >>   Batch size = 16
{'eval_loss': 0.04952642694115639, 'eval_accuracy': 0.13369405766779693, 'eval_runtime': 3.8653, 'eval_samples_per_second': 644.188, 'eval_steps_per_second': 10.09, 'epoch': 0.45632333767926986}
[INFO|trainer.py:3515] 2025-04-14 04:25:34,483 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-1400
[INFO|configuration_utils.py:733] 2025-04-14 04:25:34,739 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:25:34,739 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:25:34,753 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:25:34,754 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1400/special_tokens_map.json
[2025-04-14 04:25:34,861] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1400 is about to be saved!
[2025-04-14 04:25:34,873] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt
[2025-04-14 04:25:34,873] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt...
[2025-04-14 04:25:35,822] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt.
[2025-04-14 04:25:35,825] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:25:35,835] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:25:35,838] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:25:35,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:25:35,847 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-1000] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:25:36,256 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:25:36,257 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0516, 'grad_norm': 0.23359186947345734, 'learning_rate': 0.0001284109095915439, 'epoch': 0.4595827900912647}
{'loss': 0.0519, 'grad_norm': 0.23540928959846497, 'learning_rate': 0.00012795795964542307, 'epoch': 0.46284224250325945}
{'loss': 0.0531, 'grad_norm': 0.2518773674964905, 'learning_rate': 0.0001275097692678536, 'epoch': 0.4661016949152542}
{'loss': 0.0497, 'grad_norm': 0.29643377661705017, 'learning_rate': 0.0001270662556831409, 'epoch': 0.46936114732724904}
{'loss': 0.0571, 'grad_norm': 0.2655169665813446, 'learning_rate': 0.00012662733811709338, 'epoch': 0.4726205997392438}
{'loss': 0.0583, 'grad_norm': 0.22206127643585205, 'learning_rate': 0.0001261929377352258, 'epoch': 0.4758800521512386}
{'loss': 0.054, 'grad_norm': 0.23079603910446167, 'learning_rate': 0.00012576297758327864, 'epoch': 0.4791395045632334}
{'loss': 0.0626, 'grad_norm': 0.3279036581516266, 'learning_rate': 0.00012533738252995178, 'epoch': 0.48239895697522817}
{'loss': 0.0527, 'grad_norm': 0.24740943312644958, 'learning_rate': 0.00012491607921175554, 'epoch': 0.48565840938722293}
{'loss': 0.0538, 'grad_norm': 0.21972717344760895, 'learning_rate': 0.00012449899597988734, 'epoch': 0.48891786179921776}
{'loss': 0.044, 'grad_norm': 0.19433115422725677, 'learning_rate': 0.00012408606284904637, 'epoch': 0.4921773142112125}
{'loss': 0.0564, 'grad_norm': 0.27380919456481934, 'learning_rate': 0.0001236772114481037, 'epoch': 0.4954367666232073}
{'loss': 0.0538, 'grad_norm': 0.29961347579956055, 'learning_rate': 0.00012327237497254827, 'epoch': 0.49869621903520206}
{'loss': 0.0525, 'grad_norm': 0.1847962886095047, 'learning_rate': 0.0001228714881386345, 'epoch': 0.5019556714471969}
{'loss': 0.0489, 'grad_norm': 0.19304807484149933, 'learning_rate': 0.0001224744871391589, 'epoch': 0.5052151238591917}
{'loss': 0.0548, 'grad_norm': 0.25330838561058044, 'learning_rate': 0.00012208130960079908, 'epoch': 0.5084745762711864}
{'loss': 0.0577, 'grad_norm': 0.25500357151031494, 'learning_rate': 0.00012169189454294904, 'epoch': 0.5117340286831812}
{'loss': 0.0485, 'grad_norm': 0.21839891374111176, 'learning_rate': 0.00012130618233799019, 'epoch': 0.5149934810951761}
{'loss': 0.0514, 'grad_norm': 0.19681484997272491, 'learning_rate': 0.00012092411467293842, 'epoch': 0.5182529335071708}
{'loss': 0.0545, 'grad_norm': 0.2706793546676636, 'learning_rate': 0.00012054563451241196, 'epoch': 0.5215123859191656}
[INFO|trainer.py:3831] 2025-04-14 04:27:20,570 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:27:20,570 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:27:20,570 >>   Batch size = 16
{'eval_loss': 0.048863451927900314, 'eval_accuracy': 0.13365558040545414, 'eval_runtime': 3.6647, 'eval_samples_per_second': 679.452, 'eval_steps_per_second': 10.642, 'epoch': 0.5215123859191656}
[INFO|trainer.py:3515] 2025-04-14 04:27:25,535 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600
[INFO|configuration_utils.py:733] 2025-04-14 04:27:25,777 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:27:25,778 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:27:25,791 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:27:25,793 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/special_tokens_map.json
[2025-04-14 04:27:25,902] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1600 is about to be saved!
[2025-04-14 04:27:25,911] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt
[2025-04-14 04:27:25,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt...
[2025-04-14 04:27:26,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt.
[2025-04-14 04:27:26,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:27:26,872] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:27:26,874] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:27:26,874] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:27:26,881 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-1200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:27:27,289 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:27:27,289 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0451, 'grad_norm': 0.2876276969909668, 'learning_rate': 0.00012017068606286592, 'epoch': 0.5247718383311604}
{'loss': 0.0501, 'grad_norm': 0.22416894137859344, 'learning_rate': 0.00011979921473804348, 'epoch': 0.5280312907431551}
{'loss': 0.0595, 'grad_norm': 0.28871622681617737, 'learning_rate': 0.00011943116712559414, 'epoch': 0.5312907431551499}
{'loss': 0.0572, 'grad_norm': 0.36080121994018555, 'learning_rate': 0.0001190664909548135, 'epoch': 0.5345501955671447}
{'loss': 0.0485, 'grad_norm': 0.3224453330039978, 'learning_rate': 0.00011870513506545995, 'epoch': 0.5378096479791395}
{'loss': 0.0542, 'grad_norm': 0.3263790011405945, 'learning_rate': 0.00011834704937760614, 'epoch': 0.5410691003911343}
{'loss': 0.047, 'grad_norm': 0.26002639532089233, 'learning_rate': 0.00011799218486248467, 'epoch': 0.5443285528031291}
{'loss': 0.0521, 'grad_norm': 0.21098186075687408, 'learning_rate': 0.00011764049351429001, 'epoch': 0.5475880052151239}
{'loss': 0.0526, 'grad_norm': 0.27468106150627136, 'learning_rate': 0.00011729192832289928, 'epoch': 0.5508474576271186}
{'loss': 0.0483, 'grad_norm': 0.3001633882522583, 'learning_rate': 0.0001169464432474767, 'epoch': 0.5541069100391134}
{'loss': 0.0475, 'grad_norm': 0.2728212773799896, 'learning_rate': 0.00011660399319092881, 'epoch': 0.5573663624511083}
{'loss': 0.0507, 'grad_norm': 0.2082950919866562, 'learning_rate': 0.00011626453397517713, 'epoch': 0.560625814863103}
{'loss': 0.0547, 'grad_norm': 0.25404447317123413, 'learning_rate': 0.00011592802231721849, 'epoch': 0.5638852672750978}
{'loss': 0.0567, 'grad_norm': 0.19433154165744781, 'learning_rate': 0.00011559441580594299, 'epoch': 0.5671447196870926}
{'loss': 0.0537, 'grad_norm': 0.23376502096652985, 'learning_rate': 0.00011526367287968178, 'epoch': 0.5704041720990873}
{'loss': 0.0499, 'grad_norm': 0.2495235800743103, 'learning_rate': 0.00011493575280445711, 'epoch': 0.5736636245110821}
{'loss': 0.0526, 'grad_norm': 0.20046153664588928, 'learning_rate': 0.00011461061565290969, 'epoch': 0.5769230769230769}
{'loss': 0.051, 'grad_norm': 0.21575520932674408, 'learning_rate': 0.00011428822228387749, 'epoch': 0.5801825293350718}
{'loss': 0.0551, 'grad_norm': 0.20753410458564758, 'learning_rate': 0.00011396853432260354, 'epoch': 0.5834419817470665}
{'loss': 0.0574, 'grad_norm': 0.27163422107696533, 'learning_rate': 0.0001136515141415488, 'epoch': 0.5867014341590613}
[INFO|trainer.py:3831] 2025-04-14 04:29:14,178 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:29:14,178 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:29:14,179 >>   Batch size = 16
{'eval_loss': 0.049637988209724426, 'eval_accuracy': 0.1336820335233148, 'eval_runtime': 3.6787, 'eval_samples_per_second': 676.868, 'eval_steps_per_second': 10.602, 'epoch': 0.5867014341590613}
[INFO|trainer.py:3515] 2025-04-14 04:29:19,158 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-1800
[INFO|configuration_utils.py:733] 2025-04-14 04:29:19,406 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:29:19,407 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:29:19,421 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:29:19,422 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-1800/special_tokens_map.json
[2025-04-14 04:29:19,523] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1800 is about to be saved!
[2025-04-14 04:29:19,540] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt
[2025-04-14 04:29:19,540] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt...
[2025-04-14 04:29:20,490] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt.
[2025-04-14 04:29:20,493] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:29:20,503] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:29:20,506] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:29:20,506] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:29:20,513 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-1400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:29:20,927 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:29:20,928 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0509, 'grad_norm': 0.25935298204421997, 'learning_rate': 0.00011333712484178924, 'epoch': 0.589960886571056}
{'loss': 0.0589, 'grad_norm': 0.18269555270671844, 'learning_rate': 0.00011302533023497554, 'epoch': 0.5932203389830508}
{'loss': 0.0465, 'grad_norm': 0.2527656853199005, 'learning_rate': 0.00011271609482583614, 'epoch': 0.5964797913950456}
{'loss': 0.0517, 'grad_norm': 0.26378586888313293, 'learning_rate': 0.00011240938379520364, 'epoch': 0.5997392438070405}
{'loss': 0.0512, 'grad_norm': 0.23424933850765228, 'learning_rate': 0.00011210516298354671, 'epoch': 0.6029986962190352}
{'loss': 0.054, 'grad_norm': 0.20580744743347168, 'learning_rate': 0.00011180339887498949, 'epoch': 0.60625814863103}
{'loss': 0.0576, 'grad_norm': 0.22633415460586548, 'learning_rate': 0.00011150405858180188, 'epoch': 0.6095176010430248}
{'loss': 0.0522, 'grad_norm': 0.1608411818742752, 'learning_rate': 0.00011120710982934386, 'epoch': 0.6127770534550195}
{'loss': 0.0513, 'grad_norm': 0.1762000471353531, 'learning_rate': 0.00011091252094144871, 'epoch': 0.6160365058670143}
{'loss': 0.0511, 'grad_norm': 0.24588023126125336, 'learning_rate': 0.00011062026082623002, 'epoch': 0.6192959582790091}
{'loss': 0.0495, 'grad_norm': 0.3128039240837097, 'learning_rate': 0.00011033029896229751, 'epoch': 0.622555410691004}
{'loss': 0.0468, 'grad_norm': 0.27368637919425964, 'learning_rate': 0.00011004260538536882, 'epoch': 0.6258148631029987}
{'loss': 0.0554, 'grad_norm': 0.1934008151292801, 'learning_rate': 0.00010975715067526277, 'epoch': 0.6290743155149935}
{'loss': 0.0479, 'grad_norm': 0.2502470314502716, 'learning_rate': 0.00010947390594326225, 'epoch': 0.6323337679269883}
{'loss': 0.0573, 'grad_norm': 0.30402326583862305, 'learning_rate': 0.00010919284281983377, 'epoch': 0.635593220338983}
{'loss': 0.0475, 'grad_norm': 0.22204148769378662, 'learning_rate': 0.0001089139334426922, 'epoch': 0.6388526727509778}
{'loss': 0.0551, 'grad_norm': 0.21691130101680756, 'learning_rate': 0.00010863715044519895, 'epoch': 0.6421121251629727}
{'loss': 0.0504, 'grad_norm': 0.20203764736652374, 'learning_rate': 0.00010836246694508318, 'epoch': 0.6453715775749674}
{'loss': 0.0496, 'grad_norm': 0.24513007700443268, 'learning_rate': 0.00010808985653347468, 'epoch': 0.6486310299869622}
{'loss': 0.0598, 'grad_norm': 0.22609718143939972, 'learning_rate': 0.00010781929326423911, 'epoch': 0.651890482398957}
[INFO|trainer.py:3831] 2025-04-14 04:31:06,390 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:31:06,391 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:31:06,391 >>   Batch size = 16
{'eval_loss': 0.048879094421863556, 'eval_accuracy': 0.13369886732558978, 'eval_runtime': 3.6943, 'eval_samples_per_second': 674.015, 'eval_steps_per_second': 10.557, 'epoch': 0.651890482398957}
[INFO|trainer.py:3515] 2025-04-14 04:31:11,373 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-2000
[INFO|configuration_utils.py:733] 2025-04-14 04:31:11,630 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:31:11,630 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:31:11,644 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:31:11,646 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-2000/special_tokens_map.json
[2025-04-14 04:31:11,744] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
[2025-04-14 04:31:11,764] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt
[2025-04-14 04:31:11,764] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt...
[2025-04-14 04:31:12,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt.
[2025-04-14 04:31:12,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:31:12,722] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:31:12,724] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:31:12,724] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:31:12,731 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-1800] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:31:13,139 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:31:13,139 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0506, 'grad_norm': 0.25358399748802185, 'learning_rate': 0.00010755075164360533, 'epoch': 0.6551499348109517}
{'loss': 0.0539, 'grad_norm': 0.24315166473388672, 'learning_rate': 0.00010728420662007531, 'epoch': 0.6584093872229465}
{'loss': 0.0534, 'grad_norm': 0.22264494001865387, 'learning_rate': 0.00010701963357460794, 'epoch': 0.6616688396349413}
{'loss': 0.0525, 'grad_norm': 0.26326462626457214, 'learning_rate': 0.00010675700831106787, 'epoch': 0.6649282920469362}
{'loss': 0.0478, 'grad_norm': 0.2110501378774643, 'learning_rate': 0.00010649630704693072, 'epoch': 0.6681877444589309}
{'loss': 0.0422, 'grad_norm': 0.16993963718414307, 'learning_rate': 0.00010623750640423696, 'epoch': 0.6714471968709257}
{'loss': 0.0565, 'grad_norm': 0.21783410012722015, 'learning_rate': 0.00010598058340078627, 'epoch': 0.6747066492829205}
{'loss': 0.0473, 'grad_norm': 0.20955371856689453, 'learning_rate': 0.00010572551544156509, 'epoch': 0.6779661016949152}
{'loss': 0.0457, 'grad_norm': 0.2283010631799698, 'learning_rate': 0.00010547228031040002, 'epoch': 0.68122555410691}
{'loss': 0.0525, 'grad_norm': 0.2442249357700348, 'learning_rate': 0.00010522085616183025, 'epoch': 0.6844850065189049}
{'loss': 0.0579, 'grad_norm': 0.17938798666000366, 'learning_rate': 0.00010497122151319174, 'epoch': 0.6877444589308996}
{'loss': 0.0489, 'grad_norm': 0.2187027782201767, 'learning_rate': 0.00010472335523690725, 'epoch': 0.6910039113428944}
{'loss': 0.0456, 'grad_norm': 0.21857072412967682, 'learning_rate': 0.00010447723655297588, 'epoch': 0.6942633637548892}
{'loss': 0.0559, 'grad_norm': 0.35827237367630005, 'learning_rate': 0.0001042328450216553, 'epoch': 0.6975228161668839}
{'loss': 0.0596, 'grad_norm': 0.1760953813791275, 'learning_rate': 0.00010399016053633196, 'epoch': 0.7007822685788787}
{'loss': 0.0532, 'grad_norm': 0.2288457304239273, 'learning_rate': 0.00010374916331657276, 'epoch': 0.7040417209908736}
{'loss': 0.0472, 'grad_norm': 0.23478256165981293, 'learning_rate': 0.00010350983390135312, 'epoch': 0.7073011734028684}
{'loss': 0.0558, 'grad_norm': 0.2703792154788971, 'learning_rate': 0.00010327215314245607, 'epoch': 0.7105606258148631}
{'loss': 0.0527, 'grad_norm': 0.28552964329719543, 'learning_rate': 0.00010303610219803729, 'epoch': 0.7138200782268579}
{'loss': 0.0543, 'grad_norm': 0.18910811841487885, 'learning_rate': 0.00010280166252635129, 'epoch': 0.7170795306388527}
[INFO|trainer.py:3831] 2025-04-14 04:32:57,288 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:32:57,289 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:32:57,289 >>   Batch size = 16
{'eval_loss': 0.05265674367547035, 'eval_accuracy': 0.1336219128009042, 'eval_runtime': 3.6674, 'eval_samples_per_second': 678.959, 'eval_steps_per_second': 10.634, 'epoch': 0.7170795306388527}
[INFO|trainer.py:3515] 2025-04-14 04:33:02,239 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-2200
[INFO|configuration_utils.py:733] 2025-04-14 04:33:02,480 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:33:02,481 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:33:02,495 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-2200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:33:02,496 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-2200/special_tokens_map.json
[2025-04-14 04:33:02,597] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2200 is about to be saved!
[2025-04-14 04:33:02,616] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-2200/global_step2200/mp_rank_00_model_states.pt
[2025-04-14 04:33:02,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-2200/global_step2200/mp_rank_00_model_states.pt...
[2025-04-14 04:33:03,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2200/global_step2200/mp_rank_00_model_states.pt.
[2025-04-14 04:33:03,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-2200/global_step2200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:33:03,611] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2200/global_step2200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:33:03,614] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2200/global_step2200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:33:03,614] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2200 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:33:03,623 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-2000] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:33:04,056 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:33:04,057 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0512, 'grad_norm': 0.24229954183101654, 'learning_rate': 0.00010256881587963384, 'epoch': 0.7203389830508474}
{'loss': 0.0554, 'grad_norm': 0.3009454607963562, 'learning_rate': 0.00010233754429813614, 'epoch': 0.7235984354628422}
{'loss': 0.0507, 'grad_norm': 0.19930008053779602, 'learning_rate': 0.00010210783010430625, 'epoch': 0.7268578878748371}
{'loss': 0.0469, 'grad_norm': 0.23632211983203888, 'learning_rate': 0.00010187965589711366, 'epoch': 0.7301173402868318}
{'loss': 0.0503, 'grad_norm': 0.20024077594280243, 'learning_rate': 0.0001016530045465127, 'epoch': 0.7333767926988266}
{'loss': 0.0523, 'grad_norm': 0.32170045375823975, 'learning_rate': 0.00010142785918804077, 'epoch': 0.7366362451108214}
{'loss': 0.0537, 'grad_norm': 0.21563337743282318, 'learning_rate': 0.00010120420321754762, 'epoch': 0.7398956975228161}
{'loss': 0.057, 'grad_norm': 0.30686983466148376, 'learning_rate': 0.00010098202028605207, 'epoch': 0.7431551499348109}
{'loss': 0.0541, 'grad_norm': 0.23488524556159973, 'learning_rate': 0.00010076129429472212, 'epoch': 0.7464146023468058}
{'loss': 0.0557, 'grad_norm': 0.24131065607070923, 'learning_rate': 0.00010054200938997547, 'epoch': 0.7496740547588006}
{'loss': 0.0572, 'grad_norm': 0.2923318147659302, 'learning_rate': 0.0001003241499586967, 'epoch': 0.7529335071707953}
{'loss': 0.0499, 'grad_norm': 0.17871253192424774, 'learning_rate': 0.00010010770062356807, 'epoch': 0.7561929595827901}
{'loss': 0.0517, 'grad_norm': 0.21306799352169037, 'learning_rate': 9.989264623851042e-05, 'epoch': 0.7594524119947849}
{'loss': 0.0537, 'grad_norm': 0.22392822802066803, 'learning_rate': 9.967897188423163e-05, 'epoch': 0.7627118644067796}
{'loss': 0.0457, 'grad_norm': 0.34328460693359375, 'learning_rate': 9.946666286387914e-05, 'epoch': 0.7659713168187744}
{'loss': 0.0531, 'grad_norm': 0.28362688422203064, 'learning_rate': 9.92557046987942e-05, 'epoch': 0.7692307692307693}
{'loss': 0.0532, 'grad_norm': 0.23933714628219604, 'learning_rate': 9.90460831243643e-05, 'epoch': 0.772490221642764}
{'loss': 0.0465, 'grad_norm': 0.21790429949760437, 'learning_rate': 9.883778408597196e-05, 'epoch': 0.7757496740547588}
{'loss': 0.0533, 'grad_norm': 0.29366445541381836, 'learning_rate': 9.863079373503666e-05, 'epoch': 0.7790091264667536}
{'loss': 0.0476, 'grad_norm': 0.28186652064323425, 'learning_rate': 9.842509842514764e-05, 'epoch': 0.7822685788787483}
[INFO|trainer.py:3831] 2025-04-14 04:34:50,705 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:34:50,705 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:34:50,705 >>   Batch size = 16
{'eval_loss': 0.05087216570973396, 'eval_accuracy': 0.133660390063247, 'eval_runtime': 3.7059, 'eval_samples_per_second': 671.898, 'eval_steps_per_second': 10.524, 'epoch': 0.7822685788787483}
[INFO|trainer.py:3515] 2025-04-14 04:34:55,688 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-2400
[INFO|configuration_utils.py:733] 2025-04-14 04:34:55,938 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:34:55,938 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:34:55,953 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-2400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:34:55,954 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-2400/special_tokens_map.json
[2025-04-14 04:34:56,058] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2400 is about to be saved!
[2025-04-14 04:34:56,067] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-2400/global_step2400/mp_rank_00_model_states.pt
[2025-04-14 04:34:56,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-2400/global_step2400/mp_rank_00_model_states.pt...
[2025-04-14 04:34:57,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2400/global_step2400/mp_rank_00_model_states.pt.
[2025-04-14 04:34:57,024] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:34:57,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:34:57,037] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:34:57,037] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2400 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:34:57,045 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-2200] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:34:57,503 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:34:57,504 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.0459, 'grad_norm': 0.24448218941688538, 'learning_rate': 9.822068470828493e-05, 'epoch': 0.7855280312907431}
{'loss': 0.0511, 'grad_norm': 0.2960617244243622, 'learning_rate': 9.801753933112648e-05, 'epoch': 0.788787483702738}
{'loss': 0.0462, 'grad_norm': 0.18409815430641174, 'learning_rate': 9.781564923143894e-05, 'epoch': 0.7920469361147328}
{'loss': 0.0485, 'grad_norm': 0.24095559120178223, 'learning_rate': 9.761500153454981e-05, 'epoch': 0.7953063885267275}
{'loss': 0.0554, 'grad_norm': 0.21684536337852478, 'learning_rate': 9.741558354989897e-05, 'epoch': 0.7985658409387223}
{'loss': 0.0463, 'grad_norm': 0.19621291756629944, 'learning_rate': 9.721738276766724e-05, 'epoch': 0.8018252933507171}
{'loss': 0.0472, 'grad_norm': 0.2495613992214203, 'learning_rate': 9.702038685548026e-05, 'epoch': 0.8050847457627118}
{'loss': 0.0487, 'grad_norm': 0.1577138900756836, 'learning_rate': 9.682458365518543e-05, 'epoch': 0.8083441981747066}
{'loss': 0.0533, 'grad_norm': 0.1440591961145401, 'learning_rate': 9.662996117970018e-05, 'epoch': 0.8116036505867015}
{'loss': 0.0466, 'grad_norm': 0.34311163425445557, 'learning_rate': 9.643650760992955e-05, 'epoch': 0.8148631029986962}
{'loss': 0.0525, 'grad_norm': 0.26836714148521423, 'learning_rate': 9.624421129175155e-05, 'epoch': 0.818122555410691}
{'loss': 0.0477, 'grad_norm': 0.29464054107666016, 'learning_rate': 9.605306073306815e-05, 'epoch': 0.8213820078226858}
{'loss': 0.0424, 'grad_norm': 0.26107823848724365, 'learning_rate': 9.586304460092065e-05, 'epoch': 0.8246414602346805}
{'loss': 0.0436, 'grad_norm': 0.22693997621536255, 'learning_rate': 9.567415171866754e-05, 'epoch': 0.8279009126466753}
{'loss': 0.0516, 'grad_norm': 0.1791483461856842, 'learning_rate': 9.548637106322309e-05, 'epoch': 0.8311603650586702}
{'loss': 0.0503, 'grad_norm': 0.21920517086982727, 'learning_rate': 9.529969176235567e-05, 'epoch': 0.834419817470665}
{'loss': 0.0468, 'grad_norm': 0.23736388981342316, 'learning_rate': 9.511410309204358e-05, 'epoch': 0.8376792698826597}
{'loss': 0.0559, 'grad_norm': 0.1887674480676651, 'learning_rate': 9.492959447388757e-05, 'epoch': 0.8409387222946545}
{'loss': 0.0595, 'grad_norm': 0.21470344066619873, 'learning_rate': 9.47461554725783e-05, 'epoch': 0.8441981747066493}
{'loss': 0.0564, 'grad_norm': 0.23647958040237427, 'learning_rate': 9.456377579341728e-05, 'epoch': 0.847457627118644}
[INFO|trainer.py:3831] 2025-04-14 04:36:43,025 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:36:43,025 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:36:43,025 >>   Batch size = 16
{'eval_loss': 0.0491146519780159, 'eval_accuracy': 0.1336507707476613, 'eval_runtime': 3.6781, 'eval_samples_per_second': 676.989, 'eval_steps_per_second': 10.603, 'epoch': 0.847457627118644}
[INFO|trainer.py:3515] 2025-04-14 04:36:47,983 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu/checkpoint-2600
[INFO|configuration_utils.py:733] 2025-04-14 04:36:48,227 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:36:48,228 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:36:48,242 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-2600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:36:48,243 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/checkpoint-2600/special_tokens_map.json
[2025-04-14 04:36:48,345] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2600 is about to be saved!
[2025-04-14 04:36:48,358] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./test_run_outputs_accelerate_multi_gpu/checkpoint-2600/global_step2600/mp_rank_00_model_states.pt
[2025-04-14 04:36:48,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-2600/global_step2600/mp_rank_00_model_states.pt...
[2025-04-14 04:36:49,314] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2600/global_step2600/mp_rank_00_model_states.pt.
[2025-04-14 04:36:49,317] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./test_run_outputs_accelerate_multi_gpu/checkpoint-2600/global_step2600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:36:49,328] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2600/global_step2600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:36:49,330] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved ./test_run_outputs_accelerate_multi_gpu/checkpoint-2600/global_step2600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-14 04:36:49,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2600 is ready now!
[INFO|trainer.py:3607] 2025-04-14 04:36:49,339 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-2400] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:36:49,745 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:36:49,746 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:2406] 2025-04-14 04:36:49,817 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2644] 2025-04-14 04:36:49,817 >> Loading best model from ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600 (score: 0.048863451927900314).
[INFO|deepspeed.py:431] 2025-04-14 04:36:49,820 >> Attempting to resume from ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600
[2025-04-14 04:36:49,821] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt...
[2025-04-14 04:36:49,950] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt.
[2025-04-14 04:36:49,952] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt...
[2025-04-14 04:36:50,082] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt.
[2025-04-14 04:36:50,165] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-14 04:36:50,169] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from ./test_run_outputs_accelerate_multi_gpu/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-14 04:36:50,169] [INFO] [engine.py:3212:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 0
[2025-04-14 04:36:50,171] [INFO] [engine.py:3162:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 0
{'train_runtime': 1464.7286, 'train_samples_per_second': 268.106, 'train_steps_per_second': 2.095, 'train_loss': 0.08226447714062837, 'epoch': 0.847457627118644}
[INFO|trainer.py:2447] 2025-04-14 04:36:50,172 >> Deleting older checkpoint [test_run_outputs_accelerate_multi_gpu/checkpoint-2600] due to args.save_total_limit
[INFO|configuration_utils.py:733] 2025-04-14 04:36:50,556 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:36:50,556 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|trainer.py:3515] 2025-04-14 04:36:51,803 >> Saving model checkpoint to ./test_run_outputs_accelerate_multi_gpu
[INFO|configuration_utils.py:733] 2025-04-14 04:36:52,063 >> loading configuration file config.json from cache at /home/ka/ka_stud/ka_usxcp/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json
[INFO|configuration_utils.py:821] 2025-04-14 04:36:52,064 >> Model config LlamaConfig {
  "additional_loss_layer": null,
  "alignment_matrices_path": null,
  "apply_inverse": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "contrastive_loss_temperature": null,
  "contrastive_loss_weight": null,
  "contrastive_pooling_type": null,
  "distance_function": null,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "inject_Ws": false,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "only_train_contrastive": null,
  "only_train_language_modeling": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0.dev0",
  "unidirectional_contrastive_loss": null,
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2025-04-14 04:36:52,168 >> tokenizer config file saved in ./test_run_outputs_accelerate_multi_gpu/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2025-04-14 04:36:52,170 >> Special tokens file saved in ./test_run_outputs_accelerate_multi_gpu/special_tokens_map.json
***** train metrics *****
  epoch                    =     0.8475
  train_loss               =     0.0823
  train_runtime            = 0:24:24.72
  train_samples            =     392702
  train_samples_per_second =    268.106
  train_steps_per_second   =      2.095
04/14/2025 04:36:52 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3831] 2025-04-14 04:36:52,282 >> 
***** Running Evaluation *****
[INFO|trainer.py:3833] 2025-04-14 04:36:52,282 >>   Num examples = 2490
[INFO|trainer.py:3836] 2025-04-14 04:36:52,282 >>   Batch size = 16
{'eval_loss': 0.048863451927900314, 'eval_accuracy': 0.13365558040545414, 'eval_runtime': 3.6552, 'eval_samples_per_second': 681.224, 'eval_steps_per_second': 10.67, 'epoch': 0.847457627118644}
***** eval metrics *****
  epoch                   =     0.8475
  eval_accuracy           =     0.1337
  eval_loss               =     0.0489
  eval_runtime            = 0:00:03.65
  eval_samples            =       2490
  eval_samples_per_second =    681.224
  eval_steps_per_second   =      10.67
  perplexity              =     1.0501
[INFO|modelcard.py:449] 2025-04-14 04:36:56,064 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.13365558040545414}]}
[rank0]:[W414 04:36:56.676808945 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
